,question,answer,Answer_type,Picture,source,id,theorem,subfield,field,theorem_def
647,"suppose $lim_{n \rightarrow \infty}a_n=1$, what is the limit of (a_1+2a_2+...+na_n)/n^2?",0.5,float,NONE,mathematical analysis 1 exercise 1.12.7,mingyin/Limit-of-sequence3.json,limiting theorem,Mathematical analysis,Math,"In mathematical analysis, a limiting theorem refers to a result that describes the behavior of a sequence, function, or series as it approaches a specific value or point. These theorems are fundamental in understanding the properties of mathematical objects and their convergence or divergence. There are several important limiting theorems in mathematical analysis, including:

1. Limit of a sequence: A sequence is a list of numbers arranged in a specific order. The limit of a sequence is the value that the terms of the sequence approach as the index goes to infinity. If the limit exists, the sequence is said to be convergent; otherwise, it is divergent.

2. Limit of a function: The limit of a function is the value that the function approaches as its input approaches a specific value. Limits are used to define continuity, derivatives, and integrals, which are essential concepts in calculus.

3. Squeeze theorem: Also known as the sandwich theorem or the pinching theorem, this theorem states that if a function is ""squeezed"" between two other functions that have the same limit at a specific point, then the squeezed function must also have the same limit at that point.

4. Monotone convergence theorem: This theorem states that a monotone (either non-decreasing or non-increasing) and bounded sequence always converges to a limit.

5. Bolzano-Weierstrass theorem: This theorem states that every bounded sequence has a convergent subsequence, which is a sequence formed by selecting terms from the original sequence while preserving their order.

6. Dominated convergence theorem: This theorem provides a condition under which the limit of an integral can be interchanged with the integral of a limit. It is particularly useful in the study of Lebesgue integration.

7. Central limit theorem: In probability theory and statistics, the central limit theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the shape of the original distribution.

These limiting theorems play a crucial role in various branches of mathematics, including calculus, real analysis, complex analysis, and probability theory. They help us understand the behavior of mathematical objects and provide a foundation for further study and applications."
382,Obtain the number of real roots between 0 and 3 of the equation P(x) = x^4 -4x^3 + 3x^2 + 4x - 4 = 0 using Sturm's sequence.,2,integer,NONE,website | http://ndl.ethernet.edu.et/bitstream/123456789/79528/3/Numerical_Methods_Problems_and_Solutions_cropped.pdf,wenhuchen/Sturm.json,sturm's sequence,Numerical analysis,Math,"Sturm's Sequence is a method in numerical analysis used to find the number of distinct real roots of a polynomial within a given interval. It is based on Sturm's theorem, which was developed by French mathematician Jacques Charles François Sturm in the 19th century. The method involves constructing a sequence of polynomials derived from the original polynomial and its derivative, and then counting the sign changes between consecutive polynomials at the endpoints of the interval.

Here's how to construct Sturm's Sequence for a given polynomial P(x):

1. Start with the polynomial P(x) and its derivative P'(x).
2. Perform polynomial division to find the remainder R1(x) when P(x) is divided by P'(x). Multiply R1(x) by -1 to obtain the next polynomial in the sequence.
3. Continue this process of polynomial division and negating the remainder, dividing the previous polynomial in the sequence by the current one, until you reach a constant polynomial (i.e., a polynomial with degree 0).

The resulting sequence of polynomials is called Sturm's Sequence for P(x).

To find the number of distinct real roots of P(x) within a given interval [a, b], follow these steps:

1. Evaluate each polynomial in Sturm's Sequence at the endpoints a and b.
2. Count the number of sign changes in the sequence of evaluated polynomials at a and b.
3. Subtract the number of sign changes at a from the number of sign changes at b.

The result is the number of distinct real roots of P(x) within the interval [a, b]. Note that Sturm's Sequence does not provide the actual roots, but rather the number of roots within the specified interval."
208,In how many ways can a committee of 2 men and 3 women be selected from a group of 6 men and 8 women?,840,integer,NONE,self,jianyu_xu/Binomial_5.json,binomial theorem,Combinatorics,Math,"The Binomial Theorem, in the context of combinatorics, is a powerful mathematical principle that allows us to expand expressions of the form (a + b)^n, where 'a' and 'b' are any real numbers, and 'n' is a non-negative integer. The theorem provides a systematic way to find the coefficients of the terms in the expanded form of the binomial expression.

The Binomial Theorem states that for any non-negative integer 'n' and any real numbers 'a' and 'b':

(a + b)^n = Σ [C(n, k) * a^(n-k) * b^k]

where the summation (Σ) runs from k = 0 to k = n, and C(n, k) represents the binomial coefficient, which is the number of ways to choose 'k' items from a set of 'n' items, also denoted as ""n choose k"" or C(n, k) = n! / (k! * (n-k)!), where '!' denotes the factorial function.

The binomial coefficients can also be represented using Pascal's Triangle, a triangular array of numbers where each number is the sum of the two numbers directly above it. The 'n'th row of Pascal's Triangle contains the coefficients of the binomial expansion of (a + b)^n.

In combinatorics, the Binomial Theorem is used to solve counting problems, such as finding the number of ways to arrange objects, the number of subsets of a given size, and the probability of certain outcomes in experiments.

For example, using the Binomial Theorem, we can find the expansion of (a + b)^4:

(a + b)^4 = C(4, 0) * a^4 * b^0 + C(4, 1) * a^3 * b^1 + C(4, 2) * a^2 * b^2 + C(4, 3) * a^1 * b^3 + C(4, 4) * a^0 * b^4
           = 1 * a^4 + 4 * a^3 * b + 6 * a^2 * b^2 + 4 * a * b^3 + 1 * b^4"
287,"The planet Pluto (radius 1180 km) is populated by three species of purple caterpillar. Studies have established the following facts: 1. A line of 5 mauve caterpillars is as long as a line of 7 violet caterpillars. 2. A line of 3 lavender caterpillars and 1 mauve caterpillar is as long as a line of 8 violet caterpillars. 3. A line of 5 lavender caterpillars, 5 mauve caterpillars and 2 violet caterpillars is 1 m long in total. 4.  A lavender caterpillar takes 10 s to crawl the length of a violet caterpillar. 5. Violet and mauve caterpillars both crawl twice as fast as lavender caterpillars. How many years would it take a mauve caterpillar to crawl around the equator of Pluto?",23.0,float,NONE,website | https://www.ox.ac.uk/sites/files/oxford/field/field_document/Solutions%201.pdf,wenhuchen/kinetics2.json,kinetics theorem,Kinetics,Physics,"Kinetic theory, also known as the kinetic theory of gases, is a scientific theorem that explains the behavior of gases based on the motion of their constituent particles, such as atoms or molecules. The main idea behind the kinetic theory is that the macroscopic properties of a gas, such as pressure, temperature, and volume, can be explained by the microscopic motion and interactions of its particles.

The key assumptions of the kinetic theory are:

1. Gases are composed of a large number of small particles (atoms or molecules) that are in constant, random motion.
2. The particles are so small compared to the distances between them that their individual volumes can be considered negligible.
3. The particles are in constant, random motion, and they collide with each other and the walls of the container. These collisions are perfectly elastic, meaning that there is no loss of kinetic energy during the collisions.
4. There are no attractive or repulsive forces between the particles, except during the brief moments of collision.
5. The average kinetic energy of the particles is directly proportional to the temperature of the gas.

Based on these assumptions, the kinetic theory can be used to derive various gas laws, such as Boyle's law, Charles's law, and the ideal gas law, which describe the relationships between pressure, volume, and temperature in a gas. The kinetic theory also provides a basis for understanding the diffusion of gases, the transport of heat in gases, and the behavior of gases in different thermodynamic processes."
213,"Ms. Fogg is planning an around-the-world trip on which she plans to spend $10,000. The utility from the trip is a function of how much she actually spends on it (Y), given by U(Y) = ln Y. If there is a 25 percent probability that Ms. Fogg will lose $1,000 of her cash on the trip, what is the trip’s expected utility?",9.184,float,NONE,"text | Walter Nicholson, Microeconomic Theory Basic Principles and Extensions.",elainewan/econ_micro_7_2.json,expected utility,Economics,Finance,"Expected Utility is a concept in economics and decision theory that refers to the total satisfaction or value that an individual expects to receive from a particular choice or decision, taking into account the probabilities of different outcomes. It is a key concept in understanding how people make decisions under uncertainty and is widely used in various fields, including finance, insurance, and game theory.

The Expected Utility Theory assumes that individuals are rational decision-makers who aim to maximize their utility or satisfaction. When faced with multiple options, individuals will choose the one that provides the highest expected utility. This means that they will weigh the potential benefits and costs of each option, considering the likelihood of each outcome occurring.

To calculate the expected utility of a decision, one must:

1. Identify all possible outcomes of the decision.
2. Assign a utility value to each outcome, representing the satisfaction or value that the individual would receive from that outcome.
3. Determine the probability of each outcome occurring.
4. Multiply the utility value of each outcome by its probability.
5. Sum the products of the utility values and probabilities to obtain the expected utility of the decision.

By comparing the expected utilities of different options, individuals can make informed choices that maximize their overall satisfaction.

It is important to note that the concept of expected utility is based on subjective evaluations of utility and probabilities, which may vary from person to person. Additionally, the theory assumes that individuals have perfect information and can accurately assess probabilities and utility values, which may not always be the case in real-world situations. Despite these limitations, expected utility remains a fundamental concept in understanding decision-making under uncertainty."
284,"You throw a ball from your window $8.0 \mathrm{~m}$ above the ground. When the ball leaves your hand, it is moving at $10.0 \mathrm{~m} / \athrm{s}$ at an angle of $20^{\circ}$ below the horizontal. How far horizontally from your window will the ball hit the ground? Ignore air resistance. (Unit: m)",9.2,float,NONE,University Physics with Modern Physics | Example 3.9,panlu/pojectile_motion2.json,projectile motion,Kinetics,Physics,"Projectile motion refers to the motion of an object that is projected into the air and is influenced only by the force of gravity. It is a type of two-dimensional motion, as it involves both horizontal and vertical components. In the study of kinetics, projectile motion is analyzed to understand the behavior of objects moving under the influence of gravity.

There are a few key characteristics of projectile motion:

1. The horizontal motion and vertical motion are independent of each other. This means that the horizontal velocity remains constant throughout the motion, while the vertical velocity is affected by gravity.

2. The only force acting on the object in projectile motion is gravity, which acts vertically downward. There are no other forces, such as air resistance, considered in the ideal projectile motion.

3. The trajectory of the projectile is parabolic. This means that the path followed by the projectile is in the shape of a parabola, with the highest point called the apex.

4. The time it takes for the projectile to reach its maximum height is equal to the time it takes to fall back to the same height from which it was launched.

5. The range of the projectile, which is the horizontal distance it travels, depends on the initial velocity, launch angle, and the acceleration due to gravity.

To analyze projectile motion, the following equations are commonly used:

1. Horizontal motion:
   - Displacement: x = v_x * t
   - Velocity: v_x = constant

2. Vertical motion:
   - Displacement: y = v_y * t - 0.5 * g * t^2
   - Velocity: v_y = v_0 * sin(θ) - g * t
   - Acceleration: a_y = -g

In these equations, x and y represent the horizontal and vertical displacements, v_x and v_y are the horizontal and vertical velocities, t is the time, g is the acceleration due to gravity (approximately 9.81 m/s²), and θ is the launch angle."
99,what is the value of $\prod_{n=0}^{\infty}(1+(\frac{1}{2})^{2^n})$?,2.0,float,NONE,mathematical analysis 1; 9.7.1-(3),mingyin/Wallis-theorem3.json,wallis formula,Mathematical analysis,Math,"Wallis formula is a mathematical expression that provides an infinite product representation of the value of pi (π). It is named after the English mathematician John Wallis, who first introduced the formula in 1655. The Wallis formula is given by:

π/2 = Π(n=1 to ∞) [(2n * 2n) / ((2n - 1) * (2n + 1))]

In this formula, Π denotes the product notation, similar to the summation notation (Σ) for sums. The formula can also be written as:

π/2 = (2/1) * (2/3) * (4/3) * (4/5) * (6/5) * (6/7) * (8/7) * (8/9) * ...

The Wallis formula is derived from the integral representation of the sine and cosine functions and their relationship with the value of π. It is an important result in mathematical analysis, as it connects the value of π with the properties of trigonometric functions and infinite products.

The convergence of the Wallis formula is relatively slow, meaning that a large number of terms must be calculated to obtain an accurate approximation of π. However, it is still a fascinating and elegant representation of the fundamental constant π and has inspired further research into infinite product representations and the properties of π."
145,"What is the determinant of the matrix A = [[1, 0, 0, 0, 0, 0], [2, 7, 0, 0, 0, 0], [3, 8, 6, 0, 0, 0], [4, 9, 5, 2, 1, 4], [5, 8, 4, 0, 2, 5], [6, 7, 3, 0, 3, 6]]?",-252,integer,NONE,"text | Otto Bretscher, Linear Algebra with Applications.",elainewan/math_algebra_6_5.json,matrix determinant formula,Algebra,Math,"The matrix determinant formula is a mathematical expression used to calculate the determinant of a square matrix. The determinant is a scalar value that can be computed from the elements of a square matrix and has important properties in linear algebra, particularly in the context of systems of linear equations, matrix inversion, and transformations.

For a 2x2 matrix A, with elements a, b, c, and d, the determinant is denoted as |A| or det(A) and is calculated as follows:

|A| = ad - bc

For a 3x3 matrix A, with elements a, b, c, d, e, f, g, h, and i, the determinant is calculated as follows:

|A| = a(ei - fh) - b(di - fg) + c(dh - eg)

For larger square matrices (n x n), the determinant can be calculated using various methods, such as the Laplace expansion, which involves breaking down the matrix into smaller matrices and recursively calculating their determinants, or the more efficient LU decomposition or Gaussian elimination methods.

In general, the determinant of an n x n matrix A can be calculated using the following formula:

|A| = Σ(-1)^(i+j) * a_ij * |A_ij|

where the summation is over all elements a_ij in the first row (or any other row or column), A_ij is the (n-1) x (n-1) matrix obtained by removing the i-th row and j-th column from A, and (-1)^(i+j) is the sign factor that depends on the position of the element in the matrix."
20,"For which 2 * 2 matrices A does there exist a nonzero matrix M such that AM = MD, where D = [[2, 0], [0, 3]]? Give your answer in terms of eigenvalues of A.","[2, 3]",list of integer,NONE,"text | Otto Bretscher, Linear Algebra with Applications.",elainewan/math_algebra_7_5.json,eigenvalues and eigenvectors,Algebra,Math,"Eigenvalues and eigenvectors are fundamental concepts in linear algebra, particularly in the study of linear transformations and matrices. They provide insight into the behavior of a linear transformation and can be used to solve various problems in mathematics, physics, and engineering.

Eigenvalues:
An eigenvalue (denoted by λ) is a scalar value associated with a given square matrix (A) that satisfies the following equation:

A * v = λ * v

where A is a square matrix, v is a non-zero vector (called the eigenvector), and λ is the eigenvalue. In other words, when a matrix A is multiplied by an eigenvector v, the result is a scaled version of the same eigenvector, with the scaling factor being the eigenvalue λ.

To find the eigenvalues of a matrix, we need to solve the following equation:

det(A - λ * I) = 0

where det() denotes the determinant of a matrix, I is the identity matrix of the same size as A, and λ is the eigenvalue. The solutions to this equation are the eigenvalues of the matrix A.

Eigenvectors:
An eigenvector (denoted by v) is a non-zero vector that, when multiplied by a square matrix A, results in a scaled version of itself, with the scaling factor being the eigenvalue λ. As mentioned earlier, the relationship between a matrix A, its eigenvector v, and the corresponding eigenvalue λ can be expressed as:

A * v = λ * v

Eigenvectors are essential in understanding the geometric interpretation of a linear transformation represented by a matrix. They indicate the directions in which the transformation stretches or compresses the space, while the eigenvalues represent the magnitude of the stretching or compression.

In summary, eigenvalues and eigenvectors are crucial concepts in linear algebra that help us understand the properties and behavior of linear transformations and matrices. They have numerous applications in various fields, including differential equations, quantum mechanics, computer graphics, and data analysis."
189,Apply the Graeffe's root squaring method to find the roots of the following equation x^3 + 3x^2 - 4 = 0 correct to two decimals. What's the sum of these roots?,-3,integer,NONE,website | http://ndl.ethernet.edu.et/bitstream/123456789/79528/3/Numerical_Methods_Problems_and_Solutions_cropped.pdf,wenhuchen/Graffe's_root2.json,graeffe's theorem,Numerical analysis,Math,"Graeffe's Theorem, also known as Graeffe's Root-Squaring Method, is a numerical analysis technique used to approximate the roots of a polynomial equation. It was developed by the German mathematician August Leopold Crelle in 1828 and later popularized by the French mathematician Augustin Louis Cauchy. The method is particularly useful for finding the roots of a polynomial with real coefficients.

The main idea behind Graeffe's Theorem is to transform the original polynomial into a new polynomial with the same roots but with higher separation between them. This is achieved by squaring the roots of the original polynomial, which makes the roots with larger magnitudes grow faster than the roots with smaller magnitudes. This process is repeated iteratively until the roots are well-separated, and then other numerical methods, such as Newton-Raphson or bisection, can be applied to find the roots more easily.

The theorem can be stated as follows:

Given a polynomial P(x) of degree n with real coefficients:

P(x) = a_0 + a_1x + a_2x^2 + ... + a_nx^n

The transformed polynomial Q(x) is obtained by squaring the roots of P(x):

Q(x) = b_0 + b_1x + b_2x^2 + ... + b_nx^n

where the coefficients b_i are related to the coefficients a_i by the following recurrence relation:

b_0 = a_0^2
b_1 = 2a_0a_1
b_2 = a_1^2 + 2a_0a_2
b_3 = 2a_1a_2 + 2a_0a_3
...
b_n = a_n^2

By iteratively applying Graeffe's Theorem, the roots of the original polynomial P(x) can be approximated with increasing accuracy. However, it is important to note that this method is not always numerically stable, especially for polynomials with closely spaced roots or with roots of very different magnitudes. In such cases, other numerical methods or root-finding algorithms may be more appropriate."
719,The asteroid Pallas has an orbital period of 4.62 years and an orbital eccentricity of 0.233. Find the semi-major axis of its orbit. (Unit: 10^11 m),4.15,float,NONE,University Physics with Modern Physics | Example 13.8,panlu/kepler’s_third_law1.json,kepler's third law,Celestial mechanics,Physics,"Kepler's Third Law, also known as the Law of Harmonies, is one of the three fundamental laws of planetary motion formulated by the German astronomer Johannes Kepler in the early 17th century. This law relates the orbital period of a planet to its average distance from the Sun, stating that the square of the orbital period of a planet is directly proportional to the cube of the semi-major axis of its orbit.

Mathematically, Kepler's Third Law can be expressed as:

(T₁/T₂)² = (a₁/a₂)³

where T₁ and T₂ are the orbital periods of two planets, and a₁ and a₂ are the semi-major axes of their respective orbits.

In simpler terms, this law implies that planets that are closer to the Sun have shorter orbital periods and move faster in their orbits, while planets that are farther away from the Sun have longer orbital periods and move slower in their orbits. This relationship holds true for all planets in our solar system and can also be applied to other celestial bodies, such as moons orbiting a planet or exoplanets orbiting a star."
415,"In a group of 10 people, each of whom has one of 3 different eye colors, at least how many people must have the same eye color?",4,integer,NONE,self,jianyu_xu/pigeonhole_3.json,pigeonhole principle,Combinatorics,Math,"The Pigeonhole Principle is a fundamental concept in combinatorics, a branch of mathematics that deals with counting and arranging objects. It is a simple yet powerful idea that helps to draw conclusions about the distribution of objects among a finite number of containers or ""pigeonholes.""

The principle states that if you have more objects (pigeons) than containers (pigeonholes), then at least one container must contain more than one object. In other words, if you try to fit n+1 objects into n containers, at least one container will have at least two objects.

This principle is useful in solving various problems in mathematics and computer science, where it helps to identify patterns, make generalizations, and prove the existence of certain conditions.

For example, consider a group of 13 people. According to the Pigeonhole Principle, at least two of them must share the same birthday month since there are only 12 months in a year. This doesn't tell us which people or which month, but it guarantees that such a pair exists.

In summary, the Pigeonhole Principle is a basic yet powerful combinatorial tool that allows us to make conclusions about the distribution of objects among a finite number of containers, often leading to surprising and counterintuitive results."
74,"In a group of 1000 people, at least how many people have to share the same birthday?",3,integer,NONE,self,jianyu_xu/pigeonhole_4.json,pigeonhole principle,Combinatorics,Math,"The Pigeonhole Principle is a fundamental concept in combinatorics, a branch of mathematics that deals with counting and arranging objects. It is a simple yet powerful idea that helps to draw conclusions about the distribution of objects among a finite number of containers or ""pigeonholes.""

The principle states that if you have more objects (pigeons) than containers (pigeonholes), then at least one container must contain more than one object. In other words, if you try to fit n+1 objects into n containers, at least one container will have at least two objects.

This principle is useful in solving various problems in mathematics and computer science, where it helps to identify patterns, make generalizations, and prove the existence of certain conditions.

For example, consider a group of 13 people. According to the Pigeonhole Principle, at least two of them must share the same birthday month since there are only 12 months in a year. This doesn't tell us which people or which month, but it guarantees that such a pair exists.

In summary, the Pigeonhole Principle is a basic yet powerful combinatorial tool that allows us to make conclusions about the distribution of objects among a finite number of containers, often leading to surprising and counterintuitive results."
643,what is the limit of $2/\sqrt{\pi}*\sqrt{n}\int_0^1(1-x^2)^n dx$ as n goes to infinity?,1.0,float,NONE,mathematical analysis 2: 20.4 example 4,mingyin/gamma-function2.json,gamma function,Mathematical analysis,Math,"The Gamma function is a mathematical concept used in analysis and is an extension of the factorial function to complex numbers. It is denoted by the symbol Γ(n) and is defined for all complex numbers except for non-positive integers. The Gamma function is particularly useful in various areas of mathematics, including calculus, complex analysis, and number theory.

The Gamma function is defined as:

Γ(n) = ∫(t^(n-1) * e^(-t)) dt, where the integral is taken from 0 to infinity, and n is a complex number with a positive real part.

For positive integers, the Gamma function has the property:

Γ(n) = (n-1)!

This means that the Gamma function reduces to the factorial function for positive integers. For example, Γ(5) = 4! = 4 × 3 × 2 × 1 = 24.

The Gamma function has several important properties, including:

1. Functional equation: Γ(n+1) = nΓ(n), which relates the values of the Gamma function at consecutive points.

2. Reflection formula: Γ(1-z)Γ(z) = π/sin(πz), which connects the values of the Gamma function at points symmetric with respect to the line Re(z) = 1/2.

3. Asymptotic behavior: For large values of the real part of n, Γ(n) behaves like (n/e)^n * sqrt(2πn).

4. Analytic continuation: The Gamma function can be extended to an analytic function on the entire complex plane, except for non-positive integers, where it has simple poles.

The Gamma function is used in various mathematical applications, such as solving integrals, evaluating infinite series, and studying the distribution of prime numbers. It also appears in the solution of many problems in physics and engineering, particularly in the context of special functions and probability distributions."
731,How many ways are there to partition a set of 5 elements into 3 non-empty cycles?,35,integer,NONE,self,jianyu_xu/Stirling_number_first_kind_1.json,stirling number of the first kind,Combinatorics,Math,"Stirling Numbers of the first kind, denoted by S(n, k) or sometimes by s(n, k), are a set of numbers that arise in combinatorics, the study of counting and arranging objects. They are named after the Scottish mathematician James Stirling. These numbers are used to count the number of permutations of n elements with exactly k cycles.

A cycle in a permutation is a subset of elements where each element is replaced by another element in the subset, and the last element is replaced by the first element. For example, in the permutation (1, 3, 2), there are two cycles: (1) and (3, 2), where 3 replaces 2 and 2 replaces 3.

Stirling Numbers of the first kind can be defined recursively using the following formula:

S(n, k) = (n - 1) * S(n - 1, k) + S(n - 1, k - 1)

with the initial conditions:

S(n, 0) = 0 for n > 0,
S(0, 0) = 1,
S(n, n) = 1 for n > 0.

The first few Stirling Numbers of the first kind are:

S(1, 1) = 1
S(2, 1) = 0
S(2, 2) = 1
S(3, 1) = 0
S(3, 2) = 3
S(3, 3) = 1

These numbers have various applications in combinatorics, such as counting permutations with a given number of cycles, analyzing algorithms, and solving problems in number theory."
263,Use Green's Theorem to evaluate $\oiint_{s} y^3 dx + x^3dy$ where $C$ is the positively oriented circle of radius 2 centered at origin.,-75.396,float,NONE,website | https://tutorial.math.lamar.edu/classes/calciii/GreensTheorem.aspx,wenhuchen/green2.json,green's theorem,Calculus,Math,"Green's theorem is a fundamental result in vector calculus that relates a line integral around a simple closed curve C to a double integral over the plane region D bounded by C. It is named after the British mathematician George Green and is a powerful tool for evaluating line integrals and calculating the circulation and flux of vector fields.

Mathematically, Green's theorem can be stated as follows:

Let C be a positively oriented, piecewise-smooth, simple closed curve in the plane, and let D be the region bounded by C. If P(x, y) and Q(x, y) have continuous partial derivatives on an open region that contains D, then

∮[P dx + Q dy] = ∬[∂Q/∂x - ∂P/∂y] dA

where the left-hand side represents the line integral of the vector field F = <P, Q> around the curve C, and the right-hand side represents the double integral of the scalar function (∂Q/∂x - ∂P/∂y) over the region D.

In simple terms, Green's theorem states that the line integral of a vector field around a closed curve is equal to the double integral of the curl of the vector field over the region enclosed by the curve. This theorem has important applications in physics and engineering, particularly in the study of fluid dynamics, electromagnetism, and heat conduction."
622,A positive-definite kernel function satisfies the Cauchy-Schwartz inequality. True or false?,True,bool,NONE,textbook 6.15,xinyi/kernel_2.json,kernel method,Machine learning,EECS,"The Kernel Method, also known as the Kernel Trick or Kernel Functions, is a technique used in machine learning for transforming and processing data in a higher-dimensional space. It is particularly useful in algorithms that rely on the inner product between data points, such as Support Vector Machines (SVM) and Principal Component Analysis (PCA).

The main idea behind the Kernel Method is to map the original data into a higher-dimensional space where it becomes more easily separable or where patterns and relationships become more apparent. This is achieved by using a kernel function, which computes the inner product between the transformed data points in the higher-dimensional space without explicitly calculating the transformation.

There are several types of kernel functions, such as linear, polynomial, radial basis function (RBF), and sigmoid kernels. Each kernel function has its own properties and is suitable for different types of data and problems.

In summary, the Kernel Method is a powerful technique in machine learning that allows for efficient processing and analysis of data by transforming it into a higher-dimensional space using kernel functions. This enables better separation and understanding of complex patterns in the data, ultimately improving the performance of machine learning algorithms."
740,"For an integer a > 0 and an integer b > 0, is there any other number c > 0 such that a^10 + b^10 = c^10?",False,bool,NONE,self,wenhuchen/fermat_last.json,fermat's last theorem,Number theory,Math,"Fermat's Last Theorem is a statement in number theory that was first proposed by the French mathematician Pierre de Fermat in 1637. It states that no three positive integers a, b, and c can satisfy the equation a^n + b^n = c^n for any integer value of n greater than 2.

In mathematical notation, the theorem can be written as:

a^n + b^n ≠ c^n, for all positive integers a, b, c, and n with n > 2.

Fermat claimed to have a proof for this theorem, but he never wrote it down, and it remained unproven for more than 300 years. The theorem became one of the most famous unsolved problems in mathematics, attracting the attention of numerous mathematicians who attempted to find a proof.

In 1994, the British mathematician Andrew Wiles finally proved Fermat's Last Theorem, using advanced mathematical techniques from algebraic geometry and elliptic curves. Wiles' proof was published in 1995, and he was awarded the Abel Prize in 2016 for his groundbreaking work on this problem."
157,"Calculate the Hamming pairwise distances and determine the minimum Hamming distance among the following codewords: 00000,10101,01010",2,integer,NONE,website | https://vinesmsuic.github.io/notes-networking2-qa/#Problems-3,maxku/ipnetwork13-hammingdist.json,hamming distance,Computer networking,EECS,"Hamming distance is a concept in computer networking and information theory that measures the difference between two strings of equal length by counting the number of positions at which the corresponding symbols (bits) are different. It is named after Richard Hamming, who introduced the concept in the context of error-detecting and error-correcting codes.

In computer networking, Hamming distance is particularly useful for detecting and correcting errors in data transmission. When data is transmitted over a network, it can be corrupted due to noise, interference, or other factors. By using error-correcting codes with a certain minimum Hamming distance, it is possible to detect and correct errors that occur during transmission.

For example, consider two binary strings of equal length:

String 1: 11010101
String 2: 10011011

The Hamming distance between these two strings is 4, as there are four positions at which the bits are different (positions 2, 4, 6, and 7).

In the context of error-correcting codes, a code with a minimum Hamming distance of d can detect up to (d-1) bit errors and correct up to floor((d-1)/2) bit errors. So, a code with a minimum Hamming distance of 4 can detect up to 3 bit errors and correct up to 1 bit error."
200,"Assuming we are underground, and the only thing we can observe is whether a person brings an umbrella or not. The weather could be either rain or sunny. Assuming the P(rain)=0.6 and P(sunny)=0.4. Assuming the weather on day $k$ is dependent on the weather on day $k-1$. We can write the transition probability as P(sunny $\mid$ sunny) = P(rain $\mid$ rain) = 0.7. The person has 60% chance to bring an umbrella when the weather is rain, and 40% chance to bring an umbrella when the weather is sunny, i.e. P(umbrella $\mid$ rain) = 0.6 and P(umbrella $\mid$ sunny) = 0.4. If we observe that the person (1) brought an umbrella on day 1, (2) did not bring an umbrella on day 2, (3) brought an umbrella on day 3.  What is the probability that day 2 is raining?",0.5167,float,NONE,self,wenhuchen/hmm_smoothing1.json,forward-backward algorithm,Stochastic process,Math,"The Forward-Backward Algorithm is a dynamic programming algorithm used in Hidden Markov Models (HMMs) to compute the posterior probabilities of hidden states given a sequence of observations. It is a stochastic process that combines both the forward and backward algorithms to efficiently compute these probabilities.

The algorithm consists of two main steps:

1. Forward Algorithm:
The forward algorithm computes the probability of observing a particular sequence of observations up to a certain time step, given the hidden state at that time step. It calculates the forward probabilities, which are the joint probabilities of the observed sequence and the hidden state at each time step. The forward algorithm uses a recursive approach, where the forward probability at each time step is calculated based on the forward probabilities of the previous time step.

2. Backward Algorithm:
The backward algorithm computes the probability of observing the remaining sequence of observations from a certain time step onwards, given the hidden state at that time step. It calculates the backward probabilities, which are the conditional probabilities of the future observations given the hidden state at each time step. Similar to the forward algorithm, the backward algorithm also uses a recursive approach, where the backward probability at each time step is calculated based on the backward probabilities of the next time step.

After computing the forward and backward probabilities, the Forward-Backward Algorithm combines these probabilities to calculate the posterior probabilities of the hidden states at each time step. The posterior probability of a hidden state at a particular time step is the probability of that state given the entire sequence of observations. This is computed by multiplying the forward probability and the backward probability for that state at that time step and then normalizing the result.

The Forward-Backward Algorithm is widely used in various applications, such as speech recognition, natural language processing, and bioinformatics, where the goal is to infer the most likely sequence of hidden states given a sequence of observations."
369,"Clare manages a piano store. Her utility function is given by Utility = w - 100, where w is the total of all monetary payments to her and 100 represents the monetary equivalent of the disutility of exerting effort to run the store. Her next best alternative to managing the store gives her zero utility. The store's revenue depends on random factors, with an equal chance of being $1,000 or $400. If shareholders offered to share half of the store's revenue with her, what would her expected utility be?",250,integer,NONE,"text | Walter Nicholson, Microeconomic Theory Basic Principles and Extensions.",elainewan/econ_micro_18.json,expected utility,Economics,Finance,"Expected Utility is a concept in economics and decision theory that refers to the total satisfaction or value that an individual expects to receive from a particular choice or decision, taking into account the probabilities of different outcomes. It is a key concept in understanding how people make decisions under uncertainty and is widely used in various fields, including finance, insurance, and game theory.

The Expected Utility Theory assumes that individuals are rational decision-makers who aim to maximize their utility or satisfaction. When faced with multiple options, individuals will choose the one that provides the highest expected utility. This means that they will weigh the potential benefits and costs of each option, considering the likelihood of each outcome occurring.

To calculate the expected utility of a decision, one must:

1. Identify all possible outcomes of the decision.
2. Assign a utility value to each outcome, representing the satisfaction or value that the individual would receive from that outcome.
3. Determine the probability of each outcome occurring.
4. Multiply the utility value of each outcome by its probability.
5. Sum the products of the utility values and probabilities to obtain the expected utility of the decision.

By comparing the expected utilities of different options, individuals can make informed choices that maximize their overall satisfaction.

It is important to note that the concept of expected utility is based on subjective evaluations of utility and probabilities, which may vary from person to person. Additionally, the theory assumes that individuals have perfect information and can accurately assess probabilities and utility values, which may not always be the case in real-world situations. Despite these limitations, expected utility remains a fundamental concept in understanding decision-making under uncertainty."
175,"If the peak voltage value of a signal is 20 times the peak voltage value of the noise, what is the SNR? What is the $\mathrm{SNR}_{\mathrm{dB}}$ (in 3 sig.fig.)?",26.0,float,NONE,website | https://vinesmsuic.github.io/notes-networking1-phy/#Problems,maxku/signalprocessing15-DB.json,sound level,Signal processing,EECS,"Sound level in signal processing refers to the measurement of the intensity or amplitude of an audio signal, usually expressed in decibels (dB). It is an important aspect of audio processing, as it helps in understanding the loudness or softness of a sound and plays a crucial role in various applications such as audio mixing, noise reduction, and audio compression.

In signal processing, the sound level is often represented as a time-varying waveform, where the amplitude of the waveform corresponds to the instantaneous sound pressure level. The waveform can be analyzed in both time and frequency domains to extract useful information about the audio signal.

There are several ways to measure the sound level in signal processing:

1. Peak level: This is the maximum amplitude of the audio signal, which represents the highest sound pressure level in the waveform.

2. RMS (Root Mean Square) level: This is a more accurate representation of the average sound level, as it takes into account both positive and negative amplitude values. It is calculated by squaring the amplitude values, taking the average of the squared values, and then finding the square root of the average.

3. A-weighted level: This is a frequency-weighted sound level measurement that approximates the human ear's sensitivity to different frequencies. It is commonly used in noise measurement and environmental noise assessments.

4. Loudness level: This is a psychoacoustic measure that takes into account the human perception of loudness. It is usually calculated using algorithms such as the ITU-R BS.1770 standard, which considers the frequency content, duration, and amplitude of the audio signal.

In summary, sound level in signal processing is a crucial parameter that helps in understanding and manipulating audio signals for various applications. It is typically measured in decibels and can be analyzed using different methods to obtain information about the loudness, frequency content, and other characteristics of the audio signal."
29,"assume you are Indonesian. In 2010, the rupiah exchange rate was around IDR15,000/USD, and the consumer price index in Indonesia and the United States was at 100. In 2019, the exchange rate changed to IDR14,000/USD. Simultaneously, Indonesia’s inflation rose 5% due to the consumer price index rising to 105. Meanwhile, the United States’ inflation rate rose 10% due to the consumer price index rising to 110. Whats the real exchange rate?",14666.67,float,NONE,website | https://penpoin.com/real-exchange-rate/,xueguangma/real_exchange_rate.json,real exchange rate,Economics,Finance,"The Real Exchange Rate (RER) in economics refers to the relative value of one country's currency in terms of another country's currency, adjusted for differences in price levels or inflation rates between the two countries. It is a measure of the purchasing power of one currency against another and is used to compare the cost of goods and services across countries.

The RER is calculated by taking the nominal exchange rate (the rate at which one currency can be exchanged for another) and adjusting it for the difference in inflation rates between the two countries. This adjustment is necessary because inflation affects the overall price level in a country, which in turn affects the value of its currency.

A higher RER indicates that a country's currency has more purchasing power, meaning that goods and services in that country are relatively cheaper compared to those in other countries. Conversely, a lower RER indicates that a country's currency has less purchasing power, making goods and services relatively more expensive compared to other countries.

The Real Exchange Rate is important for several reasons:

1. It helps determine a country's competitiveness in international trade. A lower RER can make a country's exports more attractive to foreign buyers, while a higher RER can make imports cheaper for domestic consumers.

2. It can influence investment decisions, as investors may be more likely to invest in countries with lower RERs, where their investments can potentially yield higher returns.

3. It can impact economic growth, as changes in the RER can affect a country's trade balance, which in turn can influence overall economic growth.

4. It can affect the stability of a country's currency, as large fluctuations in the RER can lead to currency crises or speculative attacks on a currency.

In summary, the Real Exchange Rate is a crucial economic indicator that reflects the relative value and purchasing power of a country's currency compared to another, taking into account differences in price levels or inflation rates. It plays a significant role in international trade, investment decisions, economic growth, and currency stability."
525,"Let $g_\theta(x_1,x_2)=f_\theta(x_1)f_\theta(x_2)$. Let $J_f(\theta)$ be the Fisher information of $f_\theta$.  What is the relationship between $J_f(\theta)$ and $J_g(\theta)$? (a) $J_g(\theta) = 0.5J_f(\theta)$. (b) $J_g(\theta) = J_f(\theta)$. (c) $J_g(\theta) = 2J_f(\theta)$. (d) $J_g(\theta) = 4J_f(\theta)$. Which option is correct?",(c),option,NONE,textbook 11.9,xinyi/fisher_information_4.json,fisher information,Statistics,Math,"Fisher information is a statistical concept used to measure the amount of information that a set of observed data carries about an unknown parameter of the underlying probability distribution. It is named after the British statistician Ronald A. Fisher, who introduced the concept in the context of maximum likelihood estimation.

Fisher information is particularly useful in the field of parameter estimation, as it helps to quantify the precision with which an unknown parameter can be estimated from the given data. It is closely related to the Cramér-Rao lower bound, which states that the variance of any unbiased estimator of the parameter cannot be smaller than the inverse of the Fisher information.

Mathematically, Fisher information is defined as the expected value of the second derivative (with respect to the parameter) of the log-likelihood function, or equivalently, as the expected value of the squared first derivative of the log-likelihood function. For a probability distribution with parameter θ and likelihood function L(θ), the Fisher information I(θ) can be expressed as:

I(θ) = E[(-d²/dθ²) log L(θ)] = E[(d/dθ log L(θ))²]

In simple terms, Fisher information quantifies how sensitive the likelihood function is to changes in the parameter. A higher Fisher information indicates that the data provides more information about the parameter, leading to more precise estimates. Conversely, a lower Fisher information suggests that the data is less informative, resulting in less precise estimates.

Fisher information plays a crucial role in various statistical methods, including hypothesis testing, confidence intervals, and Bayesian inference. It is an essential tool for understanding the relationship between data and the underlying parameters of a probability distribution."
603,What's the maximum number of edges in a simple triangle free planar graph with 30 vertices?,56,integer,NONE,Self,tonyxia/maxplanar2.json,maximal planar graph,Graph theory,EECS,"A Maximal Planar Graph, in graph theory, is a type of graph that is planar and cannot have any more edges added to it without losing its planarity. In other words, it is a graph that can be drawn on a plane without any of its edges crossing, and adding any more edges would cause at least one crossing.

Here are some key properties of maximal planar graphs:

1. Every face (region enclosed by edges) in a maximal planar graph is a triangle. This is because if there is a face with more than three sides, we can add an edge between two non-adjacent vertices of that face without violating planarity, which contradicts the maximality of the graph.

2. A maximal planar graph with 'n' vertices (n ≥ 3) has exactly 3n - 6 edges. This can be derived from Euler's formula for planar graphs, which states that for any connected planar graph, the number of vertices (V), edges (E), and faces (F) are related by the equation V - E + F = 2.

3. A maximal planar graph is also called a triangulation, as it can be seen as a way to divide a planar region into triangles by adding edges.

4. A maximal planar graph is always 3-connected, meaning that it remains connected even after removing any two vertices and their incident edges. This is because removing a vertex from a maximal planar graph results in a planar graph with a single face that is a polygon, which can always be triangulated.

5. Every maximal planar graph is a subgraph of some complete graph, where a complete graph is a graph in which every pair of distinct vertices is connected by a unique edge.

In summary, a maximal planar graph is a planar graph that is ""as connected as possible"" without losing its planarity. It has several interesting properties, such as every face being a triangle and having a fixed relationship between the number of vertices and edges."
485,"Suppose Host A wants to send a large file to Host B. The path from Host A to Host B has three links, of rates R1 = 500 kbps, R2 = 2 Mbps, and R3 = Mbps. Assuming no other traffic in the network, what is the throughput for the file transfer? (in kbps)",500,integer,NONE,website | https://vinesmsuic.github.io/notes-networkingIP-qa/,maxku/ipnetwork10-datatransmission.json,data communication,Computer networking,EECS,"Data Communication, also known as computer networking, is the process of exchanging information and data between multiple computing devices through a communication channel. This exchange of data can occur over wired or wireless connections, and the devices can be located in close proximity or across vast distances. The primary goal of data communication is to enable the sharing of resources, information, and services among different users and systems.

In computer networking, data is transmitted in the form of packets, which are small units of data that are sent from one device to another. These packets are transmitted through various networking devices, such as routers, switches, and hubs, which help direct the data to its intended destination.

There are several key components and concepts in data communication:

1. Nodes: These are the devices that participate in the network, such as computers, servers, printers, and smartphones.

2. Network Topology: This refers to the arrangement and interconnection of nodes in a network. Common network topologies include bus, star, ring, and mesh.

3. Protocols: These are the rules and conventions that govern how data is transmitted and received in a network. Examples of protocols include TCP/IP, HTTP, and FTP.

4. Transmission Media: This is the physical medium through which data is transmitted, such as copper wires, fiber optic cables, or wireless radio frequencies.

5. Bandwidth: This refers to the capacity of a communication channel to transmit data. It is usually measured in bits per second (bps) or bytes per second (Bps).

6. Latency: This is the time it takes for a data packet to travel from its source to its destination. Lower latency is generally preferred for faster communication.

7. Network Security: This involves protecting the network and its data from unauthorized access, misuse, or attacks.

Data communication plays a crucial role in modern society, enabling various applications such as the internet, email, file sharing, online gaming, and video conferencing. As technology continues to advance, data communication networks are becoming faster, more reliable, and more secure, allowing for even greater connectivity and collaboration among users and devices."
329,What is the number of equivalent parameter settings due to interchange symmetries in a mixture model with 10 components?,3628800,integer,NONE,textbook 10.21 | 10!,xinyi/mixture_model.json,mixture model,Probability theory,Math,"A mixture model is a probabilistic model in probability theory that represents the presence of multiple subpopulations within an overall population, without requiring that an observed data set should identify the subpopulation to which an individual observation belongs. In other words, it is a model that combines several probability distributions to describe the variability in a data set.

Mixture models are often used in unsupervised learning and clustering tasks, where the goal is to identify the underlying structure or patterns in the data without any prior knowledge of the subpopulations.

The basic idea behind a mixture model is that the observed data is generated by a combination of several underlying probability distributions. Each of these distributions represents a subpopulation or a group within the overall population. The mixture model aims to estimate the parameters of these underlying distributions and the proportions of each subpopulation in the overall population.

Mathematically, a mixture model can be represented as:

P(x) = ∑ (π_i * P_i(x))

where P(x) is the overall probability density function (PDF) of the observed data, π_i is the proportion of the i-th subpopulation in the overall population, P_i(x) is the PDF of the i-th subpopulation, and the summation is over all the subpopulations.

A common example of a mixture model is the Gaussian Mixture Model (GMM), where the underlying probability distributions are assumed to be Gaussian (normal) distributions. In this case, the goal is to estimate the means, variances, and proportions of each Gaussian component in the mixture.

Mixture models can be fitted to the data using various algorithms, such as the Expectation-Maximization (EM) algorithm, which iteratively refines the estimates of the parameters and proportions of the subpopulations until convergence is reached."
367,What is the smallest number of vertices in a graph that guarantees the existence of a clique of size 3 or an independent set of size 2?,3,integer,NONE,self,jianyu_xu/Graph_1.json,ramsey's theorem,Combinatorics,Math,"Ramsey's theorem is a fundamental result in combinatorics, specifically in the area of graph theory and combinatorial mathematics. It is named after the British mathematician Frank P. Ramsey, who first stated the theorem in 1930. The theorem deals with the conditions under which order must appear in a large enough structure, even if that structure is initially disordered or chaotic.

In its simplest form, Ramsey's theorem states that for any given positive integers m and n, there exists a least positive integer R(m, n) such that any graph with at least R(m, n) vertices will contain either a clique of size m (a complete subgraph where every pair of vertices is connected by an edge) or an independent set of size n (a set of vertices where no two vertices are connected by an edge).

In other words, if you have a large enough graph, it is impossible to avoid having either a large complete subgraph or a large independent set, regardless of how the edges are arranged.

Ramsey's theorem can also be extended to more complex structures, such as hypergraphs and infinite graphs, and can be generalized to deal with multiple colors or more complicated combinatorial objects. The theorem has important applications in various fields, including computer science, logic, and number theory.

However, despite its significance, Ramsey's theorem is known for its non-constructive nature, meaning that it guarantees the existence of a certain structure but does not provide an explicit way to find or construct it. Additionally, the bounds for R(m, n) are often very large and difficult to compute, which limits the practical applications of the theorem."
183,"Suppose H is a Banach space, and {x_n}\in H, x\in H. Then x_n weakly converges to x is equivalent to: ||x_n|| is bounded; for a dense set M* in H*, it holds \lim_{n\rightarrow\infty} f(x_n)=f(x) for all f\in M*. Is this correct? Answer 1 for yes and 0 for no.",1.0,float,NONE,functional analysis: theorem 2.5.20,mingyin/Banach-Steinhaus-theorem1.json,banach-steinhaus theorem,Functional analysis,Math,"The Banach-Steinhaus theorem, also known as the Uniform Boundedness Principle, is a fundamental result in functional analysis, a branch of mathematics that deals with the study of vector spaces and linear operators. The theorem provides a powerful tool for understanding the behavior of families of linear operators acting on Banach spaces, which are complete normed vector spaces.

Statement of the theorem:

Let X be a Banach space, Y be a normed vector space, and let F be a family of continuous linear operators from X to Y. If for every x in X, the set {||Tx|| : T in F} is bounded, then there exists a constant C such that ||T|| <= C for all T in F.

In simpler terms, the theorem states that if for every element x in the Banach space X, the norms of the images of x under the operators in F are uniformly bounded, then the operator norms of the operators in F are also uniformly bounded.

The significance of the Banach-Steinhaus theorem lies in its ability to provide information about the boundedness of a whole family of operators based on the boundedness of their action on individual elements of the space. This result has important applications in various areas of mathematics, including partial differential equations, harmonic analysis, and the study of infinite-dimensional spaces."
400,"For the 3 payments of $1000 each end-of-year, with 7% rate of return, what is the present value if the first payment is made at the end of fifth year?",2002.0781,float,NONE,self,xueguangma/present_value_2.json,present value,Quantitive methods,Finance,"Present Value (PV) is a concept in finance and quantitative methods that refers to the current worth of a future sum of money or a series of cash flows, given a specified rate of return or discount rate. It is based on the principle of time value of money, which states that a dollar received today is worth more than a dollar received in the future, due to the earning potential of the money if it is invested or saved.

The Present Value is calculated using the following formula:

PV = CF / (1 + r)^n

Where:
- PV is the present value
- CF is the future cash flow (or a series of cash flows)
- r is the discount rate (or rate of return)
- n is the number of periods (e.g., years) until the cash flow occurs

The Present Value calculation is used in various financial applications, such as investment analysis, capital budgeting, and financial planning. By comparing the present value of different investment options or projects, decision-makers can determine which option provides the highest return or is the most cost-effective.

In summary, Present Value is a quantitative method that helps in evaluating the current worth of future cash flows, taking into account the time value of money and the opportunity cost of not having the funds available for investment or other purposes today."
635,"The cross section for a 2.0-MeV neutron (a typical energy for a neutron released in fission) being absorbed by a U-238 nucleus and producing fission is 0.68 barn. For a pure U-238 sample of thickness 3.2 cm, what is the probability of a 2.0-MeV neutron producing fission?",0.1,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Exercise 13.3,tonyxia/nuclear1.json,nuclear physics,Atomic physics,Physics,"Nuclear physics, also known as atomic physics, is a branch of physics that deals with the study of atomic nuclei and their interactions. It focuses on understanding the properties, behavior, and structure of atomic nuclei, as well as the forces that hold protons and neutrons together within the nucleus.

The key components of nuclear physics include:

1. Nuclear structure: This involves the study of the arrangement of protons and neutrons within the nucleus, as well as the energy levels and quantum states of these particles. Nuclear structure also explores the various models that describe the nucleus, such as the shell model and the liquid drop model.

2. Nuclear reactions: These are processes in which atomic nuclei undergo changes, such as fusion (combining of nuclei), fission (splitting of nuclei), and radioactive decay (spontaneous transformation of a nucleus into another). Nuclear reactions are responsible for the release of energy in nuclear power plants and the functioning of nuclear weapons.

3. Nuclear forces: The strong nuclear force, also known as the strong interaction, is the force that holds protons and neutrons together within the nucleus. It is one of the four fundamental forces of nature and is responsible for the stability of atomic nuclei. Nuclear forces also include the weak nuclear force, which is responsible for certain types of radioactive decay.

4. Radioactivity: This is the spontaneous emission of particles or electromagnetic radiation from unstable atomic nuclei. There are several types of radioactive decay, including alpha decay, beta decay, and gamma decay. Radioactivity plays a crucial role in various applications, such as medical imaging, cancer treatment, and dating of archaeological artifacts.

5. Particle physics: Nuclear physics overlaps with particle physics, which studies the fundamental particles that make up the universe and their interactions. This includes the study of quarks, which are the building blocks of protons and neutrons, as well as other subatomic particles like neutrinos and mesons.

Overall, nuclear physics is a vital field of study that has contributed significantly to our understanding of the universe and has numerous practical applications in energy production, medicine, and technology."
461,Does \lim_{x \to 0} (cos(mx - 1)/(x^2) = -(m^2)/2 for m = 2?,True,bool,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_8.json,trigonometric limits,Calculus,Math,"Trigonometric limits in calculus refer to the limits involving trigonometric functions, such as sine, cosine, tangent, cotangent, secant, and cosecant. These limits are essential in the study of calculus, as they help in understanding the behavior of trigonometric functions as the input (angle) approaches a particular value.

Some common trigonometric limits are:

1. Limit of sin(x)/x as x approaches 0:
lim (x→0) [sin(x)/x] = 1

This limit is fundamental in calculus and is derived using the Squeeze Theorem or L'Hopital's Rule.

2. Limit of (1 - cos(x))/x as x approaches 0:
lim (x→0) [(1 - cos(x))/x] = 0

3. Limit of tan(x)/x as x approaches 0:
lim (x→0) [tan(x)/x] = 1

4. Limit of (sin(ax) - sin(bx))/x as x approaches 0:
lim (x→0) [(sin(ax) - sin(bx))/x] = a - b

These trigonometric limits are used in various applications of calculus, such as finding derivatives and integrals of trigonometric functions, solving differential equations, and analyzing the behavior of functions in real-world problems. Understanding these limits is crucial for mastering calculus and its applications in science, engineering, and mathematics."
199,"Consider the matrix of A=[[1, -1], [-1, 4]], is this a positive definite matrix?",True,bool,NONE,website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf,wenhuchen/definite_matrix2.json,definite matrix criteria,Algebra,Math,"In algebra, a definite matrix is a square matrix that has certain properties related to its eigenvalues or determinants. These properties help classify the matrix as positive definite, negative definite, positive semi-definite, or negative semi-definite. Here are the criteria for each type of definite matrix:

1. Positive Definite Matrix:
A square matrix A is positive definite if:
   a. All its eigenvalues are positive.
   b. All its leading principal minors (determinants of the top-left submatrices) are positive.
   c. For any non-zero vector x, the quadratic form x^T * A * x is positive, i.e., x^T * A * x > 0.

2. Negative Definite Matrix:
A square matrix A is negative definite if:
   a. All its eigenvalues are negative.
   b. Its leading principal minors alternate in sign, starting with a negative determinant for the first order minor.
   c. For any non-zero vector x, the quadratic form x^T * A * x is negative, i.e., x^T * A * x < 0.

3. Positive Semi-Definite Matrix:
A square matrix A is positive semi-definite if:
   a. All its eigenvalues are non-negative (positive or zero).
   b. All its leading principal minors are non-negative.
   c. For any vector x, the quadratic form x^T * A * x is non-negative, i.e., x^T * A * x ≥ 0.

4. Negative Semi-Definite Matrix:
A square matrix A is negative semi-definite if:
   a. All its eigenvalues are non-positive (negative or zero).
   b. Its leading principal minors alternate in sign, starting with a non-positive determinant for the first order minor.
   c. For any vector x, the quadratic form x^T * A * x is non-positive, i.e., x^T * A * x ≤ 0.

These criteria help determine the definiteness of a matrix, which is useful in various applications, such as optimization problems, stability analysis, and solving linear systems."
89,"Perform 2 iterations with the Müller method for the following equation: x^3 - 1/2 = 0, x_0 = 0, x_1 = 1, x_2 = 1/2. What's the decimal value of x_3?",0.7929,float,NONE,website | http://ndl.ethernet.edu.et/bitstream/123456789/79528/3/Numerical_Methods_Problems_and_Solutions_cropped.pdf,wenhuchen/mueller1.json,mueller's algorithm,Numerical analysis,Math,"Mueller's Algorithm is a numerical analysis method used for finding the roots (or zeros) of a real-valued function. It is an iterative method that generalizes the secant method and is particularly useful for finding complex roots of a function. The algorithm was developed by Peter Müller in 1956.

The main idea behind Mueller's Algorithm is to use three points (x0, x1, x2) on the function f(x) to approximate the function with a parabola (quadratic function) that passes through these points. The roots of this parabola are then used as the next approximations for the roots of the original function f(x).

Here's a step-by-step description of Mueller's Algorithm:

1. Choose three initial points x0, x1, and x2, such that f(x0), f(x1), and f(x2) are not equal to zero and x0, x1, and x2 are distinct.

2. Fit a parabola through the points (x0, f(x0)), (x1, f(x1)), and (x2, f(x2)). This can be done by solving a system of linear equations to find the coefficients a, b, and c of the quadratic function Q(x) = a(x - x2)^2 + b(x - x2) + c.

3. Find the roots of the quadratic function Q(x). These roots can be complex, and they are given by the quadratic formula: x = (-b ± √(b^2 - 4ac)) / 2a.

4. Choose one of the roots of Q(x) as the next approximation x3 for the root of f(x). The choice is usually based on which root is closer to x2 or which one has a smaller absolute value of f(x).

5. Update the points: x0 = x1, x1 = x2, and x2 = x3.

6. Check for convergence. If the difference between successive approximations (|x2 - x1| and |x1 - x0|) or the function value at the approximation (|f(x2)|) is smaller than a predefined tolerance, the algorithm has converged, and x2 is considered an approximation of the root. Otherwise, go back to step 2 and continue iterating.

Mueller's Algorithm is particularly useful for finding complex roots, as it can handle complex numbers in its calculations. However, like other iterative methods, it is not guaranteed to converge, and the choice of initial points can significantly affect the algorithm's performance."
368,"Let N be a spatial Poisson process with constant intensity $11$ in R^d, where d\geq2. Let S be the ball of radius $r$ centered at zero.  Denote |S| to be the volume of the ball. What is N(S)/|S| as $r\rightarrow\infty$?",11.0,float,NONE,probability: 7.4.3,mingyin/strong-law-of-large-number1.json,law of large number,Statistics,Math,"The Law of Large Numbers (LLN) is a fundamental concept in probability and statistics that states that as the sample size (number of observations) increases, the average of the sample values will approach the expected value (mean) of the underlying population. In other words, the larger the sample size, the more likely it is that the sample mean will be close to the population mean.

This law is based on the idea that random fluctuations and outliers have less impact on the overall average as the sample size grows. The Law of Large Numbers is essential in statistical analysis, as it helps to ensure that the results obtained from a sample are representative of the entire population.

There are two versions of the Law of Large Numbers:

1. Weak Law of Large Numbers (WLLN): This version states that the probability of the sample mean deviating from the population mean by more than a specified amount approaches zero as the sample size increases.

2. Strong Law of Large Numbers (SLLN): This version states that the sample mean will almost surely converge to the population mean as the sample size goes to infinity, meaning that the probability of the sample mean not converging to the population mean is zero.

In summary, the Law of Large Numbers highlights the importance of having a large sample size in statistical analysis, as it ensures that the results obtained are more likely to be representative of the entire population."
315,"For a $1,000 investment, what is the future value of the investment if the interest rate is 8% compounded annually for 3 years?",1259.71,float,NONE,self,xueguangma/future_value_1.json,future value,Quantitive methods,Finance,"Future Value (FV) is a concept in finance and quantitative methods that refers to the estimated value of an investment or cash flow at a specific point in the future. It is based on the principle of time value of money, which states that a dollar today is worth more than a dollar in the future due to its potential earning capacity.

The Future Value calculation takes into account the initial investment amount, the interest rate, and the number of compounding periods to determine how much an investment will be worth in the future. The formula for calculating Future Value is:

FV = PV * (1 + r)^n

Where:
- FV is the Future Value of the investment
- PV is the Present Value or the initial investment amount
- r is the interest rate per compounding period (expressed as a decimal)
- n is the number of compounding periods

The Future Value calculation is essential in various financial applications, such as retirement planning, investment analysis, and capital budgeting. It helps investors and financial analysts to estimate the potential growth of an investment over time and make informed decisions about where to allocate their resources."
166,"A pure lead bar 10 cm long is maintained with one end at T &=300 K and the other at 310 K. The thermoelectric potential difference thus induced across the ends is 12.8 micro-volts. Find the thermoelectric power for lead in this temperature range in V/K. (Note: Q varies nonlinearly with temperature, but over this narrow temperature range, you may use a linear approximation.)",1.28e-06,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Example 11.5,tonyxia/semiconductor1.json,semiconductor theory,Condensed matter physics,Physics,"Semiconductor Theory is a branch of condensed matter physics that deals with the study of semiconductors, which are materials that have electrical conductivity between that of insulators and conductors. These materials have unique properties that make them essential for various electronic devices, such as transistors, diodes, and solar cells.

The fundamental concept in semiconductor theory is the electronic band structure, which describes the energy levels available to electrons in a solid material. In a semiconductor, there are two important bands: the valence band and the conduction band. The valence band is filled with electrons, while the conduction band is initially empty. The energy gap between these two bands is called the bandgap.

In insulators, the bandgap is large, making it difficult for electrons to move from the valence band to the conduction band. In conductors, the valence and conduction bands overlap, allowing electrons to move freely. Semiconductors have a small bandgap, which means that electrons can be excited from the valence band to the conduction band under certain conditions, such as the application of heat or light.

There are two main types of semiconductors: intrinsic and extrinsic. Intrinsic semiconductors are pure materials, such as silicon or germanium, with no impurities. In these materials, the number of electrons in the conduction band is equal to the number of holes (empty spaces) in the valence band. The electrical conductivity of intrinsic semiconductors is relatively low.

Extrinsic semiconductors are created by introducing impurities, or dopants, into the intrinsic semiconductor. This process, called doping, can create either n-type or p-type semiconductors. In n-type semiconductors, the dopant atoms have more valence electrons than the semiconductor atoms, resulting in an excess of free electrons. In p-type semiconductors, the dopant atoms have fewer valence electrons, creating an excess of holes. The interaction between n-type and p-type materials forms the basis for many electronic devices, such as diodes and transistors.

Semiconductor theory also involves the study of various physical phenomena, such as carrier transport (how electrons and holes move through the material), recombination (the process by which electrons and holes combine), and the behavior of semiconductors under different conditions, such as temperature and illumination.

Overall, semiconductor theory is a crucial area of condensed matter physics that has led to the development of modern electronics and continues to drive advancements in technology."
591,"Suppose we have the following differential equation with the initial condition: $\frac{\partial p}{\partial x} = 0.5 * x * (1-x)$ and $p(0)=2$. Use Euler's method to approximate p(2), using step of 1.",2.0,float,NONE,website | https://www.varsitytutors.com/calculus_2-help/euler-s-method,wenhuchen/euler's_method1.json,euler's method,Numerical analysis,Math,"Euler's Method is a numerical analysis technique used to approximate the solution of ordinary differential equations (ODEs) with a given initial value. It is named after the Swiss mathematician Leonhard Euler, who introduced the method in the 18th century. Euler's Method is considered a first-order method, meaning that its accuracy is proportional to the step size used in the calculations.

The method works by iteratively generating a sequence of points that approximate the solution curve of the ODE. Given an initial value problem of the form:

dy/dx = f(x, y)
y(x0) = y0

where f(x, y) is a function of x and y, and (x0, y0) is the initial condition, Euler's Method proceeds as follows:

1. Choose a step size, h, which determines the increments in the x-direction for the approximation.
2. Calculate the next point (x1, y1) using the formula:

   x1 = x0 + h
   y1 = y0 + h * f(x0, y0)

3. Repeat the process for a desired number of steps or until a specific endpoint is reached, using the previously calculated point as the new initial condition:

   xi+1 = xi + h
   yi+1 = yi + h * f(xi, yi)

The accuracy of Euler's Method depends on the choice of step size, h. Smaller step sizes generally yield more accurate results but require more computational effort. It is important to note that Euler's Method may not be suitable for all types of ODEs, particularly those with rapidly changing or unstable solutions. In such cases, more advanced numerical methods, such as Runge-Kutta methods, may be more appropriate."
389,Find the sum of $\sum_{n=1}^{\infty} (cost(1/n^2) - cost(1/(n+1)^2))$,-0.459,float,NONE,website | https://people.math.sc.edu/josephcf/Teaching/142/Files/Worksheets/Infinite%20Series.pdf,wenhuchen/infinite_series_sum1.json,limiting theorem,Mathematical analysis,Math,"In mathematical analysis, a limiting theorem refers to a result that describes the behavior of a sequence, function, or series as it approaches a specific value or point. These theorems are fundamental in understanding the properties of mathematical objects and their convergence or divergence. There are several important limiting theorems in mathematical analysis, including:

1. Limit of a sequence: A sequence is a list of numbers arranged in a specific order. The limit of a sequence is the value that the terms of the sequence approach as the index goes to infinity. If the limit exists, the sequence is said to be convergent; otherwise, it is divergent.

2. Limit of a function: The limit of a function is the value that the function approaches as its input approaches a specific value. Limits are used to define continuity, derivatives, and integrals, which are essential concepts in calculus.

3. Squeeze theorem: Also known as the sandwich theorem or the pinching theorem, this theorem states that if a function is ""squeezed"" between two other functions that have the same limit at a specific point, then the squeezed function must also have the same limit at that point.

4. Monotone convergence theorem: This theorem states that a monotone (either non-decreasing or non-increasing) and bounded sequence always converges to a limit.

5. Bolzano-Weierstrass theorem: This theorem states that every bounded sequence has a convergent subsequence, which is a sequence formed by selecting terms from the original sequence while preserving their order.

6. Dominated convergence theorem: This theorem provides a condition under which the limit of an integral can be interchanged with the integral of a limit. It is particularly useful in the study of Lebesgue integration.

7. Central limit theorem: In probability theory and statistics, the central limit theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the shape of the original distribution.

These limiting theorems play a crucial role in various branches of mathematics, including calculus, real analysis, complex analysis, and probability theory. They help us understand the behavior of mathematical objects and provide a foundation for further study and applications."
596,Let S be the set of integers between 1 and 2^40 that contain two 1’s when written in base 2. What is the probability that a random integer from S is divisible by 9?,0.1705,float,NONE,2004 AIME 2 10,tonyxia/modulararithmetic4.json,modular arithmetic,Number theory,Math,"Modular arithmetic, also known as clock arithmetic or the arithmetic of congruences, is a branch of number theory that deals with the properties and relationships of integers under the operation of modular addition, subtraction, multiplication, and sometimes division. It is a fundamental concept in number theory, cryptography, and computer science.

In modular arithmetic, numbers ""wrap around"" upon reaching a certain value called the modulus. The modulus is a positive integer that defines the size of the set of numbers being considered. When performing arithmetic operations, the result is always reduced to the remainder when divided by the modulus. This can be thought of as working with numbers on a circular number line, where the numbers wrap around after reaching the modulus.

The basic idea of modular arithmetic can be illustrated using a clock. A clock has a modulus of 12 (for a 12-hour clock) or 24 (for a 24-hour clock). When the hour hand moves past 12 or 24, it wraps around to 1 or 0, respectively. For example, if it is 10 o'clock and we add 5 hours, the result is 3 o'clock, not 15 o'clock. In this case, we are working modulo 12 or modulo 24.

In mathematical notation, modular arithmetic is often represented using the congruence symbol (≡). Two numbers a and b are said to be congruent modulo n if their difference (a - b) is divisible by n. This is written as:

a ≡ b (mod n)

For example, 17 ≡ 5 (mod 12) because 17 - 5 = 12, which is divisible by 12.

Modular arithmetic has many applications in various fields, including number theory, cryptography, computer science, and algebra. It is particularly useful in solving problems involving remainders, divisibility, and periodic patterns."
730,The electric flux through a spherical surface is  $4.0\times 10^4 N \cdot m^2/C$. What is the net charge enclosed by the surface?,3.54e-07,float,NONE,textbook | https://phys.libretexts.org/Bookshelves/University_Physics/Book%3A_University_Physics_(OpenStax)/Book%3A_University_Physics_II_-_Thermodynamics_Electricity_and_Magnetism_(OpenStax)/06%3A_Gauss's_Law/6.0E%3A_6.E%3A_Gauss's_Law_(Exercises),xueguangma/physics_gauss_law.json,gauss's law,Electromagnetism,Physics,"Gauss's Law, also known as Gauss's Flux Theorem, is a fundamental principle in electromagnetism that relates the electric field surrounding a distribution of electric charges to the total electric charge within that distribution. It is named after the German mathematician and physicist Carl Friedrich Gauss.

Gauss's Law is mathematically expressed as:

∮E • dA = Q_enclosed / ε₀

where:
- ∮E • dA represents the electric flux through a closed surface (integral of the electric field E over the surface area A)
- Q_enclosed is the total electric charge enclosed within the closed surface
- ε₀ is the vacuum permittivity, a constant value that characterizes the electric properties of a vacuum

In simple terms, Gauss's Law states that the electric flux through any closed surface is proportional to the total electric charge enclosed within that surface. This law is useful for calculating the electric field in situations with high symmetry, such as spherical, cylindrical, or planar charge distributions.

Gauss's Law is one of the four Maxwell's equations, which together form the foundation of classical electromagnetism. It is also closely related to the principle of conservation of electric charge, as it implies that the net electric charge within a closed surface cannot change unless there is a flow of charge across the surface."
665,"What is the Fisher information for the Gaussian distribution family $f_\theta(x)=N(0,\theta)$? (a) $2\theta$. (b) $2\theta^2$. (c) $0.5\theta^{-1}$. (d) $0.5\theta^{-2}$. Which option is correct?",(d),option,NONE,textbook 11.8(a),xinyi/fisher_information_2.json,fisher information,Statistics,Math,"Fisher information is a statistical concept used to measure the amount of information that a set of observed data carries about an unknown parameter of the underlying probability distribution. It is named after the British statistician Ronald A. Fisher, who introduced the concept in the context of maximum likelihood estimation.

Fisher information is particularly useful in the field of parameter estimation, as it helps to quantify the precision with which an unknown parameter can be estimated from the given data. It is closely related to the Cramér-Rao lower bound, which states that the variance of any unbiased estimator of the parameter cannot be smaller than the inverse of the Fisher information.

Mathematically, Fisher information is defined as the expected value of the second derivative (with respect to the parameter) of the log-likelihood function, or equivalently, as the expected value of the squared first derivative of the log-likelihood function. For a probability distribution with parameter θ and likelihood function L(θ), the Fisher information I(θ) can be expressed as:

I(θ) = E[(-d²/dθ²) log L(θ)] = E[(d/dθ log L(θ))²]

In simple terms, Fisher information quantifies how sensitive the likelihood function is to changes in the parameter. A higher Fisher information indicates that the data provides more information about the parameter, leading to more precise estimates. Conversely, a lower Fisher information suggests that the data is less informative, resulting in less precise estimates.

Fisher information plays a crucial role in various statistical methods, including hypothesis testing, confidence intervals, and Bayesian inference. It is an essential tool for understanding the relationship between data and the underlying parameters of a probability distribution."
1,"A certain underlying state graph is a tree where each node has three successor nodes, indexed $a$, $b$, $c$. There are two assets defined on this tree which pay no dividends except at the terminal time $T$. At a certain period it is known that the prices of the two accets are multiplied by factors, depending on the successor node. These factors are shown in the table below:
 | | a | b | c
security | 1 | 1.2 | 1.0 | 0.8
 | 2 | 1.2 | 1.3 | 1.4

 Is there a short-tem riskless asset for this period? Answer True or False.",True,bool,NONE,"textbook | Chapter 16, Exercise 1. Luenberger",xueguangma/state_tree.json,state tree model,Derivatives,Finance,"The State Tree Model (Derivatives) is a conceptual framework used in the field of mathematical finance and quantitative analysis to represent the evolution of financial derivatives over time. It is a tree-like structure that models the possible future states of the underlying asset, such as a stock or a bond, and the corresponding values of the derivative at each node in the tree.

In this model, the tree is constructed by dividing the time horizon into discrete intervals, and at each interval, the underlying asset can take on a finite number of possible values. Each node in the tree represents a specific state of the asset at a particular point in time, and the branches connecting the nodes represent the possible transitions between these states.

The State Tree Model is particularly useful for pricing and analyzing financial derivatives, such as options, futures, and swaps, as it allows for the calculation of the expected payoff of the derivative at each node in the tree. This is done by assigning probabilities to each possible transition between states and then using these probabilities to compute the expected value of the derivative at each node.

The main advantage of the State Tree Model is its flexibility, as it can be easily adapted to model various types of derivatives and underlying assets with different characteristics. Additionally, it provides a clear visual representation of the possible future states of the asset and the corresponding values of the derivative, which can be helpful for understanding the risk and potential return associated with the derivative.

However, the State Tree Model also has some limitations, such as the fact that it assumes discrete time intervals and a finite number of possible asset values, which may not always be realistic. Additionally, the model can become computationally intensive for large trees with many nodes and branches, making it less suitable for some applications."
630,Compute the real integral $I=\int_{-\infty}^{\infty} 1/(x^2 + 1)^2 dx$.,1.57,float,NONE,website | https://math.libretexts.org/Bookshelves/Analysis/Complex_Variables_with_Applications_(Orloff)/05%3A_Cauchy_Integral_Formula/5.02%3A_Cauchys_Integral_Formula_for_Derivatives,wenhuchen/cauchy_integral3.json,cauchy's integral theorem,Complex analysis,Math,"Cauchy's Integral Theorem is a fundamental result in complex analysis that relates the values of a holomorphic (complex-differentiable) function inside a closed contour to the values of the function on the contour itself. It states that if a function is holomorphic within and on a simple closed contour, then the integral of the function around the contour is zero.

Mathematically, let f(z) be a complex-valued function that is holomorphic in a simply connected domain D, which includes the contour C and its interior. Then, Cauchy's Integral Theorem states that:

∮_C f(z) dz = 0

Here, ∮_C denotes the contour integral taken around the closed contour C in the positive (counterclockwise) direction.

The theorem has several important consequences, including the fact that the value of a holomorphic function inside a closed contour can be recovered from its values on the contour itself (Cauchy's Integral Formula). It also implies that holomorphic functions have antiderivatives, and their integrals are path-independent in simply connected domains.

Cauchy's Integral Theorem is a powerful tool in complex analysis, as it allows us to evaluate contour integrals and study the properties of holomorphic functions in a more profound way."
453,"given a finite group A, and a collection of permutations B. Then (a) there exists B such that A is isomorphic to B; (b) for any B, A is isomorphic to B; (c) A can never be isomorphic to B; (d) none of the above. Which option is correct?",(a),option,NONE,self,mingyin/cayley-theorem1.json,cayley's theorem,Group theory,Math,"Cayley's theorem, named after the British mathematician Arthur Cayley, is a fundamental result in group theory, a branch of abstract algebra. The theorem states that every group G is isomorphic to a subgroup of the symmetric group acting on G. In simpler terms, this means that every group can be represented as a set of permutations of its elements.

To understand the theorem, let's first define some key terms:

1. Group: A group is a set G, together with a binary operation * (usually called multiplication or addition), that satisfies the following properties:
   - Closure: For all elements a, b in G, a * b is also in G.
   - Associativity: For all elements a, b, c in G, (a * b) * c = a * (b * c).
   - Identity: There exists an element e in G such that for all elements a in G, e * a = a * e = a.
   - Inverse: For every element a in G, there exists an element b in G such that a * b = b * a = e (the identity element).

2. Symmetric group: The symmetric group on a set X is the group of all possible permutations (bijective functions) of the elements of X. It is denoted by S_X or S_n, where n is the number of elements in X.

3. Isomorphism: An isomorphism between two groups G and H is a bijective function f: G → H that preserves the group structure, i.e., for all elements a, b in G, f(a * b) = f(a) * f(b). If there exists an isomorphism between G and H, we say that G and H are isomorphic.

Now, let's state Cayley's theorem more formally:

Given a group G, there exists an isomorphism between G and a subgroup of the symmetric group S_G, where S_G is the symmetric group acting on the set G.

The proof of Cayley's theorem involves constructing a specific function, called the left regular representation, that maps each element of G to a permutation of G. This function is shown to be an isomorphism between G and a subgroup of S_G.

Cayley's theorem has several important implications in group theory. It shows that every group can be thought of as a group of permutations, which provides a concrete way to study abstract groups. Additionally, it highlights the importance of symmetric groups, as they can be used to represent any other group."
310,"Find the arc length of the curve, where x=t, y=t^2 and z=2*t^3/3.",7.333,float,NONE,textbook | Demidovich,wenhuchen/line_integral2.json,line integral theorem,Calculus,Math,"The Line Integral Theorem, also known as the Fundamental Theorem for Line Integrals, is a fundamental result in vector calculus that relates the line integral of a vector field along a curve to the value of a potential function at the endpoints of the curve. It is used to evaluate line integrals of conservative vector fields and to determine if a vector field is conservative.

The theorem states that if a vector field F is conservative, meaning it has a potential function f (i.e., F = ∇f, where ∇ is the gradient operator), then the line integral of F along a curve C with endpoints A and B is equal to the difference in the potential function's values at these endpoints:

∫(C) F · dr = f(B) - f(A)

Here, F · dr represents the dot product of the vector field F and the differential displacement vector dr along the curve C.

The Line Integral Theorem has several important implications:

1. If a vector field is conservative, the line integral is path-independent, meaning the value of the integral depends only on the endpoints A and B, not on the specific path taken between them.

2. For a conservative vector field, the line integral around a closed curve (where the initial and final points are the same) is always zero.

3. The theorem provides a method for evaluating line integrals of conservative vector fields by finding the potential function and computing the difference in its values at the endpoints of the curve.

In summary, the Line Integral Theorem is a powerful tool in vector calculus that connects the concepts of line integrals, conservative vector fields, and potential functions, allowing for more efficient evaluation of line integrals and analysis of vector fields."
172,"Can we use the method of compass and straightedge construction to construct the edge length of a cube, such that the volume of the cube is equal to X time the volume of a given cube, where X belongs to the set {3,17,8,27,343,1331}? Return the answer list for the respective values of X with 1 for yes and 0 for no.","[0, 0, 1, 1, 1, 1]",list of integer,NONE,Galois theory | https://en.wikipedia.org/wiki/Doubling_the_cube,mingyin/Galois_theory1.json,doubling the cube,Geometry,Math,"Doubling the cube, also known as the Delian problem, is a geometric problem that dates back to ancient Greece. It involves constructing a cube with exactly twice the volume of a given cube, using only a compass and a straightedge.

The problem can be stated as follows: Given a cube with side length 'a' and volume V = a^3, find the side length 'b' of a new cube such that its volume is 2V, or 2a^3. Mathematically, this means finding the value of 'b' such that b^3 = 2a^3.

The Delian problem is one of the three famous geometric problems of antiquity, along with trisecting the angle and squaring the circle. These problems were considered significant challenges in ancient Greek mathematics, and many mathematicians attempted to solve them using only a compass and a straightedge, as per the Greek tradition.

It was eventually proven in the 19th century that doubling the cube is impossible using only a compass and a straightedge. This proof is based on the fact that the cube root of 2, which is the ratio between the side lengths of the two cubes (b/a), is an algebraic number of degree 3. According to the field of constructible numbers, only numbers that can be expressed using square roots (algebraic numbers of degree 2) can be constructed using a compass and a straightedge.

Despite the impossibility of solving the problem using the traditional Greek tools, doubling the cube remains an interesting problem in geometry and has led to the development of various methods and techniques in the field of mathematics."
685,The polynomial $x^3 - Ax + 15$ has three real roots. Two of these roots sum to 5. What is |A|?,22.0,float,NONE,website | https://www.andrew.cmu.edu/user/daltizio/Vietas%20Formulas.pdf,wenhuchen/vieta's_formula2.json,vieta's formula,Algebra,Math,"Vieta's formulas, named after the French mathematician François Viète, are a set of algebraic equations that relate the coefficients of a polynomial to the sums and products of its roots. These formulas are particularly useful in solving polynomial equations and finding relationships between the roots without actually calculating the roots themselves.

Consider a polynomial equation of degree n:

P(x) = a_nx^n + a_(n-1)x^(n-1) + ... + a_1x + a_0

where a_n, a_(n-1), ..., a_1, and a_0 are the coefficients of the polynomial, and x is the variable.

Let r_1, r_2, ..., r_n be the roots of the polynomial, i.e., P(r_i) = 0 for i = 1, 2, ..., n.

Vieta's formulas establish the following relationships between the coefficients and the roots:

1. Sum of the roots:
r_1 + r_2 + ... + r_n = -a_(n-1) / a_n

2. Sum of the products of the roots taken two at a time:
r_1r_2 + r_1r_3 + ... + r_(n-1)r_n = a_(n-2) / a_n

3. Sum of the products of the roots taken three at a time:
r_1r_2r_3 + r_1r_2r_4 + ... + r_(n-2)r_(n-1)r_n = -a_(n-3) / a_n

And so on, until the product of all the roots:

4. Product of the roots:
r_1r_2...r_n = (-1)^n * (a_0 / a_n)

These formulas can be applied to various problems in algebra, such as finding the roots of a polynomial, solving systems of equations, and simplifying expressions involving roots."
480,"An ordinary deck of cards containing 26 red cards and 26 black cards is shuffled and dealt out one card at a time without replacement. Let $X_i$ be the color of the $i$th card. Compute $H(X_1,X_2,\ldots,X_{52})$ in bits.",48.8,float,NONE,textbook 6.3,xinyi/entropy.json,entropy theorem,Probability theory,Math,"In probability theory, the entropy theorem, also known as Shannon entropy, is a measure of the uncertainty or randomness associated with a random variable. It was introduced by Claude Shannon in his 1948 paper ""A Mathematical Theory of Communication"" and is a fundamental concept in information theory.

The entropy of a discrete random variable X with possible values {x1, x2, ..., xn} and probability mass function P(X) is defined as:

H(X) = - ∑ [P(xi) * log2(P(xi))] for i = 1 to n

Here, H(X) represents the entropy of the random variable X, P(xi) is the probability of the event xi occurring, and log2 is the logarithm base 2.

The entropy theorem has several important properties:

1. Non-negativity: Entropy is always non-negative, i.e., H(X) ≥ 0. The entropy is zero if and only if the random variable has a single possible value (i.e., it is deterministic).

2. Maximum entropy: The entropy is maximized when all possible values of the random variable are equally likely. In this case, H(X) = log2(n), where n is the number of possible values.

3. Additivity: If X and Y are two independent random variables, then the entropy of their joint distribution is the sum of their individual entropies, i.e., H(X, Y) = H(X) + H(Y).

4. Data compression: Entropy provides a lower bound on the average number of bits needed to encode the outcomes of a random variable. This is because entropy quantifies the average amount of ""information"" or ""surprise"" contained in the outcomes.

In summary, the entropy theorem in probability theory is a measure of the uncertainty or randomness associated with a random variable. It quantifies the average amount of information required to describe the outcomes of a random process and has important applications in information theory, data compression, and cryptography."
87,What is \lim_{x \to (\pi)/2} (cos(x)cos(tan(x)))?,0,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_5.json,squeeze theorem,Calculus,Math,"The Squeeze Theorem, also known as the Sandwich Theorem or the Pinching Theorem, is a fundamental concept in calculus that helps to determine the limit of a function when direct substitution or algebraic manipulation is not possible. The theorem states that if you have three functions, f(x), g(x), and h(x), such that f(x) ≤ g(x) ≤ h(x) for all x in a certain interval around a point 'a' (except possibly at 'a' itself), and if the limit of f(x) and h(x) as x approaches 'a' is the same value L, then the limit of g(x) as x approaches 'a' must also be L.

In mathematical notation, the Squeeze Theorem can be written as:

If f(x) ≤ g(x) ≤ h(x) for all x in an interval around 'a' (except possibly at 'a') and lim (x→a) f(x) = lim (x→a) h(x) = L, then lim (x→a) g(x) = L.

The Squeeze Theorem is particularly useful when dealing with trigonometric functions or functions that are difficult to evaluate directly. By comparing the function of interest (g(x)) to two other functions (f(x) and h(x)) that ""squeeze"" or ""sandwich"" it, we can determine the limit of g(x) as x approaches a certain point."
706,"In an IPv4 datagram, the value of the total-length field is $(00 \mathrm{~A} 0)_{16}$ and the value of the headerlength (HLEN) is (5) $1_{16}$. How many bytes of payload are being carried by the datagram?",140,integer,NONE,website | https://vinesmsuic.github.io/notes-networking3-qa/,maxku/ipnetwork1-ip.json,internet protocol,Computer networking,EECS,"Internet Protocol (IP) is a set of rules and standards that govern how data is transmitted, received, and routed across computer networks, including the internet. It is a fundamental component of the Internet Protocol Suite, which is a collection of protocols and technologies that enable communication between devices over the internet.

IP operates at the network layer (Layer 3) of the Open Systems Interconnection (OSI) model and is responsible for addressing, packaging, and routing data packets between devices. It ensures that data is sent from a source device to a destination device, even if they are on different networks.

There are two main versions of IP in use today: IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol version 6).

IPv4 is the most widely used version, which uses 32-bit addresses, allowing for approximately 4.3 billion unique IP addresses. Due to the rapid growth of the internet, the number of available IPv4 addresses has become limited, leading to the development of IPv6.

IPv6 uses 128-bit addresses, providing a vastly larger number of unique IP addresses (approximately 3.4 x 10^38) to accommodate the growing number of devices connected to the internet.

Key features of Internet Protocol include:

1. Addressing: IP assigns unique addresses to devices on a network, enabling them to be identified and located. These addresses are used to route data packets to their intended destinations.

2. Packetization: IP divides data into smaller units called packets, which are then transmitted independently across the network. This allows for more efficient use of network resources and enables data to be sent over multiple paths.

3. Routing: IP uses routing algorithms to determine the best path for data packets to travel from the source device to the destination device. Routers, which are specialized devices that connect networks, use IP addresses to forward packets along the most efficient route.

4. Error detection: IP includes a checksum mechanism to detect errors in the header of data packets. If an error is detected, the packet is discarded, and the sender may be notified to resend the data.

5. Fragmentation and reassembly: IP can fragment large packets into smaller ones to accommodate the maximum transmission unit (MTU) of different networks. The destination device then reassembles the fragments back into the original data.

Overall, Internet Protocol plays a crucial role in enabling communication between devices on computer networks and the internet, providing the foundation for various applications and services we use daily."
475,Is 80 dB twice as loud as 40 dB?,False,bool,NONE,website | https://vinesmsuic.github.io/notes-dimagep-QA3/,maxku/signalprocessing2-DB.json,sound level,Signal processing,EECS,"Sound level in signal processing refers to the measurement of the intensity or amplitude of an audio signal, usually expressed in decibels (dB). It is an important aspect of audio processing, as it helps in understanding the loudness or softness of a sound and plays a crucial role in various applications such as audio mixing, noise reduction, and audio compression.

In signal processing, the sound level is often represented as a time-varying waveform, where the amplitude of the waveform corresponds to the instantaneous sound pressure level. The waveform can be analyzed in both time and frequency domains to extract useful information about the audio signal.

There are several ways to measure the sound level in signal processing:

1. Peak level: This is the maximum amplitude of the audio signal, which represents the highest sound pressure level in the waveform.

2. RMS (Root Mean Square) level: This is a more accurate representation of the average sound level, as it takes into account both positive and negative amplitude values. It is calculated by squaring the amplitude values, taking the average of the squared values, and then finding the square root of the average.

3. A-weighted level: This is a frequency-weighted sound level measurement that approximates the human ear's sensitivity to different frequencies. It is commonly used in noise measurement and environmental noise assessments.

4. Loudness level: This is a psychoacoustic measure that takes into account the human perception of loudness. It is usually calculated using algorithms such as the ITU-R BS.1770 standard, which considers the frequency content, duration, and amplitude of the audio signal.

In summary, sound level in signal processing is a crucial parameter that helps in understanding and manipulating audio signals for various applications. It is typically measured in decibels and can be analyzed using different methods to obtain information about the loudness, frequency content, and other characteristics of the audio signal."
613,What is \lim_{x \to 0} (x \lfloor 1/x 
floor)?,1,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_13.json,squeeze theorem,Calculus,Math,"The Squeeze Theorem, also known as the Sandwich Theorem or the Pinching Theorem, is a fundamental concept in calculus that helps to determine the limit of a function when direct substitution or algebraic manipulation is not possible. The theorem states that if you have three functions, f(x), g(x), and h(x), such that f(x) ≤ g(x) ≤ h(x) for all x in a certain interval around a point 'a' (except possibly at 'a' itself), and if the limit of f(x) and h(x) as x approaches 'a' is the same value L, then the limit of g(x) as x approaches 'a' must also be L.

In mathematical notation, the Squeeze Theorem can be written as:

If f(x) ≤ g(x) ≤ h(x) for all x in an interval around 'a' (except possibly at 'a') and lim (x→a) f(x) = lim (x→a) h(x) = L, then lim (x→a) g(x) = L.

The Squeeze Theorem is particularly useful when dealing with trigonometric functions or functions that are difficult to evaluate directly. By comparing the function of interest (g(x)) to two other functions (f(x) and h(x)) that ""squeeze"" or ""sandwich"" it, we can determine the limit of g(x) as x approaches a certain point."
286,"Does the function $y=xe^{-x^2/2}$, does it satisfy the equation $xy' = (1 - x^2)y$",True,bool,NONE,self,wenhuchen/derivative2.json,derivative chain rule,Calculus,Math,"The Derivative Chain Rule is a fundamental rule in calculus used to find the derivative of a composite function. A composite function is a function that is formed by combining two or more functions, where the output of one function becomes the input of another function.

The Chain Rule states that if you have a composite function, say h(x) = f(g(x)), then the derivative of h(x) with respect to x, denoted as h'(x) or dh/dx, can be found by taking the derivative of the outer function f with respect to the inner function g(x), and then multiplying it by the derivative of the inner function g(x) with respect to x.

Mathematically, the Chain Rule can be expressed as:

h'(x) = f'(g(x)) * g'(x)

or

dh/dx = (df/dg) * (dg/dx)

The Chain Rule is particularly useful when dealing with complex functions that involve multiple layers of functions, as it allows us to break down the problem into simpler parts and find the derivative step by step."
386,"A glider with mass m = 0.200 kg sits on a frictionless horizontalair track, connected to a spring with force constant k = 5.00 N/m.You pull on the glider, stretching the spring 0.100 m, and release itfrom rest. The glider moves back toward its equilibrium position (x = 0).What is its x-velocity when  x = 0.080 m? (Unit: m/s))",-0.3,float,NONE,University Physics with Modern Physics | Example 7.7,panlu/work_energy1.json,elastic potential energy,Kinetics,Physics,"Elastic potential energy is a form of potential energy that is stored in an elastic object, such as a spring or a rubber band, when it is stretched or compressed. It is associated with the deformation of the object and is directly related to the restoring force exerted by the object when it returns to its original shape.

In the field of kinetics, elastic potential energy plays a crucial role in understanding the motion and forces involved in elastic systems. The energy is stored as a result of the work done in deforming the object, and it can be converted into other forms of energy, such as kinetic energy, when the object is released.

The elastic potential energy can be mathematically represented using Hooke's Law, which states that the force required to compress or extend a spring is proportional to the displacement from its equilibrium position. The formula for elastic potential energy is:

PE_elastic = (1/2) * k * x^2

where PE_elastic is the elastic potential energy, k is the spring constant (a measure of the stiffness of the spring), and x is the displacement from the equilibrium position.

In summary, elastic potential energy is the energy stored in an elastic object when it is deformed, and it plays a significant role in understanding the motion and forces in elastic systems. This energy can be converted into other forms, such as kinetic energy, when the object is released and returns to its original shape."
471,"In the process of searching circles in an image, object O is detected. The contour of the object O is represented with the Fourier Descriptors (-20,60,-20,20,-20,21,-20,20). Given that the Fourier Descriptors of a circle are (0,40,0,0,0,0,0,0). Is the object O a circle-like polygon in the image? Bear in mind that there is some high frequency noise in the image. You should take this into account when you make your judgment.",False,bool,NONE,self | https://vinesmsuic.github.io/notes-dimagep-QA2/,maxku/cv-imageprocessing8-fourier2.json,image frequency analysis,Signal processing,EECS,"Image Frequency Analysis is a technique used in signal processing to identify and analyze the presence of unwanted signals, known as image frequencies, that may interfere with the desired signal in a communication system. This analysis is particularly important in radio frequency (RF) systems, where the process of frequency conversion (up-conversion or down-conversion) can introduce these unwanted signals.

In a typical RF system, a mixer is used to combine the input signal with a local oscillator (LO) signal to produce an output signal at a different frequency. Ideally, the output signal should only contain the desired frequency components. However, due to the non-linear behavior of mixers and other imperfections in the system, additional frequency components, called image frequencies, can be generated. These image frequencies can cause interference and degrade the performance of the communication system.

Image Frequency Analysis involves the following steps:

1. Identifying the image frequencies: The first step is to determine the possible image frequencies that can be generated in the system. This can be done by analyzing the frequency spectrum of the input signal, the LO signal, and the mixer output.

2. Filtering: To minimize the impact of image frequencies, filters are often used in the system to attenuate or eliminate these unwanted signals. The design of these filters depends on the specific requirements of the system, such as the desired signal bandwidth and the level of image rejection needed.

3. Measurement and evaluation: Once the filters are in place, the performance of the system can be evaluated by measuring the level of the image frequencies and comparing them to the desired signal. This can be done using various signal analysis tools, such as spectrum analyzers and vector network analyzers.

4. Optimization: Based on the results of the measurement and evaluation, the system can be further optimized to improve its performance. This may involve adjusting the filter parameters, changing the LO frequency, or modifying other system components.

In summary, Image Frequency Analysis is a crucial technique in signal processing that helps identify and mitigate the impact of unwanted image frequencies in communication systems, ensuring optimal performance and signal quality."
713,"Coloring the edges of a complete graph with n vertices in 2 colors (red and blue), what is the smallest n that guarantees there is either a 4-clique in red or a 5-clique in blue?",25,integer,NONE,self,jianyu_xu/Ramsey_5.json,ramsey's theorem,Combinatorics,Math,"Ramsey's theorem is a fundamental result in combinatorics, specifically in the area of graph theory and combinatorial mathematics. It is named after the British mathematician Frank P. Ramsey, who first stated the theorem in 1930. The theorem deals with the conditions under which order must appear in a large enough structure, even if that structure is initially disordered or chaotic.

In its simplest form, Ramsey's theorem states that for any given positive integers m and n, there exists a least positive integer R(m, n) such that any graph with at least R(m, n) vertices will contain either a clique of size m (a complete subgraph where every pair of vertices is connected by an edge) or an independent set of size n (a set of vertices where no two vertices are connected by an edge).

In other words, if you have a large enough graph, it is impossible to avoid having either a large complete subgraph or a large independent set, regardless of how the edges are arranged.

Ramsey's theorem can also be extended to more complex structures, such as hypergraphs and infinite graphs, and can be generalized to deal with multiple colors or more complicated combinatorial objects. The theorem has important applications in various fields, including computer science, logic, and number theory.

However, despite its significance, Ramsey's theorem is known for its non-constructive nature, meaning that it guarantees the existence of a certain structure but does not provide an explicit way to find or construct it. Additionally, the bounds for R(m, n) are often very large and difficult to compute, which limits the practical applications of the theorem."
16,Use the Trapezoidal Rule with to approximate $\int_0^{\pi} sin^2(x)dx$. Return the approximated demical value.,1.570796,float,NONE,website | https://math24.net/trapezoidal-rule.html,wenhuchen/trapezoidal_rule1.json,trapezoidal rule,Calculus,Math,"The Trapezoidal Rule is a numerical integration technique used in calculus to approximate the definite integral of a function. It works by dividing the area under the curve of the function into a series of trapezoids and then summing the areas of these trapezoids to estimate the total area. This method is particularly useful when dealing with functions that are difficult or impossible to integrate analytically.

The basic idea behind the Trapezoidal Rule is to approximate the function with a series of straight lines connecting the points on the curve. These lines form the bases of the trapezoids, and the height of each trapezoid is determined by the difference in the x-values (Δx) between consecutive points.

To apply the Trapezoidal Rule, follow these steps:

1. Divide the interval [a, b] into n equal subintervals, where a and b are the limits of integration, and n is the number of subintervals.
2. Calculate the width of each subinterval, Δx = (b - a) / n.
3. Evaluate the function at each endpoint of the subintervals: f(a), f(a + Δx), f(a + 2Δx), ..., f(b).
4. Calculate the area of each trapezoid using the formula: Area = (1/2) * (f(x_i) + f(x_(i+1))) * Δx, where x_i and x_(i+1) are consecutive endpoints of the subintervals.
5. Sum the areas of all the trapezoids to obtain the approximate value of the definite integral.

The accuracy of the Trapezoidal Rule increases as the number of subintervals (n) increases, but it may require a large number of subintervals for functions with high curvature or rapid changes. Other numerical integration techniques, such as Simpson's Rule, may provide more accurate results with fewer subintervals."
21,Arbitrarily place 19 points in a unit square and cover as many of these points as possible with a circle of diameter $\frac{\sqrt 2}{3}$. Question: At least how many points can be guaranteed to be covered?,3,integer,NONE,self,jianyu_xu/pigeonhole_2.json,pigeonhole principle,Combinatorics,Math,"The Pigeonhole Principle is a fundamental concept in combinatorics, a branch of mathematics that deals with counting and arranging objects. It is a simple yet powerful idea that helps to draw conclusions about the distribution of objects among a finite number of containers or ""pigeonholes.""

The principle states that if you have more objects (pigeons) than containers (pigeonholes), then at least one container must contain more than one object. In other words, if you try to fit n+1 objects into n containers, at least one container will have at least two objects.

This principle is useful in solving various problems in mathematics and computer science, where it helps to identify patterns, make generalizations, and prove the existence of certain conditions.

For example, consider a group of 13 people. According to the Pigeonhole Principle, at least two of them must share the same birthday month since there are only 12 months in a year. This doesn't tell us which people or which month, but it guarantees that such a pair exists.

In summary, the Pigeonhole Principle is a basic yet powerful combinatorial tool that allows us to make conclusions about the distribution of objects among a finite number of containers, often leading to surprising and counterintuitive results."
236,"Suppose C[0,1] denotes the space of all the continuous functions on the interval [0,1]. Is (C[0,1],\|\cdot\|_1 ) a Banach space? Here $\|f(x)\|_1=\int_0^1 |f(t)|dt$ with $f\in C[0,1]$. Answer 1 for yes and 0 for no.",0.0,float,NONE,functional analysis: exercise 2.3.5,mingyin/Equivalence-of-Norms-Theorem1.json,equivalence of norms theorem,Functional analysis,Math,"In functional analysis, the Equivalence of Norms Theorem is a fundamental result that states that any two norms on a finite-dimensional vector space are equivalent. This means that, although the norms may be different, they essentially provide the same information about the size and structure of the vector space. The theorem is important because it allows us to switch between different norms without changing the essential properties of the space.

To be more precise, let V be a finite-dimensional vector space over the field F (either the real numbers R or the complex numbers C), and let ||·||₁ and ||·||₂ be two norms on V. The norms ||·||₁ and ||·||₂ are said to be equivalent if there exist positive constants C₁ and C₂ such that for all vectors x in V, we have:

C₁ ||x||₁ ≤ ||x||₂ ≤ C₂ ||x||₁

The Equivalence of Norms Theorem states that any two norms on a finite-dimensional vector space are equivalent. In other words, there exist constants C₁ and C₂ such that the inequality above holds for all vectors x in V.

This result has several important consequences in functional analysis:

1. It implies that any two norms on a finite-dimensional vector space induce the same topology, meaning that the open sets, closed sets, and convergence properties are the same with respect to both norms.

2. It allows us to prove that any linear operator between finite-dimensional normed spaces is continuous, as the continuity of a linear operator depends on the choice of norms on the domain and codomain spaces.

3. It shows that any finite-dimensional subspace of a normed space is closed, which is a crucial property in the study of Banach spaces and their dual spaces.

The Equivalence of Norms Theorem does not hold for infinite-dimensional vector spaces, and the choice of norm can have significant consequences for the properties of the space and its operators. This is one of the key differences between finite-dimensional and infinite-dimensional functional analysis."
85,What is \int_{-3}^1 (7x^2 + x +1)dx?,65.333,float,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_5.json,integral rules,Calculus,Math,"Integral rules in calculus are a set of techniques and formulas used to evaluate and solve integrals. Integrals are a fundamental concept in calculus, representing the area under a curve or the accumulation of a quantity over a given interval. The integral rules provide a systematic approach to finding the antiderivative (the inverse of the derivative) of a function, which is essential for solving various mathematical and real-world problems.

Here are some of the most common integral rules:

1. Constant Rule: The integral of a constant (c) with respect to a variable (x) is equal to the product of the constant and the variable, plus a constant of integration (C). 
   ∫c dx = cx + C

2. Power Rule: The integral of x raised to the power of n (x^n) with respect to x is equal to x raised to the power of (n+1) divided by (n+1), plus a constant of integration (C). This rule is valid for n ≠ -1.
   ∫x^n dx = (x^(n+1))/(n+1) + C

3. Sum/Difference Rule: The integral of the sum or difference of two functions is equal to the sum or difference of their integrals.
   ∫(f(x) ± g(x)) dx = ∫f(x) dx ± ∫g(x) dx

4. Constant Multiple Rule: The integral of a constant multiplied by a function is equal to the constant multiplied by the integral of the function.
   ∫(cf(x)) dx = c∫f(x) dx

5. Substitution Rule (u-substitution): This rule is used when a function is composed of another function. It involves substituting a new variable (u) for a part of the original function, and then integrating with respect to the new variable.
   If u = g(x) and du/dx = g'(x), then ∫f(g(x))g'(x) dx = ∫f(u) du

6. Integration by Parts: This rule is used for integrating the product of two functions. It is based on the product rule for differentiation.
   If u = f(x) and v = g(x), then ∫u dv = uv - ∫v du

7. Trigonometric Integrals: These rules involve the integration of various trigonometric functions, such as sine, cosine, tangent, and their combinations.

8. Partial Fractions: This technique is used to integrate rational functions (fractions with polynomials in the numerator and denominator). It involves decomposing the rational function into simpler fractions, which can be integrated individually.

9. Improper Integrals: These rules deal with integrals that have infinite limits or involve functions with discontinuities. They often require the use of limits to evaluate the integral.

These integral rules, along with other advanced techniques, form the foundation of integral calculus and are essential for solving a wide range of mathematical problems."
221,"Suppose that feedback is used on a binary symmetric channel with parameter $p=0.5$. Each time a $Y$ is received, it becomes the next transmission. Thus $X_1$ is Bern(1/2), $X_2=Y_1$, $X_3=Y_2$, \ldots, X_n=Y_{n-1}. Find $\lim_{n\to\infty} \frac{1}{n} I(X_n;Y_n)$ in bits.",0.0,float,NONE,textbook 7.33,xinyi/binary_symmetric_channel_2.json,binary symmetric channel,Information theory,EECS,"A Binary Symmetric Channel (BSC) is a fundamental concept in information theory that represents a communication channel model for transmitting binary data (0s and 1s) between a sender and a receiver. The term ""symmetric"" refers to the fact that the probability of error is the same for both binary values (0 and 1).

In a BSC, there are two possible input values (0 and 1) and two possible output values (0 and 1). The channel is characterized by a single parameter, the crossover probability (p), which represents the probability that a transmitted bit is flipped (i.e., changed from 0 to 1 or from 1 to 0) during transmission. The probability of a bit being transmitted correctly is (1-p).

The BSC model assumes that the errors in the channel are independent and identically distributed, meaning that the probability of an error occurring at any given bit position is the same and does not depend on the values of other bits.

In summary, a Binary Symmetric Channel is a simple model used in information theory to study the transmission of binary data over a noisy communication channel. It is characterized by a single parameter, the crossover probability (p), which represents the probability of a bit being flipped during transmission."
77,"True or false: there exists a graph with score (1, 1, 2, 2, 3, 3, 4, 4).",True,bool,NONE,Self,tonyxia/score2.json,score theorem,Graph theory,EECS,"In graph theory, the Score Theorem, also known as the Havel-Hakimi Theorem, is a method used to determine if a given degree sequence can be realized by a simple, undirected graph. A degree sequence is a list of non-negative integers that represents the degrees of the vertices in a graph. A simple graph is a graph with no loops or multiple edges between the same pair of vertices.

The Score Theorem is based on the Havel-Hakimi algorithm, which is a recursive algorithm that works as follows:

1. Sort the degree sequence in non-increasing order.
2. If all the degrees are zero, then the sequence is graphical, meaning it can be realized by a simple graph.
3. If there are any negative degrees, the sequence is non-graphical, meaning it cannot be realized by a simple graph.
4. Remove the first degree (d1) from the sequence and reduce the next d1 degrees by 1.
5. Repeat steps 1-4 with the new sequence.

The theorem states that a degree sequence is graphical if and only if the sequence obtained by applying the Havel-Hakimi algorithm is graphical.

In other words, the Score Theorem provides a way to check if it is possible to construct a simple, undirected graph with a given degree sequence. If the algorithm terminates with all degrees being zero, then the original sequence can be realized by a simple graph. Otherwise, it cannot."
373,"A camper pours 0.300 kg of coffee, initially in a pot at 70.0°C into a 0.120-kg aluminum cup initially at 20.0°C. What is the equilibrium temperature? Assume that coffee has the same specific heat as water and that no heat is exchanged with the surroundings. (Unit: °C)",66.0,float,NONE,University Physics with Modern Physics | Example 17.7,panlu/molar_heat_capacity1.json,molar heat capacity,Thermodynamics,Physics,"Molar heat capacity is a thermodynamic property that describes the amount of heat required to change the temperature of one mole of a substance by one degree Celsius (or one Kelvin). It is an important concept in thermodynamics, as it helps to understand how substances absorb, store, and release heat energy during various processes.

Molar heat capacities can be classified into two types: constant volume (Cv) and constant pressure (Cp). 

1. Molar heat capacity at constant volume (Cv): This is the amount of heat required to raise the temperature of one mole of a substance by one degree Celsius when the volume of the substance is kept constant. In this case, the heat energy is used only to increase the internal energy of the substance, and no work is done on or by the substance.

2. Molar heat capacity at constant pressure (Cp): This is the amount of heat required to raise the temperature of one mole of a substance by one degree Celsius when the pressure of the substance is kept constant. In this case, the heat energy is used to increase both the internal energy of the substance and to do work on the surroundings due to the expansion of the substance.

The relationship between Cp and Cv can be described by the equation:

Cp = Cv + R

where R is the gas constant.

Molar heat capacities are dependent on the substance's molecular structure, phase (solid, liquid, or gas), and temperature. In general, more complex molecules have higher molar heat capacities because they can store more energy in their various vibrational, rotational, and translational modes. Additionally, molar heat capacities usually increase with temperature, as more energy levels become accessible for energy storage."
378,"Let’s assume that the 10-year annual return for the S&P 500 (market portfolio) is 10%, while the average annual return on Treasury bills (a good proxy for the risk-free rate) is 5%. The standard deviation is 15% over a 10-year period. Whats the market Sharpe Ratio?",0.33,float,NONE,website | https://www.wallstreetmojo.com/risk-adjusted-returns,xueguangma/sharpe_ratio.json,sharpe's ratio,Portfolio management,Finance,"Sharpe's Ratio, also known as the Sharpe Ratio or the Sharpe Index, is a widely used financial metric in portfolio management to evaluate the risk-adjusted return of an investment or a portfolio. It was developed by Nobel laureate William F. Sharpe in 1966 and has since become a standard tool for assessing the performance of investments, funds, and portfolios.

The Sharpe Ratio measures the excess return per unit of risk taken by an investment or a portfolio, with the excess return being the difference between the investment's return and the risk-free rate. The risk-free rate is typically represented by the return on a short-term government bond, such as a U.S. Treasury bill. The risk is measured by the standard deviation of the investment's returns, which is a common measure of volatility or the dispersion of returns.

The formula for calculating the Sharpe Ratio is:

Sharpe Ratio = (Portfolio Return - Risk-Free Rate) / Portfolio Standard Deviation

A higher Sharpe Ratio indicates that an investment or a portfolio has generated a higher return per unit of risk taken, making it more attractive to investors. Conversely, a lower Sharpe Ratio suggests that the investment or portfolio has not performed as well on a risk-adjusted basis.

It is important to note that the Sharpe Ratio should be used in conjunction with other performance metrics and risk measures to make a comprehensive assessment of an investment or a portfolio. Additionally, the Sharpe Ratio is most effective when comparing investments or portfolios with similar risk profiles, as it may not accurately reflect the risk-adjusted performance of investments with significantly different levels of risk."
289,"Find the solutions to the second order boundary-value problem. y''-2y'+2y=0, y(0)=0, y(\pi/2) = 1. What is y(\pi/4)?",0.322,float,NONE,website | https://www.varsitytutors.com/differential_equations-help/numerical-solutions-of-ordinary-differential-equations,wenhuchen/ODE3.json,ordinary differential equation,Calculus,Math,"An Ordinary Differential Equation (ODE) is a mathematical equation that describes the relationship between a function and its derivatives. In calculus, ODEs are used to model various phenomena, such as the motion of objects, population growth, chemical reactions, and more.

An ODE involves a dependent variable (usually denoted as y or u), an independent variable (usually denoted as x or t), and one or more of the dependent variable's derivatives with respect to the independent variable. The order of an ODE is determined by the highest order derivative present in the equation.

For example, a first-order ODE can be written as:

dy/dx = f(x, y)

where dy/dx is the first derivative of y with respect to x, and f(x, y) is a function of x and y.

A second-order ODE can be written as:

d²y/dx² = g(x, y, dy/dx)

where d²y/dx² is the second derivative of y with respect to x, and g(x, y, dy/dx) is a function of x, y, and dy/dx.

Solving an ODE involves finding a function (or a family of functions) that satisfies the given equation. There are various techniques for solving ODEs, such as separation of variables, integrating factors, and numerical methods. The solutions to ODEs can provide valuable insights into the behavior of the modeled system and help predict its future states."
173,"Traders in major financial institutions use the Black-Scholes formula in a backward fashion to infer other traders' estimation of $\sigma$ from option prices. In fact, traders frequently quote sigmas to each other, rather than prices, to arrange trades. Suppose a call option on a stock that pays no dividend for 6 months has a strike price of $35, a premium of $2.15, and time to maturity of 7 weeks. The current short-term T-bill rate is 7%, and the price of the underlying stock is $36.12. What is the implied volatility of the underlying security?",0.251,float,NONE,"textbook | Chapter 13, Exercise 3. Luenberger",xueguangma/sigma_estimation.json,sigma estimation,Quantitive methods,Finance,"Sigma Estimation is a quantitative method used in statistics and data analysis to estimate the standard deviation (σ) of a population or dataset. Standard deviation is a measure of the dispersion or spread of data points around the mean (average) value. In other words, it indicates how much the individual data points deviate from the mean value.

Sigma Estimation is particularly useful when dealing with large datasets or populations, as it provides a measure of the variability or uncertainty in the data. This information can be used to make informed decisions, identify trends, and assess the reliability of the data.

There are several methods to estimate the standard deviation (σ) in a dataset:

1. Sample Standard Deviation: This method is used when you have a sample of data points from a larger population. The formula for calculating the sample standard deviation is:

s = √(Σ(x - x̄)² / (n - 1))

where s is the sample standard deviation, x represents each data point, x̄ is the mean of the sample, n is the number of data points in the sample, and Σ denotes the sum of the squared differences between each data point and the mean.

2. Population Standard Deviation: This method is used when you have data for the entire population. The formula for calculating the population standard deviation is:

σ = √(Σ(x - μ)² / N)

where σ is the population standard deviation, x represents each data point, μ is the mean of the population, N is the number of data points in the population, and Σ denotes the sum of the squared differences between each data point and the mean.

3. Range Rule of Thumb: This is a quick and simple method to estimate the standard deviation based on the range of the data. The formula is:

σ ≈ (Range / 4)

where Range is the difference between the highest and lowest data points in the dataset.

It is important to note that these methods provide an estimation of the standard deviation, and the accuracy of the estimation depends on the quality and size of the dataset. In general, the larger the sample size, the more accurate the estimation will be."
500,"Consider a source X with a distortion measure $d(x, \hat{x})$ that satisfies the following property: all columns of the distortion matrix are permutations of the set $\{d_1, d_2, \ldots, d_m\}$. The function $\phi(D) = \max_{b:\sum_{i=1}^m p_i d_i \leq D} H(p)$ is concave. True or False?",True,bool,NONE,textbook 10.6(a),xinyi/concavity.json,convexity,Mathematical analysis,Math,"Convexity, in mathematical analysis, is a property of certain sets and functions that helps to understand their shape and behavior. It is an important concept in various fields such as optimization, geometry, and economics. There are two main aspects of convexity: convex sets and convex functions.

1. Convex Sets: A set S in a real vector space (or Euclidean space) is called convex if, for any two points x and y in S, the line segment connecting x and y lies entirely within S. In other words, if x, y ∈ S and 0 ≤ t ≤ 1, then tx + (1-t)y ∈ S. Geometrically, this means that a convex set has no ""holes"" or ""dents"" in its shape, and if you were to stretch a rubber band around the set, it would lie entirely on the boundary of the set.

Examples of convex sets include:
- The empty set and any single point
- Line segments, triangles, rectangles, and other convex polygons in the plane
- Spheres, ellipsoids, and other convex polyhedra in three-dimensional space

2. Convex Functions: A function f: R^n → R is called convex if its domain is a convex set and for any two points x and y in the domain, the function value at any point on the line segment connecting x and y is less than or equal to the weighted average of the function values at x and y. Mathematically, if x, y ∈ domain of f and 0 ≤ t ≤ 1, then f(tx + (1-t)y) ≤ tf(x) + (1-t)f(y). 

Convex functions have a few important properties:
- Their graphs always lie above their tangent lines (if they are differentiable)
- They have a unique global minimum (if they are continuous)
- They are closed under addition and positive scalar multiplication

Examples of convex functions include:
- Linear functions, such as f(x) = ax + b
- Quadratic functions, such as f(x) = ax^2 + bx + c, where a > 0
- Exponential functions, such as f(x) = e^(ax), where a > 0

Convexity plays a crucial role in optimization problems, as it ensures that there are no local minima other than the global minimum, making it easier to find the optimal solution. Additionally, convexity is used in various applications, such as economics (to model utility functions and production functions), machine learning (to design efficient algorithms), and geometry (to study the properties of convex shapes)."
176,Suppose a fair coin is tossed 50 times. The bound on the probability that the number of heads will be greater than 35 or less than 15 can be found using Chebyshev's Inequality. What is the upper bound of the probability?,0.125,float,NONE,"website | https://www.learndatasci.com/glossary/chebyshevs-inequality/#:~:text=Chebyshev's%20inequality%20is%20a%20theory,deviations)%20from%20the%20distribution's%20average.",wenhuchen/chebyshev1.json,chebyshev's inequality,Statistics,Math,"Chebyshev's Inequality, also known as Chebyshev's Theorem, is a fundamental concept in probability theory and statistics that provides a bound on the probability of a random variable deviating from its mean. It is named after the Russian mathematician Pafnuty Chebyshev, who first formulated the inequality in the 19th century.

The inequality states that for any random variable X with a finite mean (μ) and a finite non-zero variance (σ^2), the probability that the absolute difference between X and its mean is at least k standard deviations (where k is a positive constant) is at most 1/k^2. Mathematically, it can be expressed as:

P(|X - μ| ≥ kσ) ≤ 1/k^2

Chebyshev's Inequality is a general result that applies to any probability distribution, regardless of its shape or whether it is continuous or discrete. It is particularly useful when little is known about the underlying distribution of the data, as it provides a conservative estimate of the probability of extreme values.

The main implication of Chebyshev's Inequality is that the majority of the values of a random variable will be concentrated around its mean, within a certain number of standard deviations. For example, at least 75% of the values will be within 2 standard deviations of the mean, and at least 89% of the values will be within 3 standard deviations of the mean. This result is weaker than the more specific 68-95-99.7 rule for normal distributions, but it applies to all distributions with finite mean and variance."
160,"An investor is looking to purchase a security for $100 with an initial margin of 50% (meaning the investor is using $50 of his money to purchase the security and borrowing the remaining $50 from a broker). In addition, the maintenance margin is 25%. At what price of the security will the investor receive a margin call?",66.67,float,NONE,website | https://corporatefinanceinstitute.com/resources/wealth-management/margin-call/,xueguangma/margin_call.json,margin call,Equity investments,Finance,"A margin call in equity investments refers to a situation where a broker demands that an investor deposit additional funds or securities into their margin account to maintain the minimum required level of equity. This typically occurs when the value of the investor's account falls below the maintenance margin requirement due to a decline in the value of the securities held in the account.

Margin trading allows investors to borrow money from their broker to purchase securities, using the securities in their account as collateral. The investor's equity in the account is the difference between the market value of the securities and the amount borrowed from the broker. The maintenance margin is the minimum percentage of equity that must be maintained in the account at all times, usually around 25% to 30%.

When the value of the securities in the account declines, the investor's equity decreases, and if it falls below the maintenance margin requirement, the broker issues a margin call. The investor must then either deposit additional funds or sell some of the securities in the account to bring the equity back up to the required level. If the investor fails to meet the margin call, the broker has the right to sell the securities in the account to cover the outstanding loan, potentially resulting in significant losses for the investor.

Margin calls are a risk associated with margin trading and can lead to forced liquidation of assets at unfavorable prices. To avoid margin calls, investors should carefully monitor their account balances, diversify their portfolios, and avoid over-leveraging their investments."
727,Please solve the equation sin(4*x) + x = 54 and provide all the roots using newton-raphson method.,"[53.52, 54.25, 54.76]",list of float,NONE,self,wenhuchen/newton4.json,newton-raphson method,Numerical analysis,Math,"The Newton-Raphson method, also known as the Newton's method, is a widely used iterative numerical technique for finding the approximate roots of a real-valued function. It is named after Sir Isaac Newton and Joseph Raphson, who independently developed the method in the 17th century.

The method is based on the idea of linear approximation, where a function is approximated by its tangent line at a given point. The intersection of this tangent line with the x-axis provides a better approximation of the root than the initial point. This process is then repeated iteratively until the desired level of accuracy is achieved.

Given a function f(x) and an initial guess x0 for the root, the Newton-Raphson method can be described by the following iterative formula:

x1 = x0 - f(x0) / f'(x0)

Here, f'(x0) is the derivative of the function f(x) evaluated at the point x0. The new approximation x1 is then used as the starting point for the next iteration, and the process is repeated until the difference between successive approximations is smaller than a predefined tolerance level or a maximum number of iterations is reached.

The Newton-Raphson method converges rapidly when the initial guess is close to the actual root and the function is well-behaved. However, the method may fail to converge or converge to a wrong root if the initial guess is not close enough to the actual root, or if the function has multiple roots, or if the derivative of the function is zero or nearly zero at the root.

Despite these limitations, the Newton-Raphson method is widely used in various fields of science and engineering due to its simplicity and fast convergence properties when applied to well-behaved functions."
505,"Electrons used to produce medical x rays are accelerated from rest through a potential difference of 25,000 volts before striking a metal target. Calculate the speed of the electrons in m/s.",90000000.0,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition,tonyxia/relativity2.json,relativity,Relativity,Physics,"Relativity is a scientific theory that fundamentally changed our understanding of space, time, and gravity. It was first introduced by the renowned physicist Albert Einstein in the early 20th century and consists of two parts: the Special Theory of Relativity and the General Theory of Relativity.

1. Special Theory of Relativity (1905): This theory deals with objects moving at constant speeds, particularly those moving close to the speed of light. It is based on two main principles:

   a. The Principle of Relativity: The laws of physics are the same for all observers in uniform motion relative to one another.
   b. The Constancy of the Speed of Light: The speed of light in a vacuum is the same for all observers, regardless of their motion or the motion of the light source.

The Special Theory of Relativity led to several counterintuitive conclusions, such as time dilation (moving clocks run slower), length contraction (moving objects appear shorter), and the equivalence of mass and energy (E=mc^2), which states that mass can be converted into energy and vice versa.

2. General Theory of Relativity (1915): This theory is an extension of the Special Theory of Relativity and deals with gravity. It describes gravity not as a force between masses, as proposed by Sir Isaac Newton, but as a curvature of spacetime caused by the presence of mass. In other words, massive objects like planets and stars warp the fabric of spacetime, causing other objects to move along curved paths.

The General Theory of Relativity has been confirmed through various experiments and observations, such as the bending of light around massive objects (gravitational lensing), the shift in the orbit of Mercury, and the detection of gravitational waves.

In summary, Relativity is a groundbreaking theory that has reshaped our understanding of the universe, providing a more accurate and comprehensive description of the fundamental concepts of space, time, and gravity."
621,"A state issues a 15 year $1000 bond that pays $25 every six months. If the current market interest rate is 4%, what is the fair market value of the bond?",1111.97,float,NONE,website | https://math.libretexts.org/Bookshelves/Applied_Mathematics/Applied_Finite_Mathematics_(Sekhon_and_Bloom)/06%3A_Mathematics_of_Finance/6.05%3A_Miscellaneous_Application_Problems,xueguangma/fair_market_value_of_a_bond.json,fair market value,Fixed income,Finance,"Fair Market Value (FMV) in the context of fixed income refers to the estimated price at which a fixed income security, such as a bond or a note, would trade in a competitive and open market. It represents the value that a buyer and seller would agree upon, assuming both parties have adequate knowledge of the asset and are not under any pressure to buy or sell.

Fixed income securities are debt instruments that pay a fixed interest rate over a specified period. They include government bonds, corporate bonds, municipal bonds, and other debt instruments. The fair market value of these securities is influenced by various factors, including interest rates, credit quality, time to maturity, and market conditions.

To determine the fair market value of a fixed income security, the following factors are typically considered:

1. Interest rates: The prevailing interest rates in the market have a significant impact on the value of fixed income securities. When interest rates rise, the value of existing bonds with lower coupon rates tends to decrease, as investors seek higher-yielding investments. Conversely, when interest rates fall, the value of existing bonds with higher coupon rates tends to increase.

2. Credit quality: The creditworthiness of the issuer also affects the fair market value of fixed income securities. If the issuer's credit rating is downgraded, the value of its bonds may decrease, as investors perceive a higher risk of default. On the other hand, if the issuer's credit rating is upgraded, the value of its bonds may increase, as investors perceive a lower risk of default.

3. Time to maturity: The time remaining until the bond's maturity date also influences its fair market value. Bonds with longer maturities are generally more sensitive to changes in interest rates and credit quality, as there is a longer period for potential changes in these factors to impact the bond's value.

4. Market conditions: The overall market conditions, including supply and demand for fixed income securities, can also impact their fair market value. If there is a high demand for bonds, their prices may increase, while a low demand may lead to a decrease in prices.

In summary, the fair market value of fixed income securities is determined by various factors, including interest rates, credit quality, time to maturity, and market conditions. It represents the price at which a fixed income security would likely trade in an open and competitive market, with both buyer and seller having adequate knowledge of the asset and not being under any pressure to transact."
270,"If the sum-product algorithm is run on a factor graph with a tree structure (no loops), then after a finite number of messages have been sent, there will be no pending messages. True or false?",True,bool,NONE,textbook 8.29,xinyi/sum_product_algorithm.json,message passing algorithm,Graph theory,EECS,"Message Passing algorithm, also known as Belief Propagation or Sum-Product algorithm, is a technique used in Graph Theory for performing inference on graphical models, such as Bayesian networks and Markov random fields. It is particularly useful for solving problems in areas like error-correcting codes, artificial intelligence, and computer vision.

The main idea behind the Message Passing algorithm is to propagate local information (or beliefs) through the graph structure to compute global information (or beliefs) efficiently. The algorithm operates on a factor graph, which is a bipartite graph representing the factorization of a global function into a product of local functions.

Here's a high-level description of the Message Passing algorithm:

1. Initialization: Each node in the graph initializes its local belief and sends a message to its neighboring nodes. The message typically contains information about the node's current belief or probability distribution.

2. Iterative message passing: Nodes in the graph iteratively update their beliefs based on the messages received from their neighbors. This process continues until the beliefs converge or a maximum number of iterations is reached.

3. Termination: Once the beliefs have converged or the maximum number of iterations is reached, the algorithm terminates, and the final beliefs represent the approximate marginal probabilities or beliefs of each node in the graph.

The Message Passing algorithm can be applied to both discrete and continuous domains, and it can be adapted for various types of graphical models, such as directed and undirected graphs. The algorithm's efficiency comes from its ability to exploit the graph's structure and the local nature of the interactions between nodes, which allows for parallel and distributed computation.

However, it is important to note that the Message Passing algorithm is not guaranteed to converge or provide exact results in all cases, especially for graphs with loops or cycles. In such cases, approximate inference techniques like Loopy Belief Propagation or Generalized Belief Propagation can be used to obtain approximate solutions."
94,Let’s assume Mutual Fund A has an annualized return of 15% and a downside deviation of 8%. Mutual Fund B has an annualized return of 12% and a downside deviation of 5%. The risk-free rate is 2.5%. What is the Sortino ratio for Fund A?,1.56,float,NONE,website | https://www.wallstreetmojo.com/risk-adjusted-returns,xueguangma/sortino_ratio.json,sortino's ratio,Portfolio management,Finance,"Sortino Ratio is a financial metric used in portfolio management to evaluate the risk-adjusted performance of an investment portfolio. It was developed by Frank A. Sortino as an improvement over the widely used Sharpe Ratio. The Sortino Ratio measures the excess return of a portfolio relative to its downside risk, which is the risk of negative returns or losses.

The Sortino Ratio is calculated using the following formula:

Sortino Ratio = (Portfolio Return - Risk-Free Rate) / Downside Risk

Where:
- Portfolio Return is the average return of the investment portfolio over a specific period.
- Risk-Free Rate is the return on a risk-free investment, such as a treasury bond, over the same period.
- Downside Risk is the standard deviation of the negative returns or losses, also known as downside deviation.

The key difference between the Sortino Ratio and the Sharpe Ratio is the way they measure risk. While the Sharpe Ratio considers the total risk or volatility of the portfolio (both upside and downside), the Sortino Ratio focuses only on the downside risk, which is more relevant for investors as they are primarily concerned about potential losses.

A higher Sortino Ratio indicates better risk-adjusted performance, as it means the portfolio is generating higher returns relative to its downside risk. Investors and portfolio managers can use the Sortino Ratio to compare the performance of different portfolios or investment strategies, taking into account their risk profiles. It helps them make more informed decisions about asset allocation and risk management."
494,"Suppose Host A wants to send a large file to Host B. The path from Host A to Host B has three links, of rates R1 = 500 kbps, R2 = 2 Mbps, and R3 = Mbps. Suppose the file is 4 million bytes. Dividing the file size by the throughput, roughly how many seconds will it take to transfer the file to Host B?",64,integer,NONE,website | https://vinesmsuic.github.io/notes-networkingIP-qa/,maxku/ipnetwork9-datatransmission.json,data communication,Computer networking,EECS,"Data Communication, also known as computer networking, is the process of exchanging information and data between multiple computing devices through a communication channel. This exchange of data can occur over wired or wireless connections, and the devices can be located in close proximity or across vast distances. The primary goal of data communication is to enable the sharing of resources, information, and services among different users and systems.

In computer networking, data is transmitted in the form of packets, which are small units of data that are sent from one device to another. These packets are transmitted through various networking devices, such as routers, switches, and hubs, which help direct the data to its intended destination.

There are several key components and concepts in data communication:

1. Nodes: These are the devices that participate in the network, such as computers, servers, printers, and smartphones.

2. Network Topology: This refers to the arrangement and interconnection of nodes in a network. Common network topologies include bus, star, ring, and mesh.

3. Protocols: These are the rules and conventions that govern how data is transmitted and received in a network. Examples of protocols include TCP/IP, HTTP, and FTP.

4. Transmission Media: This is the physical medium through which data is transmitted, such as copper wires, fiber optic cables, or wireless radio frequencies.

5. Bandwidth: This refers to the capacity of a communication channel to transmit data. It is usually measured in bits per second (bps) or bytes per second (Bps).

6. Latency: This is the time it takes for a data packet to travel from its source to its destination. Lower latency is generally preferred for faster communication.

7. Network Security: This involves protecting the network and its data from unauthorized access, misuse, or attacks.

Data communication plays a crucial role in modern society, enabling various applications such as the internet, email, file sharing, online gaming, and video conferencing. As technology continues to advance, data communication networks are becoming faster, more reliable, and more secure, allowing for even greater connectivity and collaboration among users and devices."
376,"Are the vectors v_1 = [1,2,3], v_2 = [4,5,6], v_3 = [7,8,9] linearly independent?",False,bool,NONE,"text | Otto Bretscher, Linear Algebra with Applications.",elainewan/math_algebra_3_2.json,linear independence,Algebra,Math,"Linear independence is a concept in algebra, particularly in linear algebra, that refers to the relationship between vectors in a vector space. A set of vectors is said to be linearly independent if none of the vectors in the set can be expressed as a linear combination of the other vectors. In other words, no vector in the set can be created by adding or subtracting multiples of the other vectors.

Mathematically, a set of vectors {v1, v2, ..., vn} is linearly independent if the only solution to the equation:

c1 * v1 + c2 * v2 + ... + cn * vn = 0

is when all the coefficients c1, c2, ..., cn are equal to zero. Here, 0 represents the zero vector.

If there exists a non-zero solution for the coefficients, then the set of vectors is said to be linearly dependent. In this case, at least one vector can be expressed as a linear combination of the others.

Linear independence is an important concept in various areas of mathematics and engineering, as it helps determine the dimension of a vector space, the basis for a vector space, and the rank of a matrix, among other applications."
126,Find the interval in which the smallest positive root of the following equations lies: tan x + tanh x = 0. Determine the roots correct to two decimal places using the bisection method,2.37,float,NONE,website | http://ndl.ethernet.edu.et/bitstream/123456789/79528/3/Numerical_Methods_Problems_and_Solutions_cropped.pdf,wenhuchen/bisection1.json,bisection algorithm,Numerical analysis,Math,"The Bisection Algorithm, also known as the Binary Search Method or Interval Halving Method, is a numerical analysis technique used to find the root (zero) of a continuous function within a given interval. It is a simple, robust, and iterative method that works by repeatedly dividing the interval into two equal subintervals and selecting the subinterval where the function changes its sign, indicating the presence of a root.

Here's a step-by-step description of the Bisection Algorithm:

1. Define the continuous function f(x) and the interval [a, b] within which the root is to be found. Ensure that f(a) and f(b) have opposite signs, i.e., f(a) * f(b) < 0. This is based on the Intermediate Value Theorem, which guarantees the existence of a root within the interval.

2. Calculate the midpoint of the interval, c = (a + b) / 2.

3. Evaluate the function at the midpoint, f(c).

4. Check if f(c) is close enough to zero (within a specified tolerance) or if the maximum number of iterations has been reached. If either condition is met, the algorithm stops, and the midpoint c is considered as the approximate root.

5. If f(c) is not close enough to zero, determine the new interval [a, c] or [c, b] based on the sign of f(c). If f(a) * f(c) < 0, the root lies in the interval [a, c], so set b = c. If f(b) * f(c) < 0, the root lies in the interval [c, b], so set a = c.

6. Repeat steps 2-5 until the desired accuracy is achieved or the maximum number of iterations is reached.

The Bisection Algorithm is guaranteed to converge to the root, but it may be slower compared to other numerical methods like the Newton-Raphson method or the Secant method. However, its simplicity and robustness make it a popular choice for solving various problems in numerical analysis."
361,"Consider a probability density $p_x(x)$ defined over a continuous variable x, and suppose that we make a nonlinear change of variable using $x = g(y)$. In the case of a linear transformation, the location of the maximum density transforms in the same way as the variable itself.",True,bool,NONE,textbook 1.4,xinyi/change_of_variable_linear.json,change of variable theorem,Probability theory,Math,"The Change of Variable Theorem, also known as the Transformation Theorem, is a fundamental concept in probability theory that deals with the transformation of random variables. It allows us to find the probability distribution of a new random variable that is derived from an existing random variable through a deterministic function.

Suppose we have a random variable X with a known probability density function (pdf) f_X(x) and a cumulative distribution function (cdf) F_X(x). Let Y = g(X) be a new random variable obtained by applying a function g(x) to X, where g(x) is a continuous and differentiable function with an inverse function g^(-1)(y).

The Change of Variable Theorem states that the pdf of the transformed random variable Y, denoted as f_Y(y), can be obtained using the following formula:

f_Y(y) = f_X(x) * |(dg^(-1)(y) / dy)|

where x = g^(-1)(y) and |(dg^(-1)(y) / dy)| is the absolute value of the derivative of the inverse function g^(-1)(y) with respect to y.

The theorem essentially provides a method to compute the pdf of the transformed random variable Y by considering the pdf of the original random variable X and the transformation function g(x). This is particularly useful in various applications, such as statistical modeling and hypothesis testing, where we often need to work with transformed random variables."
556,Is x-1 a factor of 2*x^4+3*x^2-5x+7?,False,bool,NONE,self,wenhuchen/factor's_theory.json,factor's theorem,Algebra,Math,"Factor's Theorem, also known as the Factor Theorem, is a fundamental result in algebra that establishes a relationship between the factors of a polynomial and its roots. It states that if a polynomial f(x) has a root r, then (x-r) is a factor of the polynomial, and conversely, if (x-r) is a factor of the polynomial, then r is a root of the polynomial.

Mathematically, the Factor Theorem can be expressed as follows:

If f(r) = 0 for some value r, then (x-r) is a factor of f(x).

And conversely,

If (x-r) is a factor of f(x), then f(r) = 0.

The Factor Theorem is a special case of the Remainder Theorem, which states that when a polynomial f(x) is divided by (x-r), the remainder is f(r). If f(r) = 0, then the remainder is zero, and (x-r) is a factor of f(x).

The Factor Theorem is useful for finding the factors of a polynomial and solving polynomial equations. By identifying the roots of a polynomial, we can determine its factors and vice versa. This theorem is particularly helpful when dealing with polynomials of higher degrees, as it simplifies the process of finding factors and roots."
349,"For the equation x^4 + 2*x^3 + x = 10, there are four roots. What is the sum of the roots using newton-raphson method.",-2.0,float,NONE,self,wenhuchen/newton2.json,newton-raphson method,Numerical analysis,Math,"The Newton-Raphson method, also known as the Newton's method, is a widely used iterative numerical technique for finding the approximate roots of a real-valued function. It is named after Sir Isaac Newton and Joseph Raphson, who independently developed the method in the 17th century.

The method is based on the idea of linear approximation, where a function is approximated by its tangent line at a given point. The intersection of this tangent line with the x-axis provides a better approximation of the root than the initial point. This process is then repeated iteratively until the desired level of accuracy is achieved.

Given a function f(x) and an initial guess x0 for the root, the Newton-Raphson method can be described by the following iterative formula:

x1 = x0 - f(x0) / f'(x0)

Here, f'(x0) is the derivative of the function f(x) evaluated at the point x0. The new approximation x1 is then used as the starting point for the next iteration, and the process is repeated until the difference between successive approximations is smaller than a predefined tolerance level or a maximum number of iterations is reached.

The Newton-Raphson method converges rapidly when the initial guess is close to the actual root and the function is well-behaved. However, the method may fail to converge or converge to a wrong root if the initial guess is not close enough to the actual root, or if the function has multiple roots, or if the derivative of the function is zero or nearly zero at the root.

Despite these limitations, the Newton-Raphson method is widely used in various fields of science and engineering due to its simplicity and fast convergence properties when applied to well-behaved functions."
219,Use the linear approximation to estimate (3.99)^3 (1.01)^4 (1.98)^{-1}.,33.36,float,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_15.json,linear approximation and differentials,Calculus,Math,"Linear Approximation, also known as Tangent Line Approximation or Linearization, is a method used in calculus to approximate the value of a function near a specific point using the tangent line at that point. It is based on the idea that a function can be approximated by a straight line (tangent line) when we are close to a particular point.

The linear approximation of a function f(x) at a point x=a is given by the equation:

L(x) = f(a) + f'(a)(x - a)

where L(x) is the linear approximation, f(a) is the value of the function at x=a, f'(a) is the derivative of the function at x=a, and (x - a) represents the change in the input variable x.

Differentials, on the other hand, are used to describe the change in a function's output with respect to a change in its input. In calculus, the differential of a function f(x) is denoted as df(x) or dy, and it represents the change in the output (y) as the input (x) changes by a small amount, denoted as dx.

The differential of a function f(x) is given by the equation:

df(x) = f'(x) dx

where f'(x) is the derivative of the function with respect to x, and dx is the change in the input variable x.

Both linear approximation and differentials are closely related concepts in calculus, as they both deal with approximating the behavior of a function near a specific point. Linear approximation uses the tangent line to estimate the function's value, while differentials describe the change in the function's output as the input changes by a small amount."
479,"Coloring the edges of a complete graph with 6 vertices in 2 colors, how many triangles of the same color are there at least?",2,integer,NONE,self,jianyu_xu/Ramsey_1.json,ramsey's theorem,Combinatorics,Math,"Ramsey's theorem is a fundamental result in combinatorics, specifically in the area of graph theory and combinatorial mathematics. It is named after the British mathematician Frank P. Ramsey, who first stated the theorem in 1930. The theorem deals with the conditions under which order must appear in a large enough structure, even if that structure is initially disordered or chaotic.

In its simplest form, Ramsey's theorem states that for any given positive integers m and n, there exists a least positive integer R(m, n) such that any graph with at least R(m, n) vertices will contain either a clique of size m (a complete subgraph where every pair of vertices is connected by an edge) or an independent set of size n (a set of vertices where no two vertices are connected by an edge).

In other words, if you have a large enough graph, it is impossible to avoid having either a large complete subgraph or a large independent set, regardless of how the edges are arranged.

Ramsey's theorem can also be extended to more complex structures, such as hypergraphs and infinite graphs, and can be generalized to deal with multiple colors or more complicated combinatorial objects. The theorem has important applications in various fields, including computer science, logic, and number theory.

However, despite its significance, Ramsey's theorem is known for its non-constructive nature, meaning that it guarantees the existence of a certain structure but does not provide an explicit way to find or construct it. Additionally, the bounds for R(m, n) are often very large and difficult to compute, which limits the practical applications of the theorem."
337,"Let L^1[0,2] be the space of all the Lebesgue integrable functions on the interval [0,2], and C[0,2] be the space of all the continuous functions on the interval [0,2]. Suppose H=L^1[0,2], and X=C[0,2]. For any f\in L^1[0,2], define operator T as $(Tf)(x)=\int_0^x f(t)dt$. For the linear operator T from H to X, what is the norm of T? For the linear operator T from H to H, what is the norm of T? Return the answers of two questions as a list. For example, if the norm for the first question is 2, the second is 3, then return [2,3].","[1, 2]",list of integer,NONE,functional analysis: 2.1 example 6,mingyin/complete-metric-space2.json,complete space,Functional analysis,Math,"In functional analysis, a complete space, also known as a complete metric space, is a metric space in which every Cauchy sequence converges to a limit within the space. A metric space is a set equipped with a distance function (or metric) that defines the distance between any two points in the set.

To understand the concept of a complete space, it is essential to know what a Cauchy sequence is. A Cauchy sequence is a sequence of elements in a metric space such that the distance between any two elements in the sequence becomes arbitrarily small as the sequence progresses. In other words, the elements of the sequence get closer and closer together as the sequence goes on.

A complete space is a metric space with the property that every Cauchy sequence converges to a limit within the space. This means that, given any Cauchy sequence in the space, there exists an element in the space that the sequence converges to. This is an important property in functional analysis because it ensures that certain mathematical operations, such as taking limits, can be performed within the space without having to worry about the limit being outside the space.

An example of a complete space is the set of real numbers with the usual distance function (the absolute difference between two numbers). Every Cauchy sequence of real numbers converges to a real number, so the space is complete. On the other hand, the set of rational numbers is not complete, as there are Cauchy sequences of rational numbers that do not converge to a rational number (e.g., a sequence converging to an irrational number like the square root of 2).

In functional analysis, complete spaces play a crucial role in the study of function spaces, linear operators, and other related concepts. One of the most important types of complete spaces in functional analysis is a Banach space, which is a complete normed vector space. Banach spaces are fundamental in the study of various problems in analysis, differential equations, and optimization."
482,Let f be an entire function such that |f(z)| $\geq$ 1 for every z in C. Is f is a constant function?,True,bool,NONE,website | https://byjus.com/maths/liouvilles-theorem/,wenhuchen/Liouville’s_theorem2.json,liouville's theorem,Complex analysis,Math,"Liouville's theorem is a fundamental result in complex analysis that states that every bounded entire function must be constant. In other words, if a function is holomorphic (analytic) on the entire complex plane and its absolute value is bounded, then the function is a constant function.

To break it down further:

1. Bounded: A function f is said to be bounded if there exists a positive number M such that |f(z)| ≤ M for all z in the complex plane. In other words, the function's values do not grow arbitrarily large as you move around the complex plane.

2. Entire function: A function is called entire if it is holomorphic (analytic) on the entire complex plane. This means that the function is differentiable at every point in the complex plane and has a convergent power series representation in a neighborhood of each point.

Liouville's theorem has important implications in complex analysis, as it helps to identify constant functions and plays a crucial role in the proof of the fundamental theorem of algebra, which states that every non-constant polynomial has at least one complex root. The theorem is named after the French mathematician Joseph Liouville, who first proved it in 1844."
261,"James (mass 90.0 kg) and Ramon (mass 60.0 kg) are 20.0 m apart on a frozen pond. Midway between them is a mug of their favorite beverage. They pull on the ends of a light rope stretched between them. When James has moved 6.0 m toward the mug, how far has Ramon moved? (Unit: m)",1.0,float,NONE,University Physics with Modern Physics | Example 8.14,panlu/center_of_mass1.json,center of mass,Classic mechanics,Physics,"In classical mechanics, the center of mass (COM) is a fundamental concept that describes the average position of all the particles in a system, weighted by their masses. It is a point in space where the mass of an object or a group of objects is considered to be concentrated, and it serves as a reference point for analyzing the motion and forces acting on the system.

Mathematically, the center of mass can be calculated using the following formula:

COM = (Σm_i * r_i) / Σm_i

where m_i is the mass of the ith particle, r_i is the position vector of the ith particle, and the summation is over all the particles in the system.

In simpler terms, the center of mass is the weighted average of the positions of all the particles in the system, with the weights being their respective masses.

The center of mass has several important properties:

1. The motion of the center of mass is determined by the external forces acting on the system. If no external forces are acting on the system, the center of mass will move with a constant velocity.

2. The center of mass can be used to simplify the analysis of complex systems, as it allows us to treat the entire system as a single particle with the total mass concentrated at the center of mass.

3. The center of mass can be inside or outside the physical boundaries of an object or system. For example, in a hollow sphere, the center of mass is at the geometric center, even though there is no mass at that point.

4. The center of mass is independent of the orientation of the coordinate system used to describe the positions of the particles.

In summary, the center of mass is a crucial concept in classical mechanics that helps simplify the analysis of the motion and forces acting on a system by providing a single reference point representing the average position of all particles in the system, weighted by their masses."
39,"Consider the matrix of A=[[1, 4], [4, 1]], is this a positive definite matrix?",False,bool,NONE,website | https://www.math.usm.edu/lambers/mat419/lecture3.pdf,wenhuchen/definite_matrix1.json,definite matrix criteria,Algebra,Math,"In algebra, a definite matrix is a square matrix that has certain properties related to its eigenvalues or determinants. These properties help classify the matrix as positive definite, negative definite, positive semi-definite, or negative semi-definite. Here are the criteria for each type of definite matrix:

1. Positive Definite Matrix:
A square matrix A is positive definite if:
   a. All its eigenvalues are positive.
   b. All its leading principal minors (determinants of the top-left submatrices) are positive.
   c. For any non-zero vector x, the quadratic form x^T * A * x is positive, i.e., x^T * A * x > 0.

2. Negative Definite Matrix:
A square matrix A is negative definite if:
   a. All its eigenvalues are negative.
   b. Its leading principal minors alternate in sign, starting with a negative determinant for the first order minor.
   c. For any non-zero vector x, the quadratic form x^T * A * x is negative, i.e., x^T * A * x < 0.

3. Positive Semi-Definite Matrix:
A square matrix A is positive semi-definite if:
   a. All its eigenvalues are non-negative (positive or zero).
   b. All its leading principal minors are non-negative.
   c. For any vector x, the quadratic form x^T * A * x is non-negative, i.e., x^T * A * x ≥ 0.

4. Negative Semi-Definite Matrix:
A square matrix A is negative semi-definite if:
   a. All its eigenvalues are non-positive (negative or zero).
   b. Its leading principal minors alternate in sign, starting with a non-positive determinant for the first order minor.
   c. For any vector x, the quadratic form x^T * A * x is non-positive, i.e., x^T * A * x ≤ 0.

These criteria help determine the definiteness of a matrix, which is useful in various applications, such as optimization problems, stability analysis, and solving linear systems."
626,"Suppose V is a finite-dimensional vector space on F. $M1={a_1,a_2,a_3}$ is a basis of V, $M2={b_1,b_2,b_3}$ is another basis of V. Suppose the coordinates of b_1,b_2,b_3 under M1 are $c_1=(1,1,-1),c_2=(1,-1,1),c_3=(-1,1,1)$. Suppose the coordinate of $d\in V$ under M1 is (1,3,5). What is the coordinate of d under M2? Return the three coordinate values as a list.","[2, 3, 4]",list of integer,NONE,linear algebra 2.6 example 2,mingyin/gaussian-elimination3.json,gaussian elimination,Algebra,Math,"Gaussian elimination, also known as row reduction, is an algebraic method used to solve systems of linear equations. It involves performing a series of operations on the augmented matrix (a matrix that combines the coefficients and constants of the linear equations) to transform it into a simpler form, called the row echelon form or the reduced row echelon form. This simplified form makes it easier to find the solutions to the system of linear equations.

The main operations used in Gaussian elimination are:

1. Swapping two rows.
2. Multiplying a row by a nonzero constant.
3. Adding or subtracting a multiple of one row to another row.

The goal of Gaussian elimination is to create a triangular matrix with zeros below the main diagonal (row echelon form) or zeros both below and above the main diagonal (reduced row echelon form). Once the matrix is in one of these forms, the solutions can be found using a technique called back-substitution.

Here's a step-by-step description of the Gaussian elimination process:

1. Start with the augmented matrix representing the system of linear equations.
2. Identify the leftmost column that has a nonzero entry.
3. If the top entry in that column is zero, swap the row with another row below it that has a nonzero entry in that column.
4. Divide the row by the leading entry (the first nonzero entry from the left) to make it 1. This is called the pivot.
5. Use the pivot row to eliminate all nonzero entries below the pivot by adding or subtracting multiples of the pivot row to the rows below it.
6. Repeat steps 2-5 for the remaining submatrix (the matrix formed by removing the rows and columns that have already been processed) until the entire matrix is in row echelon form.
7. (Optional) To obtain the reduced row echelon form, eliminate the nonzero entries above the pivots by adding or subtracting multiples of the pivot rows to the rows above them.
8. Use back-substitution to find the solutions to the system of linear equations.

Gaussian elimination is a fundamental technique in linear algebra and has numerous applications in fields such as engineering, physics, computer science, and economics."
729,"In how many ways can a convex polygon with 8 sides be divided into triangles by connecting its vertices, with no intersecting lines?",132,integer,NONE,self,jianyu_xu/Catalan_3.json,catalan-mingantu number,Combinatorics,Math,"The Catalan-Mingantu numbers, also known as the Catalan numbers, are a sequence of natural numbers that have various applications in combinatorial mathematics, including counting certain types of lattice paths, the number of expressions containing n pairs of parentheses that are correctly matched, and the number of ways to triangulate a polygon with n+2 sides.

The Catalan numbers can be defined recursively as follows:

C(0) = 1
C(n) = Σ [C(i) * C(n-i-1)] for i = 0 to n-1, where n ≥ 1

Alternatively, they can be defined using the binomial coefficient:

C(n) = (1 / (n + 1)) * (2n choose n) = (2n)! / [(n + 1)! * n!]

The first few Catalan numbers are: 1, 1, 2, 5, 14, 42, 132, and so on.

The term ""Mingantu"" in the name ""Catalan-Mingantu numbers"" refers to the Mongolian mathematician Mingantu, who independently discovered the sequence in the 18th century. However, the sequence is more commonly known as the Catalan numbers, named after the French-Belgian mathematician Eugène Charles Catalan, who introduced them in the 19th century."
530,"The root of the equation x = (1 / 2) + sin x by using the iteration method: x_{k+1} = 1/2 + sin(x_k), x_0 = 1 correct to o six decimals is x = 1.497300. Determine the number of iteration steps required to reach the root by linear iteration. If the Aitken ∆2-process is used after three approximations are available, how many iterations are required?",3,integer,NONE,website | http://ndl.ethernet.edu.et/bitstream/123456789/79528/3/Numerical_Methods_Problems_and_Solutions_cropped.pdf,wenhuchen/Aiken's_theorem.json,aitken process,Numerical analysis,Math,"The Aitken process, also known as Aitken's delta-squared process, is a numerical analysis technique used to accelerate the convergence of a sequence of approximations. It was developed by the Scottish mathematician Alexander Aitken in the 1920s. The method is particularly useful for improving the convergence rate of slowly converging sequences or for refining the results of other numerical methods.

The Aitken process is based on the idea of extrapolation, which involves using the given sequence of approximations to estimate a better approximation. The method works by constructing a new sequence from the original sequence, with the new sequence converging more rapidly to the desired limit.

Given a sequence of approximations {x_n}, the Aitken process generates a new sequence {y_n} using the following formula:

y_n = x_n - (x_{n+1} - x_n)^2 / (x_{n+2} - 2x_{n+1} + x_n)

Here, x_n, x_{n+1}, and x_{n+2} are consecutive terms in the original sequence, and y_n is the corresponding term in the new sequence generated by the Aitken process.

The Aitken process can be applied iteratively to further improve the convergence rate. However, it is important to note that the method is not universally applicable and may not always lead to faster convergence. It works best for sequences that exhibit linear convergence, and its effectiveness may be limited for sequences with erratic behavior or poor initial approximations.

In summary, the Aitken process is a numerical analysis technique used to accelerate the convergence of a sequence of approximations. It is based on the idea of extrapolation and works by constructing a new sequence with improved convergence properties from the original sequence. The method is particularly useful for refining the results of other numerical methods and improving the convergence rate of slowly converging sequences."
100,"If polygon ACDF is similar to polygon VWYZ, AF = 12, CD = 9, YZ = 10, YW = 6, and ZV = 3y-1, find y.",3,integer,NONE,Geometry Textbook | 14.png,panlu/similarity2.json,similarity,Geometry,Math,"Similarity in geometry refers to the relationship between two shapes or figures that have the same shape but may have different sizes. In other words, two geometric figures are similar if they have the same proportions and angles, but their side lengths may be different. This concept is particularly important in the study of triangles, polygons, and other geometric shapes.

When two figures are similar, their corresponding angles are congruent (equal in measure), and their corresponding sides are proportional, meaning that the ratio of the lengths of corresponding sides is constant. For example, if two triangles are similar, the ratio of the lengths of their corresponding sides will be the same, and their angles will be equal.

Similarity can be determined using various methods, such as the Angle-Angle (AA) criterion, Side-Side-Side (SSS) criterion, and Side-Angle-Side (SAS) criterion. These criteria help to establish the similarity between two geometric figures by comparing their angles and side lengths.

In summary, similarity in geometry is a concept that deals with the comparison of shapes and figures based on their proportions, angles, and side lengths. Two figures are considered similar if they have the same shape but may differ in size."
299,"Assuming we are underground, and the only thing we can observe is whether a person brings an umbrella or not. The weather could be either rainy or sunny. Assuming the P(rain)=0.6 and P(sunny)=0.4. Assuming the weather on day $k$ is dependent on the weather on day $k-1$. We can write the transition probability as P(sunny $\mid$ sunny) = P(rain $\mid$ rain) = 0.7. The person has 60\% chance to bring an umbrella when the weather is rainy, and 40\% chance to bring an umbrella when the weather is sunny, i.e. P(umbrella $\mid$ rain) = 0.6 and P(umbrella $\mid$ sunny) = 0.4. If we observe that the person (1) brought an umbrella on day 1, (2) did not bring an umbrella on day 2, (3) brought an umbrella on day 3, (4) did not bring an umbrella on day 4.  What are the most likely weather from day 1 to day 4? Return the answer as a list of binary values, where 1 represents rain and 0 represents sunny.","[1, 1, 1, 1]",list of integer,NONE,self,wenhuchen/viterbi3.json,viterbi algorithm,Stochastic process,Math,"The Viterbi Algorithm is a dynamic programming algorithm used for finding the most likely sequence of hidden states, known as the Viterbi path, in a Hidden Markov Model (HMM). It is named after its inventor, Andrew Viterbi, and is widely used in various applications such as speech recognition, natural language processing, and bioinformatics.

A Hidden Markov Model (HMM) is a statistical model that represents a stochastic process involving a sequence of observable events and hidden states. In an HMM, the observable events are generated by the hidden states, which follow a Markov chain. The Markov chain is characterized by the transition probabilities between hidden states, and the emission probabilities of observable events given the hidden states.

The Viterbi Algorithm works by finding the most probable path of hidden states that generates the observed sequence of events. It does this by iteratively computing the maximum probability of reaching each state at each time step, considering all possible paths that lead to that state. The algorithm uses dynamic programming to efficiently compute these probabilities and store them in a trellis structure.

Here's a high-level description of the Viterbi Algorithm:

1. Initialization: Set the initial probabilities for each hidden state, considering the initial state probabilities and the emission probabilities for the first observed event.

2. Recursion: For each subsequent observed event, compute the maximum probability of reaching each hidden state, considering all possible previous states and their transition probabilities. Update the emission probabilities for the current observed event.

3. Termination: Identify the hidden state with the highest probability at the last time step.

4. Traceback: Starting from the identified state in the termination step, backtrack through the trellis to find the most probable path of hidden states that generated the observed sequence.

The Viterbi Algorithm is an efficient and widely used method for decoding the hidden states in a Hidden Markov Model, providing valuable insights into the underlying structure of the stochastic process."
651,"Three years ago, Fred invested $10,000 in the shares of ABC Corp. Each year, the company distributed dividends to its shareholders. Each year, Fred received $100 in dividends. Note that since Fred received $100 in dividends each year, his total income is $300. Today, Fred sold his shares for $12,000. What is the holding period return of his investment?",0.23,float,NONE,website | https://corporatefinanceinstitute.com/resources/capital-markets/holding-period-return/,xueguangma/holding_period_return.json,holding period return,Portfolio management,Finance,"Holding Period Return (HPR) in portfolio management refers to the total return on an investment or a portfolio over a specific period of time. It is a comprehensive measure that takes into account all forms of returns, such as capital gains, dividends, interest, and other income generated by the investment during the holding period. HPR is often used by investors and portfolio managers to evaluate the performance of individual investments or the overall portfolio and to compare it with other investments or benchmarks.

The Holding Period Return is calculated using the following formula:

HPR = (Ending Value - Beginning Value + Income) / Beginning Value

Where:
- Ending Value is the market value of the investment or portfolio at the end of the holding period.
- Beginning Value is the market value of the investment or portfolio at the beginning of the holding period.
- Income refers to any dividends, interest, or other income generated by the investment during the holding period.

The result is expressed as a percentage, and a positive HPR indicates a gain, while a negative HPR indicates a loss on the investment.

HPR is useful for comparing the performance of different investments or portfolios over a specific period of time, as it takes into account both capital appreciation and income generated. However, it does not account for the risk associated with the investment or the time value of money, which are important factors to consider when evaluating investment performance."
580,"In triangle ABC, AB = 9x-1, CB = 5x-0.5, AC = 4x+1, and AC = CB. Find the measure of AB.",12.5,float,NONE,Geometry Textbook | 5.png,panlu/triangle1.json,isosceles triangle,Geometry,Math,"An isosceles triangle is a type of triangle in geometry that has two sides of equal length. These equal sides are called the legs of the triangle, while the third side, which is of a different length, is called the base. The angles opposite to the equal sides are also equal, and these are called the base angles. The angle between the two equal sides is called the vertex angle. 

In an isosceles triangle, the altitude drawn from the vertex to the base bisects the base and is also the perpendicular bisector of the base, creating two right triangles. The altitude also bisects the vertex angle, creating two congruent angles at the vertex.

Isosceles triangles have a unique property of symmetry, as they are symmetrical about the axis of symmetry, which is the altitude drawn from the vertex to the midpoint of the base."
402,"Perform 2 iterations with the Müller method for the following equation: log_{10}(x) - x + 3 = 0, x_0 = 1/4, x_1 = 1/2, x_2 = 1. What's the decimal value of x_3?",3.2,float,NONE,website | http://ndl.ethernet.edu.et/bitstream/123456789/79528/3/Numerical_Methods_Problems_and_Solutions_cropped.pdf,wenhuchen/mueller2.json,mueller's algorithm,Numerical analysis,Math,"Mueller's Algorithm is a numerical analysis method used for finding the roots (or zeros) of a real-valued function. It is an iterative method that generalizes the secant method and is particularly useful for finding complex roots of a function. The algorithm was developed by Peter Müller in 1956.

The main idea behind Mueller's Algorithm is to use three points (x0, x1, x2) on the function f(x) to approximate the function with a parabola (quadratic function) that passes through these points. The roots of this parabola are then used as the next approximations for the roots of the original function f(x).

Here's a step-by-step description of Mueller's Algorithm:

1. Choose three initial points x0, x1, and x2, such that f(x0), f(x1), and f(x2) are not equal to zero and x0, x1, and x2 are distinct.

2. Fit a parabola through the points (x0, f(x0)), (x1, f(x1)), and (x2, f(x2)). This can be done by solving a system of linear equations to find the coefficients a, b, and c of the quadratic function Q(x) = a(x - x2)^2 + b(x - x2) + c.

3. Find the roots of the quadratic function Q(x). These roots can be complex, and they are given by the quadratic formula: x = (-b ± √(b^2 - 4ac)) / 2a.

4. Choose one of the roots of Q(x) as the next approximation x3 for the root of f(x). The choice is usually based on which root is closer to x2 or which one has a smaller absolute value of f(x).

5. Update the points: x0 = x1, x1 = x2, and x2 = x3.

6. Check for convergence. If the difference between successive approximations (|x2 - x1| and |x1 - x0|) or the function value at the approximation (|f(x2)|) is smaller than a predefined tolerance, the algorithm has converged, and x2 is considered an approximation of the root. Otherwise, go back to step 2 and continue iterating.

Mueller's Algorithm is particularly useful for finding complex roots, as it can handle complex numbers in its calculations. However, like other iterative methods, it is not guaranteed to converge, and the choice of initial points can significantly affect the algorithm's performance."
406,The Chi-square statistic $\chi^2=\sum_c\frac{(P(x)-Q(x))^2}{Q(x)}$ is (twice) the first term in the Taylor series expansion of $D(P||Q)$ about $Q$. True or False?,True,bool,NONE,textbook 11.2,xinyi/chi_square_test.json,chi-square test,Statistics,Math,"The Chi-square test is a statistical method used to determine if there is a significant association between two categorical variables in a sample. It is a non-parametric test, meaning it does not assume any specific distribution for the underlying population. The test is based on comparing the observed frequencies in each category of a contingency table with the frequencies that would be expected under the assumption of independence between the variables (i.e., no association).

The Chi-square test involves the following steps:

1. Set up a contingency table: A contingency table is a matrix that displays the frequency distribution of the variables under study. The rows represent the categories of one variable, and the columns represent the categories of the other variable.

2. Calculate expected frequencies: Under the assumption of independence between the variables, the expected frequency for each cell in the table is calculated using the formula: (row total * column total) / grand total.

3. Compute the Chi-square statistic: The Chi-square statistic (χ²) is calculated using the formula: χ² = Σ [(observed frequency - expected frequency)² / expected frequency]. The summation is done over all cells in the contingency table.

4. Determine the degrees of freedom: The degrees of freedom (df) for the test is calculated as (number of rows - 1) * (number of columns - 1).

5. Compare the Chi-square statistic to the critical value: Using the calculated degrees of freedom and a chosen significance level (usually 0.05), the critical value of the Chi-square distribution is obtained. If the computed Chi-square statistic is greater than the critical value, the null hypothesis of independence between the variables is rejected, indicating a significant association between the variables.

The Chi-square test is widely used in various fields, including social sciences, biology, and marketing, to test the relationship between categorical variables. However, it has some limitations, such as being sensitive to sample size and not providing information about the strength or direction of the association."
711,"In Image processing, closing is a process in which first dilation operation is performed and then erosion operation is performed. Is it true?",True,bool,NONE,self,maxku/cv-imageprocessing2-morphology.json,image morphology,Signal processing,EECS,"Image Morphology is a technique in signal processing and computer vision that deals with the analysis and processing of geometrical structures within images. It is a subfield of mathematical morphology, which is a theory and technique for the analysis and processing of geometrical structures, based on set theory, lattice theory, topology, and random functions.

In the context of image processing, morphology focuses on the shape and structure of objects within an image, such as boundaries, skeletons, and convex hulls. The primary goal of image morphology is to extract, modify, or simplify the structure of objects in an image while preserving their essential features.

Morphological operations are typically performed on binary images (black and white) or grayscale images. Some common morphological operations include:

1. Erosion: This operation erodes the boundaries of the objects in the image, effectively shrinking them. It is useful for removing noise and small irregularities.

2. Dilation: This operation expands the boundaries of the objects in the image, effectively growing them. It is useful for filling gaps and connecting disjointed parts of an object.

3. Opening: This operation is a combination of erosion followed by dilation. It is useful for removing noise while preserving the shape of the objects in the image.

4. Closing: This operation is a combination of dilation followed by erosion. It is useful for filling gaps and connecting disjointed parts of an object while preserving the shape of the objects in the image.

5. Skeletonization: This operation reduces the objects in the image to their skeletal structure, which is a thin, connected representation of the object's shape.

6. Morphological gradient: This operation computes the difference between the dilation and erosion of an image, highlighting the boundaries of the objects in the image.

These operations can be combined and applied iteratively to achieve various image processing tasks, such as noise reduction, edge detection, object segmentation, and shape analysis. Image morphology is widely used in various applications, including computer vision, medical imaging, remote sensing, and pattern recognition."
303,"Assuming we are underground, and the only thing we can observe is whether a person brings an umbrella or not. The weather could be either rainy or sunny. Assuming the P(rain)=0.6 and P(sunny)=0.4. Assuming the weather on day $k$ is dependent on the weather on day $k-1$. We can write the transition probability as P(sunny $\mid$ sunny) = P(rain $\mid$ rain) = 0.55. The person has 60\% chance to bring an umbrella when the weather is rainy, and 40\% chance to bring an umbrella when the weather is sunny, i.e. P(umbrella $\mid$ rain) = 0.6 and P(umbrella $\mid$ sunny) = 0.4. If we observe that the person (1) brought an umbrella on day 1, (2) did not bring an umbrella on day 2, (3) brought an umbrella on day 3.  What are the most likely weather from day 1 to day 3? Return the answer as a list of binary values, where 1 represents rain and 0 represents sunny.","[1, 0, 1]",list of integer,NONE,self,wenhuchen/viterbi1.json,viterbi algorithm,Stochastic process,Math,"The Viterbi Algorithm is a dynamic programming algorithm used for finding the most likely sequence of hidden states, known as the Viterbi path, in a Hidden Markov Model (HMM). It is named after its inventor, Andrew Viterbi, and is widely used in various applications such as speech recognition, natural language processing, and bioinformatics.

A Hidden Markov Model (HMM) is a statistical model that represents a stochastic process involving a sequence of observable events and hidden states. In an HMM, the observable events are generated by the hidden states, which follow a Markov chain. The Markov chain is characterized by the transition probabilities between hidden states, and the emission probabilities of observable events given the hidden states.

The Viterbi Algorithm works by finding the most probable path of hidden states that generates the observed sequence of events. It does this by iteratively computing the maximum probability of reaching each state at each time step, considering all possible paths that lead to that state. The algorithm uses dynamic programming to efficiently compute these probabilities and store them in a trellis structure.

Here's a high-level description of the Viterbi Algorithm:

1. Initialization: Set the initial probabilities for each hidden state, considering the initial state probabilities and the emission probabilities for the first observed event.

2. Recursion: For each subsequent observed event, compute the maximum probability of reaching each hidden state, considering all possible previous states and their transition probabilities. Update the emission probabilities for the current observed event.

3. Termination: Identify the hidden state with the highest probability at the last time step.

4. Traceback: Starting from the identified state in the termination step, backtrack through the trellis to find the most probable path of hidden states that generated the observed sequence.

The Viterbi Algorithm is an efficient and widely used method for decoding the hidden states in a Hidden Markov Model, providing valuable insights into the underlying structure of the stochastic process."
132,"Passengers on a carnival ride move at constant speed in a horizontal circle of radius 5.0 m, making a complete circle in 4.0 s. What is their acceleration? (Unit: m/s^2))",12,integer,NONE,University Physics with Modern Physics | Example 3.12,panlu/uniform_circular_motion2.json,centripetal acceleration,Kinetics,Physics,"Centripetal acceleration is a type of acceleration experienced by an object moving in a circular path or orbit at a constant speed. It is always directed towards the center of the circle or the axis of rotation, and its magnitude depends on the object's speed and the radius of the circular path.

In kinetics, centripetal acceleration is essential for maintaining the circular motion of an object, as it continuously changes the object's direction while keeping its speed constant. The term ""centripetal"" comes from the Latin words ""centrum"" (center) and ""petere"" (to seek), which reflects the fact that this acceleration always points towards the center of the circle.

The formula for centripetal acceleration is:

a_c = v^2 / r

where:
- a_c is the centripetal acceleration
- v is the linear velocity of the object
- r is the radius of the circular path

Centripetal acceleration is measured in units of meters per second squared (m/s²). It is important to note that centripetal acceleration is not a force itself, but rather a result of the net force acting on the object to keep it in circular motion. This net force is often called centripetal force."
359,"A Chord based distributed hash table (DHT) with 25 address space is used in a peer- to-peer file sharing network. There are currently 5 active peers in the network with node ID N3, N8, N15, N19 and N30. Show all the target key (in ascending order, ignore the node's identifier itself) for N3.","[4, 5, 7, 11, 19]",list of integer,NONE,website | https://vinesmsuic.github.io/notes-networkingIP-qa3/#Q4-Multicast-Finger-Table,maxku/ipnetwork16-application-chord.json,chord network,Computer networking,EECS,"A Chord network is a distributed hash table (DHT) based computer networking protocol designed for peer-to-peer (P2P) systems. It was introduced in 2001 by Ion Stoica, Robert Morris, David Karger, M. Frans Kaashoek, and Hari Balakrishnan in their research paper ""Chord: A Scalable Peer-to-peer Lookup Service for Internet Applications."" The primary goal of Chord is to efficiently locate the node responsible for a particular data item in a large-scale, dynamic P2P network.

Chord network has the following key features:

1. Scalability: Chord can handle a large number of nodes and adapt to the frequent joining and leaving of nodes in the network.

2. Decentralization: There is no central authority or server in a Chord network. Each node in the network is responsible for a portion of the data and can act as both a client and a server.

3. Fault tolerance: Chord can handle node failures and network partitions, ensuring that the system remains operational even in the presence of failures.

4. Load balancing: Chord distributes data and workload evenly among the nodes in the network, preventing any single node from becoming a bottleneck.

The Chord protocol uses consistent hashing to assign keys to nodes in the network. Each node and data item is assigned a unique identifier (ID) using a hash function, typically a 160-bit identifier using the SHA-1 algorithm. The data items are stored in the node whose ID is equal to or immediately follows the data item's key in the identifier space.

Chord maintains a routing table called the ""finger table"" at each node, which contains information about a small subset of other nodes in the network. This table helps in efficiently routing queries to the appropriate node responsible for a given key. The finger table size is logarithmic in the number of nodes, which ensures that the lookup time is also logarithmic.

In summary, a Chord network is a distributed hash table-based P2P protocol that provides a scalable, decentralized, fault-tolerant, and load-balanced system for locating data items in large-scale networks."
