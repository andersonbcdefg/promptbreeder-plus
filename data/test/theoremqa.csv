,question,answer,Answer_type,Picture,source,id,theorem,subfield,field,theorem_def
223,"Let's assume that the 10-year annual return for the S&P 500 (market portfolio) is 10%, while the average annual return on Treasury bills (a good proxy for the risk-free rate) is 5%. Whats the market Treynor Ratio? Return the numeric value between 0 and 1.",0.05,float,NONE,website | https://www.wallstreetmojo.com/risk-adjusted-returns,xueguangma/treynor_ratio.json,treynor's ratio,Portfolio management,Finance,"Treynor's Ratio, also known as the Treynor Measure, is a performance metric used in portfolio management to evaluate the risk-adjusted returns of a portfolio or investment. It was developed by Jack L. Treynor, an American economist, and is used to determine how well an investment has performed in comparison to the risk it carries.

The Treynor Ratio is calculated by dividing the excess return of a portfolio (the return above the risk-free rate) by the portfolio's beta, which is a measure of the portfolio's sensitivity to market movements. The risk-free rate is typically represented by the return on a short-term government bond, such as a U.S. Treasury bill.

Treynor's Ratio formula:

Treynor's Ratio = (Portfolio Return - Risk-Free Rate) / Portfolio Beta

A higher Treynor Ratio indicates that the portfolio has generated better returns per unit of systematic risk (market risk) taken. In other words, a higher ratio means that the portfolio manager has been more successful in generating returns while managing exposure to market risk.

It is important to note that the Treynor Ratio only considers systematic risk, which is the risk inherent to the entire market, and does not account for unsystematic risk, which is the risk specific to individual investments. Therefore, the Treynor Ratio is most useful when comparing portfolios or investments with similar exposure to market risk.

In summary, Treynor's Ratio is a valuable tool in portfolio management for evaluating the risk-adjusted performance of investments. It helps investors and portfolio managers to assess how effectively a portfolio has generated returns while managing exposure to market risk."
744,"Let $X$ be uniformly distributed over $\{1, 2, \ldots, 256\}$. We ask random questions: Is $X\in S_1$? Is $X\in S_2$? ... until only one integer remains. All $2^256$ subsets of $\{1, 2, \ldots, 256\}$ are equally likely. How many deterministic questions are needed to determine $X$?",8,integer,NONE,textbook 5.45,xinyi/huffman_code_3.json,huffman coding,Information theory,EECS,"Huffman coding is a lossless data compression algorithm used in information theory, which is based on the principle of assigning shorter codes to more frequently occurring symbols and longer codes to less frequently occurring symbols. It was developed by David A. Huffman in 1952 as a method for constructing optimal prefix codes, which are codes where no code is a prefix of another code, ensuring that the original data can be uniquely reconstructed from the compressed data.

The main steps involved in Huffman coding are as follows:

1. Frequency analysis: Calculate the frequency (or probability) of each symbol in the input data. The symbols can be characters, bytes, or any other data unit.

2. Build a Huffman tree: Create a binary tree where each node represents a symbol and its frequency. The process starts by creating a leaf node for each symbol and placing them in a priority queue based on their frequencies. The nodes with the lowest frequencies are given the highest priority.

3. Merge nodes: Repeatedly remove the two nodes with the lowest frequencies from the priority queue, and create a new internal node with a frequency equal to the sum of the two removed nodes' frequencies. The removed nodes become the left and right children of the new internal node. Insert the new internal node back into the priority queue. Repeat this process until there is only one node left in the priority queue, which becomes the root of the Huffman tree.

4. Assign codes: Traverse the Huffman tree from the root to each leaf node, assigning a '0' for every left branch and a '1' for every right branch. The code for each symbol is the sequence of bits encountered along the path from the root to the corresponding leaf node.

5. Encode the data: Replace each symbol in the input data with its corresponding Huffman code. The resulting bitstream is the compressed data.

6. Decode the data: To reconstruct the original data from the compressed bitstream, start at the root of the Huffman tree and follow the branches according to the bits in the compressed data. When a leaf node is reached, output the corresponding symbol and return to the root to continue decoding the next bits.

Huffman coding is widely used in various applications, such as file compression (e.g., ZIP files), image compression (e.g., JPEG), and video compression (e.g., MPEG). It is an efficient and effective method for compressing data without losing any information."
101,Does f (x) = x2 + cx + 1 have a real root when c=0?,False,bool,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_1.json,quadratic formula,Calculus,Math,"The Quadratic Formula is a method used in algebra, not calculus, to find the solutions (roots) of a quadratic equation, which is an equation of the form ax^2 + bx + c = 0, where a, b, and c are constants. The Quadratic Formula is given by:

x = (-b ± √(b^2 - 4ac)) / (2a)

Here, x represents the solutions of the quadratic equation, and the ± symbol indicates that there are two possible solutions: one with the positive square root and one with the negative square root.

The Quadratic Formula is derived from the process of completing the square, which involves rewriting the quadratic equation in a form that allows us to easily find its solutions. The formula is useful because it provides a general solution for any quadratic equation, regardless of the specific values of a, b, and c."
602,What is the order of the group S_3 * Z_2?,12,integer,NONE,"text | Thomas W. Hungerford, Abstract Algebra An Introduction.",elainewan/math_abstact_algebra_7_3.json,order,Group theory,Math,"Order in group theory refers to two related concepts: the order of a group and the order of an element in a group.

1. Order of a group: The order of a group is the number of elements in the group. It is usually denoted by |G|, where G is the group. For example, if a group G has 5 elements, we write |G| = 5. The order of a group gives us information about the size and structure of the group.

2. Order of an element: The order of an element in a group is the smallest positive integer n such that the element raised to the power of n equals the identity element of the group. In other words, if a is an element of a group G and e is the identity element of G, then the order of a, denoted by o(a), is the smallest positive integer n such that a^n = e.

For example, consider the group of integers modulo 4 under addition, denoted by Z_4 = {0, 1, 2, 3}. The identity element in this group is 0. The order of the element 1 is 4 because 1+1+1+1 = 4 ≡ 0 (mod 4), and there is no smaller positive integer n for which 1+1+...+1 (n times) is congruent to 0 modulo 4. Similarly, the order of the element 2 is 2 because 2+2 = 4 ≡ 0 (mod 4).

In general, the order of an element in a group is an important concept because it helps us understand the structure and properties of the group. For example, in a finite group, the order of every element must divide the order of the group, which is a consequence of Lagrange's theorem."
559,"A bungee cord is 30.0 m long and, when stretched a distance x, it exerts a restoring force of magnitude kx. Your father-in-law (mass 95.0 kg) stands on a platform 45.0 m above the ground, and one end of the cord is tied securely to his ankle and the other end to the platform. You have promised him that when he steps off the platform he will fall a maximum distance of only 41.0 m before the cord stops him. You had several bungee cords to select from, and you tested them by stretching them out, tying one end to a tree, and pulling on the other end with a force of 380.0 N. When you do this, what distance (in m) will the bungee cord that you should select have stretched?",0.602,float,NONE,textbook 7.51,xinyi/potential_energy.json,kinetics theorem,Kinetics,Physics,"Kinetic theory, also known as the kinetic theory of gases, is a scientific theorem that explains the behavior of gases based on the motion of their constituent particles, such as atoms or molecules. The main idea behind the kinetic theory is that the macroscopic properties of a gas, such as pressure, temperature, and volume, can be explained by the microscopic motion and interactions of its particles.

The key assumptions of the kinetic theory are:

1. Gases are composed of a large number of small particles (atoms or molecules) that are in constant, random motion.
2. The particles are so small compared to the distances between them that their individual volumes can be considered negligible.
3. The particles are in constant, random motion, and they collide with each other and the walls of the container. These collisions are perfectly elastic, meaning that there is no loss of kinetic energy during the collisions.
4. There are no attractive or repulsive forces between the particles, except during the brief moments of collision.
5. The average kinetic energy of the particles is directly proportional to the temperature of the gas.

Based on these assumptions, the kinetic theory can be used to derive various gas laws, such as Boyle's law, Charles's law, and the ideal gas law, which describe the relationships between pressure, volume, and temperature in a gas. The kinetic theory also provides a basis for understanding the diffusion of gases, the transport of heat in gases, and the behavior of gases in different thermodynamic processes."
502,"In the process of searching circles in an image, object O is detected. The contour of the object O is represented with the Fourier Descriptors (0,113,0,0,1,0,0,1). Given that the Fourier Descriptors of a circle are (0,40,0,0,0,0,0,0). Is the object O a circle-like polygon in the image? Bear in mind that there is some high frequency noise in the image. You should take this into account when you make your judgment.",True,bool,NONE,self | https://vinesmsuic.github.io/notes-dimagep-QA2/,maxku/cv-imageprocessing8-fourier3.json,image frequency analysis,Signal processing,EECS,"Image Frequency Analysis is a technique used in signal processing to identify and analyze the presence of unwanted signals, known as image frequencies, that may interfere with the desired signal in a communication system. This analysis is particularly important in radio frequency (RF) systems, where the process of frequency conversion (up-conversion or down-conversion) can introduce these unwanted signals.

In a typical RF system, a mixer is used to combine the input signal with a local oscillator (LO) signal to produce an output signal at a different frequency. Ideally, the output signal should only contain the desired frequency components. However, due to the non-linear behavior of mixers and other imperfections in the system, additional frequency components, called image frequencies, can be generated. These image frequencies can cause interference and degrade the performance of the communication system.

Image Frequency Analysis involves the following steps:

1. Identifying the image frequencies: The first step is to determine the possible image frequencies that can be generated in the system. This can be done by analyzing the frequency spectrum of the input signal, the LO signal, and the mixer output.

2. Filtering: To minimize the impact of image frequencies, filters are often used in the system to attenuate or eliminate these unwanted signals. The design of these filters depends on the specific requirements of the system, such as the desired signal bandwidth and the level of image rejection needed.

3. Measurement and evaluation: Once the filters are in place, the performance of the system can be evaluated by measuring the level of the image frequencies and comparing them to the desired signal. This can be done using various signal analysis tools, such as spectrum analyzers and vector network analyzers.

4. Optimization: Based on the results of the measurement and evaluation, the system can be further optimized to improve its performance. This may involve adjusting the filter parameters, changing the LO frequency, or modifying other system components.

In summary, Image Frequency Analysis is a crucial technique in signal processing that helps identify and mitigate the impact of unwanted image frequencies in communication systems, ensuring optimal performance and signal quality."
724,"Find the volume of a solid bounded by the elliptical paraboloid $z=2x^2 + y^2 + 1$, the plane x+y=1, and the coordinate planes.",0.75,float,NONE,textbook | Demidovich,wenhuchen/volume.json,volume,Geometry,Math,"In geometry, volume refers to the measure of the three-dimensional space occupied by an object or a closed shape. It is typically expressed in cubic units, such as cubic centimeters (cm³), cubic meters (m³), or cubic inches (in³). Volume is an important concept in various fields, including science, engineering, and mathematics.

Calculating the volume of an object depends on its shape. For example, the volume of a rectangular prism can be calculated by multiplying its length, width, and height (V = lwh), while the volume of a cylinder can be calculated using the formula V = πr²h, where r is the radius of the base and h is the height.

In general, the volume of an object can be found by integrating the area of its cross-sections along a particular axis or by using specific formulas for different shapes."
196,Using n=8 approximate the value of $\int_{0}^4 cos(1 + \sqrt{x}) dx$ using the Simpson's rule.,-2.47160136,float,NONE,website | https://tutorial.math.lamar.edu/Solutions/CalcII/ApproximatingDefIntegrals,wenhuchen/Simpson's_rule2.json,simpson's rule,Calculus,Math,"Simpson's Rule is a numerical integration technique used in calculus to approximate the definite integral of a function. It is named after the British mathematician Thomas Simpson, who popularized it in the 18th century. The rule is based on the idea of approximating the area under the curve of a function by using parabolic (quadratic) segments instead of linear segments, as in the case of the trapezoidal rule.

Simpson's Rule works by dividing the interval of integration [a, b] into an even number of equally spaced subintervals (2n), and then fitting a parabola (a quadratic polynomial) through the points of the function at the endpoints and the midpoint of each subinterval. The area under each parabolic segment is then calculated and summed up to approximate the total area under the curve, which represents the definite integral of the function.

The formula for Simpson's Rule is given by:

∫(a to b) f(x) dx ≈ (Δx/3) [f(x0) + 4f(x1) + 2f(x2) + 4f(x3) + ... + 2f(x_{2n-2}) + 4f(x_{2n-1}) + f(x_{2n})]

where:
- a and b are the limits of integration
- f(x) is the function to be integrated
- Δx = (b - a) / (2n) is the width of each subinterval
- n is the number of subintervals (must be an even number)
- x_i = a + iΔx for i = 0, 1, 2, ..., 2n are the endpoints and midpoints of the subintervals

Simpson's Rule provides a more accurate approximation of the definite integral compared to other methods like the trapezoidal rule, especially for functions that have continuous second derivatives. However, it may not be as accurate for functions with discontinuities or rapidly changing behavior."
674,"Consider a source $X$ uniformly distributed on the set $\{1, 2, \dots, m\}$. The rate distortion function for this source with Hamming distortion is $R(D) = \log{m}-H(D)-D\log{(m-1)}$ for $0\leq D\leq 1-\frac{1}{m}$, and $R(D) = 0$ otherwise. True or False?",True,bool,NONE,textbook 10.5,xinyi/rate_distortion_function_1.json,rate-distortion theory,Information theory,EECS,"Rate-distortion theory is a fundamental concept in information theory that deals with the trade-off between the compression rate of a source and the distortion or loss of information that occurs during the compression process. It was first introduced by Claude Shannon in 1948 and has since become an essential tool in the analysis and design of communication systems, particularly in the field of data compression and signal processing.

In simple terms, rate-distortion theory aims to find the optimal balance between the amount of data that can be compressed (rate) and the quality of the reconstructed data after decompression (distortion). The main idea is that as the compression rate increases, the distortion in the reconstructed data also increases, and vice versa. The goal is to minimize the distortion while maintaining an acceptable compression rate.

Rate-distortion theory is based on two main components:

1. Rate: The rate refers to the number of bits per symbol required to represent the compressed data. A lower rate means higher compression, but it may also result in more distortion in the reconstructed data.

2. Distortion: Distortion is a measure of the difference between the original data and the reconstructed data after compression and decompression. It quantifies the loss of information or quality that occurs during the compression process. Distortion can be measured in various ways, such as mean squared error, signal-to-noise ratio, or perceptual quality metrics.

The rate-distortion function (R(D)) is a mathematical representation of the relationship between the rate and distortion. It describes the minimum achievable rate for a given level of distortion or the minimum distortion that can be achieved for a given rate. The rate-distortion function is typically derived using probabilistic models of the source data and the distortion measure.

In practical applications, rate-distortion theory is used to design efficient compression algorithms, such as image and video codecs, audio codecs, and lossy data compression techniques. By understanding the trade-offs between rate and distortion, engineers can develop algorithms that provide the best possible compression performance while maintaining an acceptable level of quality in the reconstructed data."
520,"A one-hour color video in YUV format has a frame resolution of 1920x1080 with a 4:2:2 color sub-sampling format, 8 bits for each component, and a frame rate of 30 frames/s. Determine the storage requirement for the video in Gbytes (3 sig. fig.).",417,integer,NONE,self,maxku/cv-videoprocessing1-digital-video.json,digital storage,Signal processing,EECS,"Digital storage in signal processing refers to the process of converting analog signals into digital data and storing that data in a digital format for further processing, analysis, or transmission. This is an essential aspect of modern signal processing, as it allows for more accurate and efficient handling of information compared to analog storage methods.

In digital storage, the continuous analog signal is first sampled at regular intervals, and each sample is assigned a discrete value based on its amplitude. This process is called analog-to-digital conversion (ADC). The resulting digital data is then stored in a digital storage medium, such as a hard drive, solid-state drive, or memory chip.

Digital storage offers several advantages over analog storage, including:

1. Improved accuracy: Digital data is less susceptible to noise and distortion, which can degrade the quality of analog signals over time or during transmission.

2. Easy manipulation: Digital data can be easily processed, analyzed, and manipulated using digital signal processing techniques and algorithms.

3. Efficient storage and transmission: Digital data can be compressed and stored more efficiently than analog data, reducing the required storage space and bandwidth for transmission.

4. Error detection and correction: Digital storage systems can incorporate error detection and correction techniques to ensure data integrity and reliability.

5. Interoperability: Digital data can be easily shared and exchanged between different devices and systems, facilitating collaboration and integration.

Overall, digital storage plays a crucial role in modern signal processing, enabling the efficient and accurate handling of information in various applications, such as telecommunications, audio and video processing, medical imaging, and control systems."
60,Is the set of 3 * 3 matrices in reduced row-echelon form a subspace of R^{3 * 3}?,False,bool,NONE,"text | Otto Bretscher, Linear Algebra with Applications.",elainewan/math_algebra_4.json,linear subspaces,Algebra,Math,"In algebra, a linear subspace, also known as a vector subspace, is a subset of a vector space that is closed under the operations of vector addition and scalar multiplication. In simpler terms, it is a smaller space within a larger vector space that still follows the rules of a vector space.

A vector space is a set of vectors along with two operations, vector addition and scalar multiplication, that satisfy certain properties. These properties include commutativity, associativity, existence of an additive identity (zero vector), existence of additive inverses, distributivity of scalar multiplication over vector addition, and compatibility of scalar multiplication with scalar multiplication.

A linear subspace is a subset of a vector space that also satisfies these properties. To be a linear subspace, a subset must meet the following conditions:

1. The zero vector of the larger vector space is also in the subspace.
2. If you add any two vectors in the subspace, their sum is also in the subspace.
3. If you multiply any vector in the subspace by a scalar, the resulting vector is also in the subspace.

If a subset of a vector space meets these conditions, it is considered a linear subspace. Linear subspaces are important in various areas of mathematics, including linear algebra, functional analysis, and differential equations. They provide a way to study smaller, more manageable pieces of a larger vector space and can help simplify complex problems."
638,"In the process of searching circles in an image, object O is detected. The contour of the object O is represented with the Fourier Descriptors (35,129,0,1,0,0,-1,0). Given that the Fourier Descriptors of a circle are (0,40,0,0,0,0,0,0). Is the object O a circle-like polygon in the image? Bear in mind that there is some high frequency noise in the image. You should take this into account when you make your judgment.",True,bool,NONE,self | https://vinesmsuic.github.io/notes-dimagep-QA2/,maxku/cv-imageprocessing8-fourier4.json,image frequency analysis,Signal processing,EECS,"Image Frequency Analysis is a technique used in signal processing to identify and analyze the presence of unwanted signals, known as image frequencies, that may interfere with the desired signal in a communication system. This analysis is particularly important in radio frequency (RF) systems, where the process of frequency conversion (up-conversion or down-conversion) can introduce these unwanted signals.

In a typical RF system, a mixer is used to combine the input signal with a local oscillator (LO) signal to produce an output signal at a different frequency. Ideally, the output signal should only contain the desired frequency components. However, due to the non-linear behavior of mixers and other imperfections in the system, additional frequency components, called image frequencies, can be generated. These image frequencies can cause interference and degrade the performance of the communication system.

Image Frequency Analysis involves the following steps:

1. Identifying the image frequencies: The first step is to determine the possible image frequencies that can be generated in the system. This can be done by analyzing the frequency spectrum of the input signal, the LO signal, and the mixer output.

2. Filtering: To minimize the impact of image frequencies, filters are often used in the system to attenuate or eliminate these unwanted signals. The design of these filters depends on the specific requirements of the system, such as the desired signal bandwidth and the level of image rejection needed.

3. Measurement and evaluation: Once the filters are in place, the performance of the system can be evaluated by measuring the level of the image frequencies and comparing them to the desired signal. This can be done using various signal analysis tools, such as spectrum analyzers and vector network analyzers.

4. Optimization: Based on the results of the measurement and evaluation, the system can be further optimized to improve its performance. This may involve adjusting the filter parameters, changing the LO frequency, or modifying other system components.

In summary, Image Frequency Analysis is a crucial technique in signal processing that helps identify and mitigate the impact of unwanted image frequencies in communication systems, ensuring optimal performance and signal quality."
58,"In Image processing, opening is a process in which first dilation operation is performed and then erosion operation is performed. Is it true?",False,bool,NONE,self,maxku/cv-imageprocessing1-morphology.json,image morphology,Signal processing,EECS,"Image Morphology is a technique in signal processing and computer vision that deals with the analysis and processing of geometrical structures within images. It is a subfield of mathematical morphology, which is a theory and technique for the analysis and processing of geometrical structures, based on set theory, lattice theory, topology, and random functions.

In the context of image processing, morphology focuses on the shape and structure of objects within an image, such as boundaries, skeletons, and convex hulls. The primary goal of image morphology is to extract, modify, or simplify the structure of objects in an image while preserving their essential features.

Morphological operations are typically performed on binary images (black and white) or grayscale images. Some common morphological operations include:

1. Erosion: This operation erodes the boundaries of the objects in the image, effectively shrinking them. It is useful for removing noise and small irregularities.

2. Dilation: This operation expands the boundaries of the objects in the image, effectively growing them. It is useful for filling gaps and connecting disjointed parts of an object.

3. Opening: This operation is a combination of erosion followed by dilation. It is useful for removing noise while preserving the shape of the objects in the image.

4. Closing: This operation is a combination of dilation followed by erosion. It is useful for filling gaps and connecting disjointed parts of an object while preserving the shape of the objects in the image.

5. Skeletonization: This operation reduces the objects in the image to their skeletal structure, which is a thin, connected representation of the object's shape.

6. Morphological gradient: This operation computes the difference between the dilation and erosion of an image, highlighting the boundaries of the objects in the image.

These operations can be combined and applied iteratively to achieve various image processing tasks, such as noise reduction, edge detection, object segmentation, and shape analysis. Image morphology is widely used in various applications, including computer vision, medical imaging, remote sensing, and pattern recognition."
412,"A train pulls out of the station at constant velocity. The received signal energy thus falls off with time as $1/i^2$. The total received signal at time $i$ is $Y_i = \frac{1}{i}X_i + Z_i$ where $Z_1, Z_2, \ldots$ are i.i.d. drawn from $N(0,1)$. The transmitter constraint for block length $n$ is $\frac{1}{n}\sum_{i=1}^n x_i^2(w) \leq 2  $ for $w \in \{1,2,\ldots, 2^{nR}\}$. Use Fano's inequality to find the capacity for this channel.",0.0,float,NONE,textbook 9.12,xinyi/fano_inequality.json,fano's inequality,Information theory,EECS,"Fano's inequality is a fundamental result in information theory that provides an upper bound on the probability of error in decoding a message transmitted over a noisy channel. It is named after the Italian-American engineer and information theorist Robert Fano, who first derived the inequality in 1961.

Fano's inequality relates the probability of error in decoding, the conditional entropy of the transmitted message given the received message, and the entropy of the transmitted message. It is particularly useful in establishing the limits of reliable communication over a noisy channel and in proving the converse of the channel coding theorem.

The inequality can be stated as follows:

P_e ≥ (H(X) - H(X|Y)) / (H(X) - 1)

where:
- P_e is the probability of error in decoding the message,
- H(X) is the entropy of the transmitted message (a measure of its uncertainty),
- H(X|Y) is the conditional entropy of the transmitted message given the received message (a measure of the remaining uncertainty about the transmitted message after observing the received message), and
- H(X) - 1 is the maximum possible reduction in entropy due to the observation of the received message.

Fano's inequality shows that the probability of error in decoding is lower-bounded by the ratio of the reduction in uncertainty about the transmitted message due to the observation of the received message to the maximum possible reduction in uncertainty. In other words, the more information the received message provides about the transmitted message, the lower the probability of error in decoding."
460,"What is the Fisher information for the distribution family $f_\theta(x)=\theta e^{-\theta x}$, $x \geq 0$? (a) $\theta$. (b) $\theta^2$. (c) $\theta^{-1}$. (d) $\theta^{-2}$. Which option is correct?",(d),option,NONE,textbook 11.8(b),xinyi/fisher_information_3.json,fisher information,Statistics,Math,"Fisher information is a statistical concept used to measure the amount of information that a set of observed data carries about an unknown parameter of the underlying probability distribution. It is named after the British statistician Ronald A. Fisher, who introduced the concept in the context of maximum likelihood estimation.

Fisher information is particularly useful in the field of parameter estimation, as it helps to quantify the precision with which an unknown parameter can be estimated from the given data. It is closely related to the Cramér-Rao lower bound, which states that the variance of any unbiased estimator of the parameter cannot be smaller than the inverse of the Fisher information.

Mathematically, Fisher information is defined as the expected value of the second derivative (with respect to the parameter) of the log-likelihood function, or equivalently, as the expected value of the squared first derivative of the log-likelihood function. For a probability distribution with parameter θ and likelihood function L(θ), the Fisher information I(θ) can be expressed as:

I(θ) = E[(-d²/dθ²) log L(θ)] = E[(d/dθ log L(θ))²]

In simple terms, Fisher information quantifies how sensitive the likelihood function is to changes in the parameter. A higher Fisher information indicates that the data provides more information about the parameter, leading to more precise estimates. Conversely, a lower Fisher information suggests that the data is less informative, resulting in less precise estimates.

Fisher information plays a crucial role in various statistical methods, including hypothesis testing, confidence intervals, and Bayesian inference. It is an essential tool for understanding the relationship between data and the underlying parameters of a probability distribution."
374,"Suppose we are given the following information. Use this information to calculate abnormal return. Rf: 4%
Rm: 12%
Beta of the Portfolio: 1.8
Beginning Value of Portfolio: $50,000
Ending Value of Portfolio: $60,000
What is the abnormal return?",0.016,float,NONE,website | https://www.wallstreetmojo.com/abnormal-return,xueguangma/abnormal_return.json,abnormal return,Portfolio management,Finance,"Abnormal Return, in the context of portfolio management, refers to the difference between the actual return of a security or portfolio and its expected return, given its risk profile and market performance. In other words, it is the excess return generated by a security or portfolio over and above what would be expected based on its risk level and the overall market conditions.

Abnormal returns can be positive or negative, indicating that the security or portfolio has either outperformed or underperformed its expected return. Positive abnormal returns suggest that the portfolio manager has made successful investment decisions, while negative abnormal returns indicate that the manager's decisions have not generated the desired results.

Abnormal returns are often used to evaluate the performance of portfolio managers and investment strategies, as they provide insight into whether the manager has added value through their investment decisions. A consistently positive abnormal return may indicate that the manager has skill in selecting investments, while a consistently negative abnormal return may suggest that the manager's strategy is not effective.

It is important to note that abnormal returns should be considered in the context of the overall market conditions and the risk profile of the portfolio. A high abnormal return may not necessarily indicate a successful investment strategy if it is accompanied by a high level of risk. Similarly, a low abnormal return may not necessarily indicate poor performance if the portfolio is designed to minimize risk."
119,"For the two linear equations $2 * x + 3 * y + z = 8$ and $4 * x + 4 * y + 4z = 12$ and $x + y + 8z = 10$ with variables x, y and z. Use cramer's rule to solve these three variables.","[-1, 3, 1]",list of integer,NONE,self,wenhuchen/cramer's_rule2.json,cramer's rule,Algebra,Math,"Cramer's Rule is a mathematical theorem in linear algebra that provides an explicit formula for the solution of a system of linear equations with as many equations as unknowns, if the system has a unique solution. It is named after Swiss mathematician Gabriel Cramer, who introduced the rule in 1750.

Cramer's Rule uses determinants to find the solution of the system. The determinant is a scalar value that can be computed from a square matrix and has various applications in linear algebra, including finding the inverse of a matrix and calculating the area or volume of geometric shapes.

Here's how Cramer's Rule works for a system of linear equations:

1. Write down the coefficient matrix (A) of the system, which is formed by the coefficients of the unknowns in the equations.

2. Calculate the determinant of the coefficient matrix (|A|). If the determinant is zero, Cramer's Rule cannot be applied, and the system has either no solution or infinitely many solutions.

3. For each unknown variable (x_i), replace the i-th column of the coefficient matrix with the constant terms of the equations, forming a new matrix (A_i).

4. Calculate the determinant of each new matrix (|A_i|).

5. Divide the determinant of each new matrix (|A_i|) by the determinant of the coefficient matrix (|A|) to find the value of the corresponding unknown variable (x_i).

For example, consider a system of two linear equations with two unknowns, x and y:

a1 * x + b1 * y = c1
a2 * x + b2 * y = c2

To solve this system using Cramer's Rule:

1. Form the coefficient matrix A = | a1  b1 |
                                      | a2  b2 |

2. Calculate |A| = a1 * b2 - a2 * b1. If |A| = 0, the system has no unique solution.

3. Replace the first column of A with the constants (c1, c2) to form matrix A_x = | c1  b1 |
                                                                                               | c2  b2 |

4. Replace the second column of A with the constants (c1, c2) to form matrix A_y = | a1  c1 |
                                                                                               | a2  c2 |

5. Calculate |A_x| and |A_y|.

6. Find the values of x and y by dividing the determinants: x = |A_x| / |A| and y = |A_y| / |A|.

Cramer's Rule can be extended to systems with more than two equations and unknowns, following the same procedure of replacing columns in the coefficient matrix and calculating determinants. However, for larger systems, Cramer's Rule can be computationally expensive and less efficient than other methods, such as Gaussian elimination or matrix inversion."
629,The atomic mass of the 4He atom is 4.002603 u. Find the binding energy of the 4He nucleus in MeV.,28.3,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Equation 2.81,tonyxia/relativity3.json,relativity,Relativity,Physics,"Relativity is a scientific theory that fundamentally changed our understanding of space, time, and gravity. It was first introduced by the renowned physicist Albert Einstein in the early 20th century and consists of two parts: the Special Theory of Relativity and the General Theory of Relativity.

1. Special Theory of Relativity (1905): This theory deals with objects moving at constant speeds, particularly those moving close to the speed of light. It is based on two main principles:

   a. The Principle of Relativity: The laws of physics are the same for all observers in uniform motion relative to one another.
   b. The Constancy of the Speed of Light: The speed of light in a vacuum is the same for all observers, regardless of their motion or the motion of the light source.

The Special Theory of Relativity led to several counterintuitive conclusions, such as time dilation (moving clocks run slower), length contraction (moving objects appear shorter), and the equivalence of mass and energy (E=mc^2), which states that mass can be converted into energy and vice versa.

2. General Theory of Relativity (1915): This theory is an extension of the Special Theory of Relativity and deals with gravity. It describes gravity not as a force between masses, as proposed by Sir Isaac Newton, but as a curvature of spacetime caused by the presence of mass. In other words, massive objects like planets and stars warp the fabric of spacetime, causing other objects to move along curved paths.

The General Theory of Relativity has been confirmed through various experiments and observations, such as the bending of light around massive objects (gravitational lensing), the shift in the orbit of Mercury, and the detection of gravitational waves.

In summary, Relativity is a groundbreaking theory that has reshaped our understanding of the universe, providing a more accurate and comprehensive description of the fundamental concepts of space, time, and gravity."
54,What is (sin(2x) / x)^(1+x) when x is approaching 0?,2.0,float,NONE,textbook | demidovich,wenhuchen/L'Hôpital_rule1.json,l'hôpital's rule,Calculus,Math,"L'Hôpital's rule is a mathematical technique used in calculus to evaluate limits of indeterminate forms, specifically when the limit involves a fraction where both the numerator and the denominator approach zero or infinity. It is named after the French mathematician Guillaume de l'Hôpital, who published the rule in his book ""Analyse des Infiniment Petits"" in 1696.

The rule states that if the limit of a function f(x)/g(x) as x approaches a certain value (say, x=a) results in an indeterminate form of the type 0/0 or ∞/∞, then the limit of the function can be found by taking the limit of the derivative of the numerator divided by the derivative of the denominator, i.e.,

lim (x→a) [f(x) / g(x)] = lim (x→a) [f'(x) / g'(x)],

provided that the limit on the right-hand side exists or is a finite number.

L'Hôpital's rule can be applied repeatedly if the resulting limit after applying the rule is still an indeterminate form. It is important to note that L'Hôpital's rule can only be applied when the given conditions are met, and it is not a universal method for solving all types of limits.

In summary, L'Hôpital's rule is a powerful technique in calculus for evaluating limits of indeterminate forms involving fractions where both the numerator and the denominator approach zero or infinity. It involves taking the derivatives of the numerator and the denominator and then finding the limit of the resulting fraction."
743,Let $C$ be a variable length code that satisfies the Kraft inequality with equality but does not satisfy the prefix condition. Then $C$ has finite decoding delay. True or False?,False,bool,NONE,textbook 5.23(b),xinyi/kraft_inequality.json,kraft inequality,Information theory,EECS,"Kraft inequality is a fundamental concept in information theory, specifically in the area of prefix coding. It is named after Leon G. Kraft, who first introduced it in 1949. The inequality provides a necessary and sufficient condition for the existence of uniquely decodable prefix codes, which are widely used in data compression algorithms, such as Huffman coding.

In simple terms, Kraft inequality states that for a prefix code with a set of codewords, the sum of the probabilities of each codeword, raised to the power of the length of the codeword, must be less than or equal to 1. Mathematically, it can be expressed as:

Σ (2^(-li)) ≤ 1

Here, 'li' represents the length of the ith codeword, and the summation is taken over all the codewords in the code.

The importance of Kraft inequality lies in its ability to ensure the existence of a prefix code with given codeword lengths. If a set of codeword lengths satisfies the Kraft inequality, then there exists a prefix code with those lengths. Conversely, if a prefix code exists with a certain set of codeword lengths, then those lengths must satisfy the Kraft inequality.

In summary, Kraft inequality is a crucial concept in information theory that helps in designing efficient and uniquely decodable prefix codes for data compression and error correction. It provides a necessary and sufficient condition for the existence of such codes, ensuring that the code can be constructed and decoded unambiguously."
667,Find the fraction of 7.7-MeV alpha particles that is deflected at an angle of 90° or more from a gold foil of 10^-6 m thickness.,4e-05,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Example 4.2,tonyxia/atom1.json,atomic theorem,Atomic physics,Physics,"Atomic Theorem, also known as the Bohr's Atomic Theory, is a fundamental concept in atomic physics that was proposed by Danish physicist Niels Bohr in 1913. The theory describes the behavior of electrons in atoms and provides a model for understanding the structure of atoms. Bohr's Atomic Theory is based on the principles of quantum mechanics and is an extension of Rutherford's nuclear model of the atom.

The main postulates of Bohr's Atomic Theory are:

1. Electrons orbit the nucleus in fixed energy levels or orbits, called ""shells."" Each shell corresponds to a specific energy level, and the energy of an electron in a particular shell is quantized, meaning it can only take certain discrete values.

2. Electrons can move between energy levels by absorbing or emitting energy in the form of photons (light particles). When an electron absorbs a photon, it moves to a higher energy level (excited state), and when it emits a photon, it moves to a lower energy level (ground state).

3. The energy of a photon emitted or absorbed by an electron is equal to the difference in energy between the initial and final energy levels of the electron. This is represented by the formula: E = hf, where E is the energy of the photon, h is Planck's constant, and f is the frequency of the photon.

4. The angular momentum of an electron in a particular orbit is quantized and is an integer multiple of Planck's constant divided by 2π (h/2π). This means that only certain orbits with specific radii and energies are allowed for electrons in an atom.

5. The electron's position and momentum cannot be precisely determined simultaneously, as per the Heisenberg Uncertainty Principle. This means that the electron's exact location within an orbit cannot be pinpointed, but its probability distribution can be described.

Bohr's Atomic Theory successfully explained the hydrogen atom's energy levels and the hydrogen spectrum's line series. However, it had limitations in explaining the spectra of more complex atoms and the chemical behavior of elements. Later developments in quantum mechanics, such as the Schrödinger equation and the concept of electron orbitals, provided a more comprehensive understanding of atomic structure and behavior."
565,How many trees are there on n (n > 1) labeled vertices with no vertices of degree 1 or 2?,0,integer,NONE,self,jianyu_xu/Cayley_5.json,cayley's formula,Graph theory,EECS,"Cayley's formula is a result in graph theory that provides the number of distinct labeled trees that can be formed using a specific number of vertices. It is named after the British mathematician Arthur Cayley, who first stated the formula in 1889.

The formula states that for a given number of vertices n, there are n^(n-2) distinct labeled trees that can be formed. In other words, if you have n vertices, you can create n^(n-2) different trees where each vertex is uniquely labeled.

Cayley's formula can be derived using several methods, including the Matrix Tree Theorem and Prüfer sequences. The formula is particularly useful in combinatorics and graph theory, as it helps to enumerate the number of possible tree structures for a given set of vertices. This has applications in various fields, such as computer science, biology, and network analysis."
344,"Calculate the Hamming pairwise distances and determine the minimum Hamming distance among the following codewords: 000000,010101,101010,110110",3,integer,NONE,website | https://vinesmsuic.github.io/notes-networking2-qa/#Problems-3,maxku/ipnetwork14-hammingdist.json,hamming distance,Computer networking,EECS,"Hamming distance is a concept in computer networking and information theory that measures the difference between two strings of equal length by counting the number of positions at which the corresponding symbols (bits) are different. It is named after Richard Hamming, who introduced the concept in the context of error-detecting and error-correcting codes.

In computer networking, Hamming distance is particularly useful for detecting and correcting errors in data transmission. When data is transmitted over a network, it can be corrupted due to noise, interference, or other factors. By using error-correcting codes with a certain minimum Hamming distance, it is possible to detect and correct errors that occur during transmission.

For example, consider two binary strings of equal length:

String 1: 11010101
String 2: 10011011

The Hamming distance between these two strings is 4, as there are four positions at which the bits are different (positions 2, 4, 6, and 7).

In the context of error-correcting codes, a code with a minimum Hamming distance of d can detect up to (d-1) bit errors and correct up to floor((d-1)/2) bit errors. So, a code with a minimum Hamming distance of 4 can detect up to 3 bit errors and correct up to 1 bit error."
323,"The difference equation of a causal system is $y[n]+0.5 y[n-1]=x[n]-x[n-2]$, where $y[n]$ is its output and $x[n]$ is its input. Is the system a FIR filter?",False,bool,NONE,self,maxku/signalprocessing6-Ztransform.json,z-transform,Signal processing,EECS,"The Z-transform is a mathematical technique used in signal processing and control theory to analyze and represent discrete-time signals and systems. It is a powerful tool for analyzing the behavior of discrete-time systems, such as digital filters, and is widely used in digital signal processing, communications, and control system design.

The Z-transform is a generalization of the discrete-time Fourier transform (DTFT), which is used to analyze continuous-time signals. The Z-transform maps a discrete-time signal, typically represented as a sequence of samples, into a complex-valued function of a complex variable, Z. This transformation allows for the manipulation and analysis of the signal in the Z-domain, which can provide insights into the signal's properties, such as stability, causality, and frequency response.

The Z-transform of a discrete-time signal x[n] is defined as:

X(z) = Σ (x[n] * z^(-n))

where X(z) is the Z-transform of the signal x[n], z is a complex variable, and the summation is taken over all integer values of n.

The Z-transform has several important properties, such as linearity, time-shifting, and convolution, which make it a useful tool for analyzing and manipulating discrete-time signals and systems. Additionally, the inverse Z-transform can be used to recover the original discrete-time signal from its Z-domain representation.

In summary, the Z-transform is a powerful mathematical technique used in signal processing and control theory to analyze and represent discrete-time signals and systems. It provides a convenient framework for studying the properties of these systems and designing algorithms for processing and manipulating discrete-time signals."
31,"How many ways are there to color the vertices of a cube with two colors, up to rotation?",23,integer,NONE,self,jianyu_xu/Burnside_1.json,burnside's lemma,Combinatorics,Math,"Burnside's Lemma, also known as the Cauchy-Frobenius Lemma or the Orbit-Counting Theorem, is a fundamental result in combinatorics that deals with counting the number of distinct elements in a set under the action of a group. It is particularly useful in counting problems involving symmetries and permutations.

The lemma is named after the British mathematician William Burnside, who contributed significantly to the development of group theory.

Statement of Burnside's Lemma:

Let G be a finite group that acts on a finite set X. Then the number of distinct orbits of X under the action of G is given by:

(1/|G|) * Σ |Fix(g)|

where |G| is the order of the group (i.e., the number of elements in G), the sum is taken over all elements g in G, and |Fix(g)| is the number of elements in X that are fixed by the action of g (i.e., the number of elements x in X such that g(x) = x).

In simpler terms, Burnside's Lemma states that the number of distinct orbits (or equivalence classes) in a set under the action of a group can be found by averaging the number of fixed points of each group element.

Burnside's Lemma is often used in combinatorial problems where we need to count the number of distinct configurations of an object, taking into account its symmetries. By applying the lemma, we can avoid overcounting configurations that are equivalent under a given symmetry operation."
555,"If u is the real part of a function, and v is the imaginary part, then the Cauchy-Riemann equations for u and v take the following form in polar coordinates: r\frac{\partial u}{\partial r} = \frac{\partial v}{\partial \theta} and r\frac{\partial v}{\partial r} = -\frac{\partial u}{\partial \theta}. Is this argument True or False?",True,bool,NONE,website | https://haroldpboas.gitlab.io/courses/407-2017c/solution2.pdf,wenhuchen/cauchy_riemann4.json,cauchy riemann theorem,Complex analysis,Math,"The Cauchy-Riemann Theorem is a fundamental result in complex analysis that provides a set of necessary and sufficient conditions for a function to be holomorphic (i.e., complex-differentiable) in a domain. Holomorphic functions are complex functions that are differentiable at every point in their domain, and they play a central role in complex analysis.

The theorem is named after Augustin-Louis Cauchy and Bernhard Riemann, who independently developed the conditions now known as the Cauchy-Riemann equations. These equations relate the partial derivatives of the real and imaginary parts of a complex function.

Let f(z) be a complex function defined in a domain D, where z = x + iy is a complex variable with x and y being real numbers, and i is the imaginary unit (i.e., i^2 = -1). We can write f(z) as:

f(z) = u(x, y) + iv(x, y),

where u(x, y) and v(x, y) are real-valued functions representing the real and imaginary parts of f(z), respectively.

The Cauchy-Riemann equations are given by:

1. ∂u/∂x = ∂v/∂y
2. ∂u/∂y = -∂v/∂x

These equations state that the partial derivatives of u and v with respect to x and y must satisfy the above relationships for f(z) to be holomorphic in D.

The Cauchy-Riemann Theorem can be stated as follows:

A function f(z) = u(x, y) + iv(x, y) is holomorphic in a domain D if and only if the following conditions are satisfied:

1. The partial derivatives ∂u/∂x, ∂u/∂y, ∂v/∂x, and ∂v/∂y exist and are continuous in D.
2. The Cauchy-Riemann equations hold in D.

In other words, if a complex function satisfies the Cauchy-Riemann equations and its partial derivatives are continuous, then the function is holomorphic in its domain. Conversely, if a function is holomorphic, it must satisfy the Cauchy-Riemann equations."
222,"Let f be a bounded entire function, z_1,z_2 be two points in the ball B(0,r). What is the value of the integral $\int_{|z|=r} f(z)/(z-z_1)(z-z_2) dz$?",0.0,float,NONE,complex analysis: exerise 3.5.1,mingyin/cauchy-integral-theorem1.json,cauchy's integral theorem,Complex analysis,Math,"Cauchy's Integral Theorem is a fundamental result in complex analysis that relates the values of a holomorphic (complex-differentiable) function inside a closed contour to the values of the function on the contour itself. It states that if a function is holomorphic within and on a simple closed contour, then the integral of the function around the contour is zero.

Mathematically, let f(z) be a complex-valued function that is holomorphic in a simply connected domain D, which includes the contour C and its interior. Then, Cauchy's Integral Theorem states that:

∮_C f(z) dz = 0

Here, ∮_C denotes the contour integral taken around the closed contour C in the positive (counterclockwise) direction.

The theorem has several important consequences, including the fact that the value of a holomorphic function inside a closed contour can be recovered from its values on the contour itself (Cauchy's Integral Formula). It also implies that holomorphic functions have antiderivatives, and their integrals are path-independent in simply connected domains.

Cauchy's Integral Theorem is a powerful tool in complex analysis, as it allows us to evaluate contour integrals and study the properties of holomorphic functions in a more profound way."
432,"A single firm monopolizes the entire market for widgets and can produce at constant average and marginal costs of AC = MC = 10. Originally, the firm faces a market demand curve given by Q = 60 - P. Calculate the profit-maximizing price for the firm.",35,integer,NONE,"text | Walter Nicholson, Microeconomic Theory Basic Principles and Extensions.",elainewan/econ_micro_14_2.json,profit maximization,Economics,Finance,"Profit maximization is an economic concept that refers to the process by which firms or businesses aim to achieve the highest possible level of profit in their operations. In economics, profit is the difference between a firm's total revenue and its total costs. Profit maximization is a primary objective for many firms, as it directly impacts the firm's financial success, growth potential, and shareholder value.

To maximize profit, firms must find the optimal balance between their production costs and the prices they charge for their goods or services. This involves making strategic decisions about production levels, pricing, marketing, and resource allocation, among other factors.

There are two main approaches to profit maximization:

1. Total Revenue - Total Cost (TR-TC) Approach: This approach involves finding the level of output where the difference between total revenue and total cost is the greatest. Firms must consider both fixed and variable costs in their calculations and determine the optimal production level that maximizes the profit.

2. Marginal Revenue - Marginal Cost (MR-MC) Approach: This approach focuses on the additional revenue and cost generated by producing one more unit of output. Profit maximization occurs when marginal revenue (the additional revenue from selling one more unit) equals marginal cost (the additional cost of producing one more unit). At this point, any further increase in production would not result in higher profits, as the additional cost of producing more units would outweigh the additional revenue generated.

In a perfectly competitive market, firms are price takers, meaning they have no control over the market price of their product. In this case, profit maximization occurs when the firm produces at the level where its marginal cost equals the market price. In contrast, firms with market power, such as monopolies or oligopolies, can influence the market price and must consider the demand for their product when determining the profit-maximizing price and output level.

It is important to note that profit maximization may not always be the sole objective of a firm. Other objectives, such as market share growth, social responsibility, or long-term sustainability, may also influence a firm's decision-making process."
10,"Suppose there are three routers between a source host and a destination host. Ignoring fragmentation, an IP datagram sent from the source host to the destination host will travel over how many interfaces? How many forwarding tables will be indexed to move the datagram from the source to the destination? Answer in [Interfaces, Tables].","[8, 4]",list of integer,NONE,website | https://vinesmsuic.github.io/notes-networking3-qa/,maxku/ipnetwork2-ip.json,internet protocol,Computer networking,EECS,"Internet Protocol (IP) is a set of rules and standards that govern how data is transmitted, received, and routed across computer networks, including the internet. It is a fundamental component of the Internet Protocol Suite, which is a collection of protocols and technologies that enable communication between devices over the internet.

IP operates at the network layer (Layer 3) of the Open Systems Interconnection (OSI) model and is responsible for addressing, packaging, and routing data packets between devices. It ensures that data is sent from a source device to a destination device, even if they are on different networks.

There are two main versions of IP in use today: IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol version 6).

IPv4 is the most widely used version, which uses 32-bit addresses, allowing for approximately 4.3 billion unique IP addresses. Due to the rapid growth of the internet, the number of available IPv4 addresses has become limited, leading to the development of IPv6.

IPv6 uses 128-bit addresses, providing a vastly larger number of unique IP addresses (approximately 3.4 x 10^38) to accommodate the growing number of devices connected to the internet.

Key features of Internet Protocol include:

1. Addressing: IP assigns unique addresses to devices on a network, enabling them to be identified and located. These addresses are used to route data packets to their intended destinations.

2. Packetization: IP divides data into smaller units called packets, which are then transmitted independently across the network. This allows for more efficient use of network resources and enables data to be sent over multiple paths.

3. Routing: IP uses routing algorithms to determine the best path for data packets to travel from the source device to the destination device. Routers, which are specialized devices that connect networks, use IP addresses to forward packets along the most efficient route.

4. Error detection: IP includes a checksum mechanism to detect errors in the header of data packets. If an error is detected, the packet is discarded, and the sender may be notified to resend the data.

5. Fragmentation and reassembly: IP can fragment large packets into smaller ones to accommodate the maximum transmission unit (MTU) of different networks. The destination device then reassembles the fragments back into the original data.

Overall, Internet Protocol plays a crucial role in enabling communication between devices on computer networks and the internet, providing the foundation for various applications and services we use daily."
617,What is the coefficient of $x^2y^5$ for the formula $(x + 2y)^7$?,672,integer,NONE,self,wenhuchen/binomial.json,binomial theorem,Combinatorics,Math,"The Binomial Theorem, in the context of combinatorics, is a powerful mathematical principle that allows us to expand expressions of the form (a + b)^n, where 'a' and 'b' are any real numbers, and 'n' is a non-negative integer. The theorem provides a systematic way to find the coefficients of the terms in the expanded form of the binomial expression.

The Binomial Theorem states that for any non-negative integer 'n' and any real numbers 'a' and 'b':

(a + b)^n = Σ [C(n, k) * a^(n-k) * b^k]

where the summation (Σ) runs from k = 0 to k = n, and C(n, k) represents the binomial coefficient, which is the number of ways to choose 'k' items from a set of 'n' items, also denoted as ""n choose k"" or C(n, k) = n! / (k! * (n-k)!), where '!' denotes the factorial function.

The binomial coefficients can also be represented using Pascal's Triangle, a triangular array of numbers where each number is the sum of the two numbers directly above it. The 'n'th row of Pascal's Triangle contains the coefficients of the binomial expansion of (a + b)^n.

In combinatorics, the Binomial Theorem is used to solve counting problems, such as finding the number of ways to arrange objects, the number of subsets of a given size, and the probability of certain outcomes in experiments.

For example, using the Binomial Theorem, we can find the expansion of (a + b)^4:

(a + b)^4 = C(4, 0) * a^4 * b^0 + C(4, 1) * a^3 * b^1 + C(4, 2) * a^2 * b^2 + C(4, 3) * a^1 * b^3 + C(4, 4) * a^0 * b^4
           = 1 * a^4 + 4 * a^3 * b + 6 * a^2 * b^2 + 4 * a * b^3 + 1 * b^4"
262,"The following data related the rubber percentage of two types of rubber plants, where the sample have been drawn independently. Test for their mean difference. Type 1: 6.21 5.70 6.04 4.47 5.22 4.45 4.84 5.84 5.88 5.82 6.09 5.59 6.06 5.59 6.74 5.55, Type 2: 4.28 7.71 6.48 7.71 7.37 7.20 7.06 6.40 8.93 5.91 5.51 6.36. Are there difference between these two rubber plants?",True,bool,NONE,website | http://eagri.org/eagri50/STAM101/pdf/pract07.pdf,wenhuchen/t_test3.json,t-test,Statistics,Math,"A T-Test, or Student's T-Test, is a statistical hypothesis test used to determine whether there is a significant difference between the means of two groups or samples. It is commonly used in research and data analysis to compare the means of two independent groups and assess whether any observed differences are due to chance or are statistically significant.

The T-Test is based on the T-distribution, which is a probability distribution that closely resembles the normal distribution but has thicker tails. The T-distribution is used when the sample size is small or the population variance is unknown.

There are three main types of T-Tests:

1. Independent Samples T-Test: This test is used when comparing the means of two independent groups, such as comparing the test scores of students from two different schools.

2. Paired Samples T-Test: This test is used when comparing the means of two related groups, such as comparing the test scores of students before and after a tutoring program.

3. One-Sample T-Test: This test is used when comparing the mean of a single group to a known population mean, such as comparing the average height of a group of students to the national average height.

To perform a T-Test, the following steps are typically followed:

1. State the null hypothesis (H0) and the alternative hypothesis (H1). The null hypothesis usually states that there is no significant difference between the means of the two groups, while the alternative hypothesis states that there is a significant difference.

2. Calculate the T-statistic, which is a measure of the difference between the sample means relative to the variability within the samples.

3. Determine the degrees of freedom, which is a measure of the amount of information available in the data to estimate the population parameters.

4. Find the critical value or p-value, which is the probability of observing a T-statistic as extreme or more extreme than the one calculated, assuming the null hypothesis is true.

5. Compare the T-statistic to the critical value or p-value to determine whether to reject or fail to reject the null hypothesis. If the T-statistic is greater than the critical value or the p-value is less than the significance level (commonly set at 0.05), the null hypothesis is rejected, and the difference between the means is considered statistically significant.

In summary, the T-Test is a widely used statistical method for comparing the means of two groups or samples to determine if there is a significant difference between them. It is particularly useful when dealing with small sample sizes or when the population variance is unknown."
18,"Find the curvature for f(x) = \sqrt{4x - x^2}, x = 2.",0.5,float,NONE,"website | https://openstax.org/books/calculus-volume-3/pages/3-3-arc-length-and-curvature#:~:text=T%20(%20t%20)%20%3D%20r%20%E2%80%B2,s%20)%20with%20respect%20to%20s.",wenhuchen/curvature2.json,curvature,Calculus,Math,"Curvature is a concept in calculus that measures the amount of bending or deviation from a straight line in a curve or surface. It is a fundamental concept in differential geometry, which is the study of curves and surfaces in higher-dimensional spaces.

In the context of a curve in two-dimensional space, curvature is defined as the rate of change of the tangent vector with respect to the arc length of the curve. Intuitively, it represents how quickly the curve is changing direction. A straight line has zero curvature, while a circle has constant curvature.

Mathematically, the curvature (k) of a curve can be defined as:

k = |dT/ds|

where T is the unit tangent vector to the curve, and s is the arc length parameter.

For a curve defined by a parametric equation, r(t) = (x(t), y(t)), the curvature can be computed using the formula:

k = (x'(t)y''(t) - x''(t)y'(t)) / (x'(t)^2 + y'(t)^2)^(3/2)

where x'(t) and y'(t) are the first derivatives of x(t) and y(t) with respect to t, and x''(t) and y''(t) are the second derivatives.

In three-dimensional space, the curvature of a curve can be similarly defined using the Frenet-Serret formulas, which involve the tangent, normal, and binormal vectors.

For surfaces, curvature is a more complex concept, as there are multiple ways to measure the bending of a surface. Two common types of curvature for surfaces are Gaussian curvature and mean curvature. Gaussian curvature is an intrinsic property of the surface, meaning it is invariant under isometric transformations (such as bending without stretching). Mean curvature, on the other hand, is an extrinsic property that depends on the surface's embedding in three-dimensional space."
620,matrix $A=(\begin{array}{rrrr} -2 & -1 & -1 & -1 \ 2 & 1 & 3 & 2 \ 1 & 1 & 0 & 1 \ -1 & -1 & -2 & -2 \end{array})$. Suppose f is the minimal polynomial of A. What is f(99)? Return the numeric without explanation.,990000.0,float,NONE,linear algebra 7.1 example 1,mingyin/minimal-polynomial1.json,minimal polynomial,Algebra,Math,"In algebra, the minimal polynomial of an element α over a field F is the monic polynomial of the smallest degree that has α as a root and has coefficients in F. In other words, it is the polynomial f(x) with the least degree such that f(α) = 0 and f(x) has coefficients in F.

The minimal polynomial is an important concept in field theory and linear algebra, as it helps to determine the algebraic properties of an element and its relationship with the field it belongs to. It is particularly useful in the study of field extensions, algebraic numbers, and linear transformations.

For example, consider the element α = √2, which is not in the field of rational numbers Q. The minimal polynomial of α over Q is f(x) = x^2 - 2, as it is the monic polynomial of the smallest degree with rational coefficients that has α as a root."
22,Suppose g(x) is the horizontal asymptote of function f(x) = (3^x)/(1+3^{-x}). What are possible values of g(2023)?,0,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_10.json,asymptotic theory,Calculus,Math,"Asymptotic Theory in calculus is a branch of mathematical analysis that deals with the study of the behavior of functions, sequences, or series as their arguments or indices approach specific values, such as infinity or other limits. The main goal of asymptotic theory is to understand and describe the long-term behavior of these mathematical objects by finding simpler functions or expressions that approximate them when the argument or index is very large.

In the context of calculus, asymptotic theory is often used to analyze the growth rates of functions, the convergence of series, and the behavior of functions near singularities or other critical points. Some key concepts and techniques in asymptotic theory include:

1. Asymptotic notation: This is a set of notations used to describe the limiting behavior of functions. The most common notations are Big O (O), Little o (o), Big Omega (Ω), Little omega (ω), and Theta (Θ). These notations help to compare the growth rates of functions and provide a way to express the asymptotic behavior concisely.

2. Asymptotic expansion: An asymptotic expansion is a series representation of a function that approximates the function in the limit as the argument approaches a specific value. The terms in the expansion are usually ordered by their growth rates, with the fastest-growing terms appearing first. Asymptotic expansions are useful for obtaining approximate solutions to problems when exact solutions are difficult or impossible to find.

3. Asymptotes: An asymptote is a line or curve that a function approaches as its argument approaches a specific value. There are three types of asymptotes: horizontal, vertical, and oblique (or slant). Asymptotes help to visualize the long-term behavior of functions and can be used to analyze their properties.

4. Limits: Limits are a fundamental concept in calculus and are used to define continuity, derivatives, and integrals. In the context of asymptotic theory, limits are used to study the behavior of functions, sequences, and series as their arguments or indices approach specific values.

5. L'Hôpital's rule: This is a technique used to find the limit of a ratio of two functions when both the numerator and denominator approach zero or infinity. L'Hôpital's rule can be applied to determine the asymptotic behavior of functions in indeterminate forms.

Asymptotic theory has applications in various fields of mathematics, including analysis, number theory, and combinatorics. It is also widely used in applied mathematics, physics, and engineering to analyze and solve problems that involve large-scale or long-term behavior."
107,How many labeled trees are there on 6 vertices?,1296,integer,NONE,self,jianyu_xu/Graph_2.json,cayley's formula,Graph theory,EECS,"Cayley's formula is a result in graph theory that provides the number of distinct labeled trees that can be formed using a specific number of vertices. It is named after the British mathematician Arthur Cayley, who first stated the formula in 1889.

The formula states that for a given number of vertices n, there are n^(n-2) distinct labeled trees that can be formed. In other words, if you have n vertices, you can create n^(n-2) different trees where each vertex is uniquely labeled.

Cayley's formula can be derived using several methods, including the Matrix Tree Theorem and Prüfer sequences. The formula is particularly useful in combinatorics and graph theory, as it helps to enumerate the number of possible tree structures for a given set of vertices. This has applications in various fields, such as computer science, biology, and network analysis."
380,"We know that $y'=(x+y) / 2$, we also know that $y(x=0) = 2, y(x=0.5) = 2.636, y(x=1) = 3.595, y(x=1.5) = 4.9868$, what is the value of y(2) using Adams bashforth predictor method.",6.8731,float,NONE,website | https://atozmath.com/example/CONM/PredCorrector.aspx?he=e&q=adams,wenhuchen/Adams-Bashforth2.json,adams-bashforth method,Numerical analysis,Math,"The Adams-Bashforth method is a family of explicit numerical methods used for solving ordinary differential equations (ODEs) with initial value problems. These methods are part of a broader class of techniques called linear multistep methods, which use information from previous steps to compute the solution at the current step.

The general form of an ODE is:

dy/dt = f(t, y(t))

where y(t) is the unknown function we want to approximate, and f(t, y(t)) is a given function that describes the rate of change of y with respect to t.

The Adams-Bashforth methods are based on the idea of approximating the integral of the rate function f(t, y(t)) over a small time interval [t_n, t_(n+1)] using polynomial interpolation. The methods use the values of f(t, y(t)) at previous time steps to construct a polynomial that approximates f(t, y(t)) over the interval, and then integrate this polynomial to obtain an estimate for y(t_(n+1)).

The order of the Adams-Bashforth method depends on the number of previous time steps used in the polynomial interpolation. For example, the first-order Adams-Bashforth method (also known as the forward Euler method) uses only the most recent time step:

y(t_(n+1)) ≈ y(t_n) + h * f(t_n, y(t_n))

where h is the time step size.

The second-order Adams-Bashforth method uses the two most recent time steps:

y(t_(n+1)) ≈ y(t_n) + h * (3/2 * f(t_n, y(t_n)) - 1/2 * f(t_(n-1), y(t_(n-1))))

Higher-order Adams-Bashforth methods can be derived similarly by including more previous time steps in the polynomial interpolation.

The main advantage of Adams-Bashforth methods is that they can achieve high accuracy with relatively low computational cost, especially for higher-order methods. However, being explicit methods, they may suffer from stability issues for stiff ODEs, which may require the use of implicit methods like the Adams-Moulton methods."
321,"Let W(t) be the standard Brownian motion, and 0 < s < t. Find the conditional PDF of W(s = 1/2) given that W(t = 1) = 2. What are the mean and variance? Return the list of [mean, variance].","[1.0, 0.25]",list of float,NONE,website | https://www.probabilitycourse.com/chapter11/11_4_3_solved_probs.php,wenhuchen/wiener_process2.json,wiener process,Stochastic process,Math,"A Wiener process, also known as Brownian motion or random walk, is a continuous-time stochastic process that models the random movement of particles suspended in a fluid or the erratic fluctuations in financial markets. It is named after the mathematician Norbert Wiener, who provided a rigorous mathematical description of the process.

The Wiener process has the following key properties:

1. It starts at zero: W(0) = 0.
2. It has independent increments: The change in the process over non-overlapping time intervals is independent of each other.
3. It has normally distributed increments: The change in the process over a given time interval follows a normal distribution with mean 0 and variance proportional to the length of the time interval (i.e., W(t) - W(s) ~ N(0, t-s) for t > s).
4. It has continuous paths: The function W(t) is continuous in time, meaning that the process does not have any jumps or discontinuities.

The Wiener process is widely used in various fields, including physics, finance, and engineering, to model random phenomena. In finance, for example, it is used to model the unpredictable movement of stock prices and exchange rates. In physics, it is used to describe the random motion of particles in a fluid, known as Brownian motion."
44,"Titan, the largest moon of Saturn, has a mean orbital radius of 1.22x10^9 m. The orbital period of Titan is 15.95 days. Hyperion, another moon of Saturn, orbits at a mean radius of 1.48x10^9 m. Use Kepler's third law of planetary motion to predict the orbital period of Hyperion in days.",21.3,float,NONE,website | https://www.lehman.edu/faculty/anchordoqui/101-P4_s.pdf,wenhuchen/kepler's_law1.json,kepler's third law,Celestial mechanics,Physics,"Kepler's Third Law, also known as the Law of Harmonies, is one of the three fundamental laws of planetary motion formulated by the German astronomer Johannes Kepler in the early 17th century. This law relates the orbital period of a planet to its average distance from the Sun, stating that the square of the orbital period of a planet is directly proportional to the cube of the semi-major axis of its orbit.

Mathematically, Kepler's Third Law can be expressed as:

(T₁/T₂)² = (a₁/a₂)³

where T₁ and T₂ are the orbital periods of two planets, and a₁ and a₂ are the semi-major axes of their respective orbits.

In simpler terms, this law implies that planets that are closer to the Sun have shorter orbital periods and move faster in their orbits, while planets that are farther away from the Sun have longer orbital periods and move slower in their orbits. This relationship holds true for all planets in our solar system and can also be applied to other celestial bodies, such as moons orbiting a planet or exoplanets orbiting a star."
676,What are the generators of the additive cyclic group Z?,"[1, -1]",list of integer,NONE,"text | Thomas W. Hungerford, Abstract Algebra An Introduction.",elainewan/math_abstact_algebra_7_6.json,generating set of a group,Group theory,Math,"In group theory, a generating set of a group is a subset of the group's elements such that every element of the group can be expressed as a finite combination of these elements and their inverses. In other words, a generating set is a collection of elements that can be used to ""build"" the entire group through the group operation (e.g., multiplication, addition, etc.) and taking inverses.

A group G is said to be generated by a set S if every element of G can be obtained by applying the group operation to the elements of S and their inverses, possibly multiple times. The set S is then called a generating set of G. If a group has a finite generating set, it is called finitely generated.

For example, consider the group of integers under addition, denoted as (Z, +). The set {1, -1} is a generating set for this group, as every integer can be expressed as a sum of 1's and/or -1's. Another generating set for the same group is {2, 3}, as every integer can be expressed as a linear combination of 2 and 3.

A group can have multiple generating sets, and the size of the smallest generating set is called the rank of the group. A group with a single element as its generating set is called a cyclic group.

In summary, a generating set of a group is a subset of the group's elements that can be used to construct the entire group through the group operation and taking inverses. Generating sets are essential in understanding the structure and properties of groups in group theory."
207,"An observer S who lives on the x-axis sees a flash of red light at x = 1210 m, then after 4.96 µs, a flash of blue at x = 480 m. Use subscripts R and B to label the coordinates of the events. What is the measured time interval (in µs) between these flashes?",4.32,float,NONE,website | https://oyc.yale.edu/sites/default/files/problem_set_7_solutions_4.pdf,wenhuchen/relativity3.json,relativity,Relativity,Physics,"Relativity is a scientific theory that fundamentally changed our understanding of space, time, and gravity. It was first introduced by the renowned physicist Albert Einstein in the early 20th century and consists of two parts: the Special Theory of Relativity and the General Theory of Relativity.

1. Special Theory of Relativity (1905): This theory deals with objects moving at constant speeds, particularly those moving close to the speed of light. It is based on two main principles:

   a. The Principle of Relativity: The laws of physics are the same for all observers in uniform motion relative to one another.
   b. The Constancy of the Speed of Light: The speed of light in a vacuum is the same for all observers, regardless of their motion or the motion of the light source.

The Special Theory of Relativity led to several counterintuitive conclusions, such as time dilation (moving clocks run slower), length contraction (moving objects appear shorter), and the equivalence of mass and energy (E=mc^2), which states that mass can be converted into energy and vice versa.

2. General Theory of Relativity (1915): This theory is an extension of the Special Theory of Relativity and deals with gravity. It describes gravity not as a force between masses, as proposed by Sir Isaac Newton, but as a curvature of spacetime caused by the presence of mass. In other words, massive objects like planets and stars warp the fabric of spacetime, causing other objects to move along curved paths.

The General Theory of Relativity has been confirmed through various experiments and observations, such as the bending of light around massive objects (gravitational lensing), the shift in the orbit of Mercury, and the detection of gravitational waves.

In summary, Relativity is a groundbreaking theory that has reshaped our understanding of the universe, providing a more accurate and comprehensive description of the fundamental concepts of space, time, and gravity."
708,"Let A be an invertible n * n matrix and v and eigenvector of both A and B, is v necesarily an eigenvector of A + B?",True,bool,NONE,"text | Otto Bretscher, Linear Algebra with Applications.",elainewan/math_algebra_7.json,eigenvalues and eigenvectors,Algebra,Math,"Eigenvalues and eigenvectors are fundamental concepts in linear algebra, particularly in the study of linear transformations and matrices. They provide insight into the behavior of a linear transformation and can be used to solve various problems in mathematics, physics, and engineering.

Eigenvalues:
An eigenvalue (denoted by λ) is a scalar value associated with a given square matrix (A) that satisfies the following equation:

A * v = λ * v

where A is a square matrix, v is a non-zero vector (called the eigenvector), and λ is the eigenvalue. In other words, when a matrix A is multiplied by an eigenvector v, the result is a scaled version of the same eigenvector, with the scaling factor being the eigenvalue λ.

To find the eigenvalues of a matrix, we need to solve the following equation:

det(A - λ * I) = 0

where det() denotes the determinant of a matrix, I is the identity matrix of the same size as A, and λ is the eigenvalue. The solutions to this equation are the eigenvalues of the matrix A.

Eigenvectors:
An eigenvector (denoted by v) is a non-zero vector that, when multiplied by a square matrix A, results in a scaled version of itself, with the scaling factor being the eigenvalue λ. As mentioned earlier, the relationship between a matrix A, its eigenvector v, and the corresponding eigenvalue λ can be expressed as:

A * v = λ * v

Eigenvectors are essential in understanding the geometric interpretation of a linear transformation represented by a matrix. They indicate the directions in which the transformation stretches or compresses the space, while the eigenvalues represent the magnitude of the stretching or compression.

In summary, eigenvalues and eigenvectors are crucial concepts in linear algebra that help us understand the properties and behavior of linear transformations and matrices. They have numerous applications in various fields, including differential equations, quantum mechanics, computer graphics, and data analysis."
108,"Find the fraction of the standard solar flux reaching the Earth (about 1000 W/m^22) available to a solar collector lying flat on the Earth’s surface at Regina, Saskatchewan (latitude 50°N) at noon on the summer solstice.",0.891,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Exercise 11.13,tonyxia/semiconductor6.json,semiconductor theory,Condensed matter physics,Physics,"Semiconductor Theory is a branch of condensed matter physics that deals with the study of semiconductors, which are materials that have electrical conductivity between that of insulators and conductors. These materials have unique properties that make them essential for various electronic devices, such as transistors, diodes, and solar cells.

The fundamental concept in semiconductor theory is the electronic band structure, which describes the energy levels available to electrons in a solid material. In a semiconductor, there are two important bands: the valence band and the conduction band. The valence band is filled with electrons, while the conduction band is initially empty. The energy gap between these two bands is called the bandgap.

In insulators, the bandgap is large, making it difficult for electrons to move from the valence band to the conduction band. In conductors, the valence and conduction bands overlap, allowing electrons to move freely. Semiconductors have a small bandgap, which means that electrons can be excited from the valence band to the conduction band under certain conditions, such as the application of heat or light.

There are two main types of semiconductors: intrinsic and extrinsic. Intrinsic semiconductors are pure materials, such as silicon or germanium, with no impurities. In these materials, the number of electrons in the conduction band is equal to the number of holes (empty spaces) in the valence band. The electrical conductivity of intrinsic semiconductors is relatively low.

Extrinsic semiconductors are created by introducing impurities, or dopants, into the intrinsic semiconductor. This process, called doping, can create either n-type or p-type semiconductors. In n-type semiconductors, the dopant atoms have more valence electrons than the semiconductor atoms, resulting in an excess of free electrons. In p-type semiconductors, the dopant atoms have fewer valence electrons, creating an excess of holes. The interaction between n-type and p-type materials forms the basis for many electronic devices, such as diodes and transistors.

Semiconductor theory also involves the study of various physical phenomena, such as carrier transport (how electrons and holes move through the material), recombination (the process by which electrons and holes combine), and the behavior of semiconductors under different conditions, such as temperature and illumination.

Overall, semiconductor theory is a crucial area of condensed matter physics that has led to the development of modern electronics and continues to drive advancements in technology."
59,Does cos(x) = x^k have a solution for k = 2023?,True,bool,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_11.json,intermediate value theorem,Calculus,Math,"The Intermediate Value theorem (IVT) is a fundamental theorem in calculus that states that if a continuous function, f(x), is defined on a closed interval [a, b] and takes values f(a) and f(b) at each end of the interval, then for any value k between f(a) and f(b), there exists at least one value c in the open interval (a, b) such that f(c) = k.

In simpler terms, the theorem states that if you have a continuous function on a closed interval, and you pick any value between the function's values at the endpoints of the interval, then there must be at least one point within the interval where the function takes that value.

The IVT is particularly useful for proving the existence of solutions to equations and for approximating the roots of functions. It is based on the idea that continuous functions do not have any gaps or jumps in their graphs, so if the function starts at one value and ends at another, it must pass through all the values in between."
336,"Given that each cone can contain two ice cream balls, how many different ice cream cones can you make if you have 6 flavors of ice cream and 5 types of cones?",180,integer,NONE,self,jianyu_xu/multiplication_1.json,counting,Combinatorics,Math,"Counting, in the context of combinatorics, is a branch of mathematics that deals with the enumeration, arrangement, and selection of objects or elements in a set. It involves finding the number of ways to perform a specific task, such as arranging items in a certain order, selecting a subset of items from a larger set, or distributing items among different groups.

Combinatorics uses various techniques and principles to solve counting problems, including:

1. The Rule of Sum: If there are m ways to perform one task and n ways to perform another task, and these tasks cannot be performed simultaneously, then there are m + n ways to perform either task.

2. The Rule of Product: If there are m ways to perform one task and n ways to perform another task, and these tasks can be performed independently, then there are m * n ways to perform both tasks.

3. Permutations: A permutation is an arrangement of objects in a specific order. The number of permutations of n objects is given by n! (n factorial), which is the product of all positive integers up to n.

4. Combinations: A combination is a selection of objects without regard to their order. The number of combinations of n objects taken r at a time is given by the binomial coefficient, denoted as C(n, r) or ""n choose r,"" and calculated as C(n, r) = n! / (r! * (n-r)!).

5. The Pigeonhole Principle: If n items are placed into m containers, and n > m, then at least one container must contain more than one item. This principle is used to prove the existence of certain arrangements or selections.

6. Inclusion-Exclusion Principle: This principle is used to count the number of elements in the union of multiple sets by considering the overlaps between the sets. It involves adding the sizes of individual sets and subtracting the sizes of their intersections.

Counting problems are common in various fields, including probability theory, statistics, computer science, and cryptography. Combinatorics helps in solving these problems by providing systematic methods and techniques for counting and organizing objects."
662,"Let X_1, X_2,... be independent variables each taking values +1 or -1 with probabilities 1/2 and 1/2. It is know that $\sqrt{3/n^3}*\sum_{k=1}^n k*X_k$ converges in distribution normal distribution N(a,b) as n goes to infinity. Here a is the expectation and b is the variance. What are the values of a and b? Return the answers as a list. For example, if a=2, b=100, return [2,100].","[0, 1]",list of integer,NONE,probability: 5.12.41,mingyin/central-limit-theorem1.json,central limit theorem,Statistics,Math,"The Central Limit Theorem (CLT) is a fundamental concept in statistics that states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, also known as a Gaussian or bell curve, regardless of the original distribution of the variables.

In simpler terms, the Central Limit Theorem explains why many natural phenomena and processes tend to follow a normal distribution, even if the individual variables that contribute to the phenomena do not follow a normal distribution themselves.

The key conditions for the Central Limit Theorem to hold are:

1. The random variables must be independent, meaning that the occurrence of one variable does not affect the occurrence of another variable.
2. The random variables must be identically distributed, meaning that they all have the same probability distribution.
3. The number of random variables being summed or averaged must be sufficiently large, typically assumed to be greater than or equal to 30.

The Central Limit Theorem has important implications in statistics, as it allows for the use of normal distribution-based techniques, such as confidence intervals and hypothesis testing, even when the underlying data may not be normally distributed. This is particularly useful in fields like sampling and inferential statistics, where researchers often work with large samples to make inferences about populations."
34,"Consider Convolutional Neural Network D2 which takes input images of size 32x32 with 1 colour channels. The first layer of D2 uses 4 filters of size 5x5, a stride of 2, and zero-padding of width 1. Consider CNN D2 which takes input images of size 32x32 with 1 colour channels. The first layer of D2 uses 4 filters of size 5x5, a stride of 2, and zero-padding of width 1. What is the total number of weights defined for the entire activation output of this first layer? (ie. If you flattened all filters and channels into a single vector)",900,integer,NONE,self,maxku/cv-cnn5.json,neural network theory,Machine learning,EECS,"Neural Network theory is a subfield of machine learning that focuses on the development and application of artificial neural networks (ANNs) to model and solve complex problems. ANNs are computational models inspired by the structure and functioning of the human brain, specifically the way neurons process and transmit information.

The basic building block of an ANN is the artificial neuron, also known as a node or unit. These neurons are organized into layers: an input layer, one or more hidden layers, and an output layer. Each neuron receives input from the previous layer, processes it, and sends the output to the next layer. The connections between neurons have associated weights, which determine the strength of the signal being transmitted.

Neural Network theory involves the following key concepts:

1. Activation function: This is a mathematical function applied to the input of a neuron to determine its output. Common activation functions include the sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).

2. Learning algorithm: This is the process by which the neural network adjusts its weights to minimize the error between its predicted output and the actual output (ground truth). The most common learning algorithm is backpropagation, which involves computing the gradient of the error with respect to each weight and updating the weights accordingly.

3. Loss function: This is a measure of the difference between the predicted output and the actual output. The goal of the learning algorithm is to minimize the loss function. Common loss functions include mean squared error, cross-entropy, and hinge loss.

4. Regularization: This is a technique used to prevent overfitting, which occurs when the neural network learns the training data too well and performs poorly on unseen data. Regularization methods, such as L1 and L2 regularization, add a penalty term to the loss function to encourage the network to learn simpler models with smaller weights.

5. Optimization: This involves finding the best set of weights for the neural network to minimize the loss function. Common optimization algorithms include gradient descent, stochastic gradient descent, and more advanced methods like Adam and RMSprop.

Neural Network theory has evolved over the years, leading to the development of various types of neural networks, such as convolutional neural networks (CNNs) for image recognition, recurrent neural networks (RNNs) for sequence data, and deep learning architectures that can model complex patterns and representations in large datasets. These advancements have enabled neural networks to achieve state-of-the-art performance in various tasks, including image classification, natural language processing, speech recognition, and game playing."
413,"You have a coin and you would like to check whether it is fair or biased. More specifically, let $\theta$ be the probability of heads, $\theta = P(H)$. Suppose that you need to choose between the following hypotheses: H_0 (null hypothesis): The coin is fair, i.e. $\theta = \theta_0 = 1 / 2$. H_1 (the alternative hypothesis): The coin is not fair, i.e. $\theta > 1 / 2$. We toss 100 times and observe 60 heads. Can we reject H_0 at significance level $\alpha = 0.01$?",False,bool,NONE,website | https://www.probabilitycourse.com/chapter8/8_4_4_p_vals.php,wenhuchen/p_value2.json,p-value,Statistics,Math,"In statistics, the P-value (probability value) is a measure used to help determine the significance of a result or finding in hypothesis testing. It represents the probability of observing a test statistic as extreme or more extreme than the one obtained from the sample data, assuming that the null hypothesis is true.

The null hypothesis is a statement that assumes there is no effect or relationship between the variables being tested, and the alternative hypothesis is the statement that there is an effect or relationship. The P-value is used to make a decision about whether to reject or fail to reject the null hypothesis.

A smaller P-value indicates stronger evidence against the null hypothesis, suggesting that the observed result is unlikely to have occurred by chance alone. A larger P-value indicates weaker evidence against the null hypothesis, suggesting that the observed result may have occurred by chance.

Typically, a threshold value called the significance level (commonly denoted as α) is set, often at 0.05 or 5%. If the P-value is less than or equal to α, the null hypothesis is rejected, and the result is considered statistically significant. If the P-value is greater than α, the null hypothesis is not rejected, and the result is considered not statistically significant."
272,How many ways are there to divide a set of 5 elements into 2 non-empty ordered subsets?,240,integer,NONE,self,jianyu_xu/Lah_number_2.json,lah number,Combinatorics,Math,"In combinatorics, Lah numbers are a sequence of numbers that arise in the study of permutations and combinations. They are denoted by L(n, k) and are defined as the number of ways to arrange n distinct items into k non-empty linear lists, where each list is ordered, and the order of the lists also matters. In other words, Lah numbers count the number of ways to partition a set of n elements into k non-empty ordered subsets.

Lah numbers can be expressed using factorials and Stirling numbers of the second kind, which are denoted by S(n, k). The formula for Lah numbers is:

L(n, k) = (n - 1)! * S(n, k) * k!

where n! (n factorial) is the product of all positive integers up to n, and S(n, k) is the Stirling number of the second kind.

Some properties of Lah numbers include:

1. L(n, 1) = (n - 1)! for all n ≥ 1, since there is only one way to arrange n items into a single ordered list.
2. L(n, n) = n! for all n ≥ 1, since there are n! ways to arrange n items into n ordered lists, each containing one item.
3. L(n, k) = 0 for k > n, since it is not possible to arrange n items into more than n non-empty ordered lists.

Lah numbers have applications in various areas of mathematics, including combinatorics, probability theory, and the study of special functions."
298,"Does the utility function U(x,y) = xy/(x+y) has a convex indifference curve?",True,bool,NONE,"text | Walter Nicholson, Microeconomic Theory Basic Principles and Extensions.",elainewan/econ_micro_3.json,indifference curves,Economics,Finance,"Indifference curves are graphical representations used in economics to illustrate the preferences of a consumer for different combinations of goods or services. They show various bundles of goods that provide the same level of satisfaction or utility to the consumer, meaning the consumer is indifferent between these bundles.

Some key features of indifference curves are:

1. Downward sloping: Indifference curves slope downward from left to right, indicating that as the quantity of one good increases, the quantity of the other good must decrease to maintain the same level of satisfaction.

2. Convex to the origin: Indifference curves are usually convex to the origin, reflecting the concept of diminishing marginal rate of substitution. This means that as a consumer consumes more of one good, they are willing to give up less and less of the other good to maintain the same level of satisfaction.

3. Higher indifference curves represent higher levels of satisfaction: A consumer prefers a combination of goods on a higher indifference curve to one on a lower curve, as higher curves represent higher levels of satisfaction or utility.

4. Non-intersecting: Indifference curves cannot intersect each other, as this would imply that the consumer has inconsistent preferences. If two curves intersect, it would mean that the consumer has the same level of satisfaction at two different points on both curves, which contradicts the assumption of consistent preferences.

5. Continuous: Indifference curves are assumed to be continuous, meaning that there are no gaps or jumps in the consumer's preferences.

Indifference curves are used in conjunction with budget constraints to analyze consumer behavior and determine the optimal consumption bundle that maximizes a consumer's satisfaction or utility, given their income and the prices of goods."
88,"G = Q, and G is under the operation a * b = a + b + 3. Is G a group?",True,bool,NONE,"text | Thomas W. Hungerford, Abstract Algebra An Introduction.",elainewan/math_abstact_algebra_7_4.json,definition of groups,Group theory,Math,"Group theory is a branch of mathematics that deals with the study of symmetry and structures that exhibit symmetry. In this context, a group is a set of elements combined with an operation that satisfies certain properties, which together define the structure of the group.

A group (G, *) consists of a set G and an operation * that combines two elements of G to produce a third element, also in G. The group must satisfy the following four properties:

1. Closure: For all elements a, b in G, the result of the operation a * b is also in G. This means that the group is closed under the operation.

2. Associativity: For all elements a, b, and c in G, the equation (a * b) * c = a * (b * c) holds. This means that the order in which the operation is performed does not affect the result.

3. Identity element: There exists an element e in G such that for every element a in G, the equation e * a = a * e = a holds. This element e is called the identity element of the group.

4. Inverse element: For every element a in G, there exists an element b in G such that a * b = b * a = e, where e is the identity element. This element b is called the inverse of a.

Groups can be finite or infinite, depending on the number of elements in the set G. They can also be classified as abelian (or commutative) if the operation is commutative, meaning that for all elements a and b in G, a * b = b * a. Otherwise, the group is called non-abelian.

Group theory has applications in various fields of mathematics, as well as in physics, chemistry, and computer science. It is particularly useful for understanding the symmetries of geometric objects and analyzing the structure of algebraic systems."
411,"Solve the following linear programming problems: maximize 3x + y subject to (1) -x + y <= 1, (2) 2x + y <= 4, (3) x>= 0 and y >= 0. What's [x, y] for the optimal solution?","[2, 0]",list of integer,NONE,self,jianyu_xu/integer_programming_2.json,integer programming,Algebra,Math,"Integer Programming (IP) is a mathematical optimization technique that deals with linear programming problems where some or all of the variables are restricted to take integer values. It is a subfield of algebra and operations research, and it is used to model and solve a wide range of real-world problems, such as scheduling, resource allocation, transportation, and supply chain management.

In an integer programming problem, the objective is to optimize a linear function of variables, subject to a set of linear constraints, while ensuring that the variables take integer values. The general form of an integer programming problem can be represented as follows:

Objective function:
Maximize or minimize Z = c1 * x1 + c2 * x2 + ... + cn * xn

Subject to constraints:
a11 * x1 + a12 * x2 + ... + a1n * xn ≤ b1
a21 * x1 + a22 * x2 + ... + a2n * xn ≤ b2
...
am1 * x1 + am2 * x2 + ... + amn * xn ≤ bm

And integer restrictions:
x1, x2, ..., xn ∈ Z (integer values)

Here, Z is the objective function to be maximized or minimized, xi (i = 1, 2, ..., n) are the decision variables, ci are the coefficients of the objective function, aij are the coefficients of the constraints, and bi are the constraint limits.

Integer programming problems can be classified into different types based on the nature of the integer restrictions:

1. Pure Integer Programming (PIP): All decision variables are required to be integers.
2. Mixed Integer Programming (MIP): Some decision variables are required to be integers, while others can take continuous values.
3. Binary Integer Programming (BIP) or 0-1 Integer Programming: All decision variables are binary, i.e., they can take only 0 or 1 values.

Solving integer programming problems can be computationally challenging, especially for large-scale problems, as the search space for integer solutions can be vast. Various algorithms and techniques, such as branch and bound, cutting planes, and heuristics, have been developed to efficiently solve integer programming problems."
709,"Assume that the Black-Scholes framework holds. The price of a nondividened-paying stock is $30. The price of a put option on this stock is $4.00. You are given $(i) $\Delta=-0.28$. (ii) $\Gamma=0.10$ Using the delta-gamma approximation, determine the price of the put option if the stock price changes to $31.50.",3.7,float,NONE,website | https://www.soa.org/globalassets/assets/files/edu/edu-mc-exam-mfe-0507.pdf,xueguangma/delta_gamma_approximation.json,delta gamma approximation,Derivatives,Finance,"Delta Gamma Approximation, also known as the second-order Taylor series approximation, is a method used in the field of financial derivatives to estimate the change in the value of an option or other derivative instruments due to small changes in the underlying asset's price. This approximation takes into account both the first-order (Delta) and second-order (Gamma) sensitivities of the option's price to the underlying asset's price.

Delta is the first derivative of the option's price with respect to the underlying asset's price. It measures the sensitivity of the option's price to a small change in the underlying asset's price. In other words, Delta represents the expected change in the option's price for a $1 change in the underlying asset's price.

Gamma is the second derivative of the option's price with respect to the underlying asset's price. It measures the rate of change of Delta as the underlying asset's price changes. In other words, Gamma represents the expected change in Delta for a $1 change in the underlying asset's price.

The Delta Gamma Approximation is particularly useful for managing the risk associated with options and other derivative instruments, as it helps traders and risk managers to estimate the potential impact of small price movements in the underlying asset on the value of their positions.

The formula for the Delta Gamma Approximation is as follows:

ΔP ≈ ΔS * Delta + 0.5 * (ΔS)^2 * Gamma

Where:
- ΔP is the change in the option's price
- ΔS is the change in the underlying asset's price
- Delta is the first-order sensitivity of the option's price to the underlying asset's price
- Gamma is the second-order sensitivity of the option's price to the underlying asset's price

This approximation assumes that higher-order derivatives (such as Vega, which measures sensitivity to changes in implied volatility) are negligible and that the changes in the underlying asset's price are small."
161,Let h(x) = (x^{-1/2} + 2x)(7 - x^{-1}). What is h'(x) when x = 4?,13.609,float,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_3_4.json,differential product rule,Calculus,Math,"The Differential Product Rule in calculus is a formula used to find the derivative of a product of two functions. It states that the derivative of the product of two functions is equal to the derivative of the first function times the second function plus the first function times the derivative of the second function. Mathematically, it can be represented as:

If u(x) and v(x) are two differentiable functions of x, then the derivative of their product, w(x) = u(x) * v(x), with respect to x is given by:

w'(x) = u'(x) * v(x) + u(x) * v'(x)

where w'(x) is the derivative of w(x) with respect to x, u'(x) is the derivative of u(x) with respect to x, and v'(x) is the derivative of v(x) with respect to x.

The Product Rule is essential in calculus as it simplifies the process of finding derivatives for products of functions, which is a common occurrence in various mathematical and real-world applications."
594,Does $p(x) = x^5 + x − 1$ have any real roots?,True,bool,NONE,textbook | Forrest_M147CN_F20.pdf,xueguangma/intermediate_value_theorem.json,intermediate value theorem,Calculus,Math,"The Intermediate Value theorem (IVT) is a fundamental theorem in calculus that states that if a continuous function, f(x), is defined on a closed interval [a, b] and takes values f(a) and f(b) at each end of the interval, then for any value k between f(a) and f(b), there exists at least one value c in the open interval (a, b) such that f(c) = k.

In simpler terms, the theorem states that if you have a continuous function on a closed interval, and you pick any value between the function's values at the endpoints of the interval, then there must be at least one point within the interval where the function takes that value.

The IVT is particularly useful for proving the existence of solutions to equations and for approximating the roots of functions. It is based on the idea that continuous functions do not have any gaps or jumps in their graphs, so if the function starts at one value and ends at another, it must pass through all the values in between."
631,"X rays scattered from rock salt (NaCl) are observed to have an intense maximum at an angle of 20° from the incident direction. Assuming n = 1 (from the intensity), what must be the Wavelength of the incident radiation in nm?",0.098,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Example 5.1,tonyxia/wave1.json,wave theorem,Wave,Physics,"Wave Theorem, also known as the Wave Equation, is a fundamental concept in physics that describes the behavior of waves, such as sound waves, light waves, and water waves. It is a partial differential equation that relates the wave's displacement at a given point in space and time to the properties of the medium through which the wave is propagating.

The general form of the wave equation is:

∂²ψ/∂t² = c² ∇²ψ

Here, ψ represents the wave's displacement, t is time, c is the wave's speed, and ∇² is the Laplacian operator, which represents the spatial derivatives of the wave's displacement. The equation states that the acceleration of the wave's displacement with respect to time (∂²ψ/∂t²) is proportional to the spatial curvature of the wave (∇²ψ) multiplied by the square of the wave's speed (c²).

The wave equation is essential in understanding various phenomena in physics, such as the propagation of sound in air, the behavior of electromagnetic waves, and the motion of waves on a string or in a fluid. It helps predict the behavior of waves under different conditions and is widely used in engineering, acoustics, optics, and other fields."
715,"for the matrix $A=(\begin{array}{rrrrr} 1 & 2 & 3 & 4 & -3 \1 & 2 & 0 & -5 & 1 \2 & 4 & -3 & -19 & 6 \3 & 6 & -3 & -24 & 7\end{array})$, what is its row rank and column rank? return the two numbers as a list.","[2, 2]",list of integer,NONE,linear algebra 2.2 example 3,mingyin/gaussian-elimination1.json,gaussian elimination,Algebra,Math,"Gaussian elimination, also known as row reduction, is an algebraic method used to solve systems of linear equations. It involves performing a series of operations on the augmented matrix (a matrix that combines the coefficients and constants of the linear equations) to transform it into a simpler form, called the row echelon form or the reduced row echelon form. This simplified form makes it easier to find the solutions to the system of linear equations.

The main operations used in Gaussian elimination are:

1. Swapping two rows.
2. Multiplying a row by a nonzero constant.
3. Adding or subtracting a multiple of one row to another row.

The goal of Gaussian elimination is to create a triangular matrix with zeros below the main diagonal (row echelon form) or zeros both below and above the main diagonal (reduced row echelon form). Once the matrix is in one of these forms, the solutions can be found using a technique called back-substitution.

Here's a step-by-step description of the Gaussian elimination process:

1. Start with the augmented matrix representing the system of linear equations.
2. Identify the leftmost column that has a nonzero entry.
3. If the top entry in that column is zero, swap the row with another row below it that has a nonzero entry in that column.
4. Divide the row by the leading entry (the first nonzero entry from the left) to make it 1. This is called the pivot.
5. Use the pivot row to eliminate all nonzero entries below the pivot by adding or subtracting multiples of the pivot row to the rows below it.
6. Repeat steps 2-5 for the remaining submatrix (the matrix formed by removing the rows and columns that have already been processed) until the entire matrix is in row echelon form.
7. (Optional) To obtain the reduced row echelon form, eliminate the nonzero entries above the pivots by adding or subtracting multiples of the pivot rows to the rows above them.
8. Use back-substitution to find the solutions to the system of linear equations.

Gaussian elimination is a fundamental technique in linear algebra and has numerous applications in fields such as engineering, physics, computer science, and economics."
290,"At a waterpark, sleds with riders are sent along a slippery, horizontal surface by the release of a large compressed spring. The spring with force constant k = 40.0 N/cm and negligible mass rests on the frictionless horizontal surface. One end is in contact with a stationary wall. A sled and rider with total mass 70.0 kg are pushed against the other end, compressing the spring 0.375 m. The sled is then released with zero initial velocity. What is the sled's speed (in m/s) when the spring returns to its uncompressed length?",2.83,float,NONE,textbook 6.43(a),xinyi/work_energy_theorem.json,work energy,Kinetics,Physics,"Work-Energy (Kinetics) is a concept in physics that deals with the relationship between the work done on an object and the change in its kinetic energy. Kinetic energy is the energy possessed by an object due to its motion, and work is the transfer of energy that occurs when a force is applied to an object, causing it to move.

The Work-Energy Principle states that the work done on an object is equal to the change in its kinetic energy. Mathematically, this can be represented as:

W = ΔKE = KE_final - KE_initial

Where W is the work done, ΔKE is the change in kinetic energy, KE_final is the final kinetic energy, and KE_initial is the initial kinetic energy.

This principle is useful in analyzing various physical situations, such as collisions, motion under the influence of forces, and energy transformations. It helps us understand how the energy of a system changes as work is done on it, and how this change in energy affects the motion of the object.

In summary, Work-Energy (Kinetics) is a fundamental concept in physics that describes the relationship between the work done on an object and the change in its kinetic energy, providing insights into the energy transformations and motion of objects under the influence of forces."
295,Consider a two-layer fully-connected neural network in which the hidden-unit nonlinear activation functions are given by logistic sigmoid functions. Does there exist an equivalent network in which the hidden unit nonlinear activation functions are given by hyperbolic tangent functions?,True,bool,NONE,textbook 5.1,xinyi/neural_networks.json,neural network theory,Machine learning,EECS,"Neural Network theory is a subfield of machine learning that focuses on the development and application of artificial neural networks (ANNs) to model and solve complex problems. ANNs are computational models inspired by the structure and functioning of the human brain, specifically the way neurons process and transmit information.

The basic building block of an ANN is the artificial neuron, also known as a node or unit. These neurons are organized into layers: an input layer, one or more hidden layers, and an output layer. Each neuron receives input from the previous layer, processes it, and sends the output to the next layer. The connections between neurons have associated weights, which determine the strength of the signal being transmitted.

Neural Network theory involves the following key concepts:

1. Activation function: This is a mathematical function applied to the input of a neuron to determine its output. Common activation functions include the sigmoid, hyperbolic tangent (tanh), and rectified linear unit (ReLU).

2. Learning algorithm: This is the process by which the neural network adjusts its weights to minimize the error between its predicted output and the actual output (ground truth). The most common learning algorithm is backpropagation, which involves computing the gradient of the error with respect to each weight and updating the weights accordingly.

3. Loss function: This is a measure of the difference between the predicted output and the actual output. The goal of the learning algorithm is to minimize the loss function. Common loss functions include mean squared error, cross-entropy, and hinge loss.

4. Regularization: This is a technique used to prevent overfitting, which occurs when the neural network learns the training data too well and performs poorly on unseen data. Regularization methods, such as L1 and L2 regularization, add a penalty term to the loss function to encourage the network to learn simpler models with smaller weights.

5. Optimization: This involves finding the best set of weights for the neural network to minimize the loss function. Common optimization algorithms include gradient descent, stochastic gradient descent, and more advanced methods like Adam and RMSprop.

Neural Network theory has evolved over the years, leading to the development of various types of neural networks, such as convolutional neural networks (CNNs) for image recognition, recurrent neural networks (RNNs) for sequence data, and deep learning architectures that can model complex patterns and representations in large datasets. These advancements have enabled neural networks to achieve state-of-the-art performance in various tasks, including image classification, natural language processing, speech recognition, and game playing."
493,"Determine values of the real numbers a, b, and c to make the function $x^2 + ay^2 + y + i(bxy + cx)$ by an analytical function of the complex variable of $x+iy$? Return your answer as a list [a, b, c].","[-1, 2, -1]",list of integer,NONE,website | https://haroldpboas.gitlab.io/courses/407-2017c/solution2.pdf,wenhuchen/cauchy_riemann1.json,cauchy riemann theorem,Complex analysis,Math,"The Cauchy-Riemann Theorem is a fundamental result in complex analysis that provides a set of necessary and sufficient conditions for a function to be holomorphic (i.e., complex-differentiable) in a domain. Holomorphic functions are complex functions that are differentiable at every point in their domain, and they play a central role in complex analysis.

The theorem is named after Augustin-Louis Cauchy and Bernhard Riemann, who independently developed the conditions now known as the Cauchy-Riemann equations. These equations relate the partial derivatives of the real and imaginary parts of a complex function.

Let f(z) be a complex function defined in a domain D, where z = x + iy is a complex variable with x and y being real numbers, and i is the imaginary unit (i.e., i^2 = -1). We can write f(z) as:

f(z) = u(x, y) + iv(x, y),

where u(x, y) and v(x, y) are real-valued functions representing the real and imaginary parts of f(z), respectively.

The Cauchy-Riemann equations are given by:

1. ∂u/∂x = ∂v/∂y
2. ∂u/∂y = -∂v/∂x

These equations state that the partial derivatives of u and v with respect to x and y must satisfy the above relationships for f(z) to be holomorphic in D.

The Cauchy-Riemann Theorem can be stated as follows:

A function f(z) = u(x, y) + iv(x, y) is holomorphic in a domain D if and only if the following conditions are satisfied:

1. The partial derivatives ∂u/∂x, ∂u/∂y, ∂v/∂x, and ∂v/∂y exist and are continuous in D.
2. The Cauchy-Riemann equations hold in D.

In other words, if a complex function satisfies the Cauchy-Riemann equations and its partial derivatives are continuous, then the function is holomorphic in its domain. Conversely, if a function is holomorphic, it must satisfy the Cauchy-Riemann equations."
167,A cylindrical tank of height 4 m and radius 1 m is filled with water. Water drains through a square hole of side 2 cm in the bottom. How long does it take for the tank to go from full to empty?,7142,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_10.json,torricelli's law,Calculus,Math,"Torricelli's Law is a principle in fluid dynamics that describes the speed at which a fluid flows out of a hole in a container under the influence of gravity. It is named after the Italian scientist Evangelista Torricelli, who derived the law in the 17th century.

The law states that the speed (v) of the fluid flowing out of the hole is proportional to the square root of the height (h) of the fluid above the hole and the acceleration due to gravity (g). Mathematically, it can be expressed as:

v = √(2gh)

Here, g is the acceleration due to gravity (approximately 9.81 m/s² on Earth).

Torricelli's Law can be derived using calculus and the principles of conservation of energy. The potential energy of the fluid at the height h is converted into kinetic energy as it flows out of the hole. By equating the potential energy and kinetic energy, and using the continuity equation (which relates the flow rate and the cross-sectional area of the hole), we can derive the expression for the speed of the fluid.

In practical applications, Torricelli's Law is used to determine the flow rate of fluids from containers with holes, such as draining a tank or measuring the flow of liquids in pipes. It is important to note that the law assumes an ideal fluid with no viscosity or air resistance, and that the hole is small compared to the size of the container. In real-world situations, these factors may affect the flow rate and need to be taken into account."
649,"Let a undirected graph G with edges E = {<0,1>,<0,2>,<0,3>,<0,5>,<2,3>,<2,4>,<4,5>}, which <A,B> represent Node A is connected to Node B. What is the shortest path from node 0 to node 5? Represent the path as a list.","[0, 5]",list of integer,NONE,self,maxku/graphtheory6-shortestpath.json,shortest path,Graph theory,EECS,"In graph theory, the Shortest Path refers to the problem of finding the path with the minimum total weight or length between two vertices (or nodes) in a graph. A graph is a mathematical structure consisting of vertices (also called nodes or points) connected by edges (also called arcs or lines). The weight or length of an edge represents the cost or distance between two vertices.

The Shortest Path problem can be solved using various algorithms, such as Dijkstra's algorithm, Bellman-Ford algorithm, or Floyd-Warshall algorithm, depending on the graph's properties (e.g., directed or undirected, weighted or unweighted, positive or negative weights).

In the context of real-world applications, the Shortest Path problem is often used in network routing, transportation planning, social network analysis, and many other fields where finding the most efficient route or connection between two points is essential."
70,"the monotone function f on [0,1] is differentiable almost everywhere. This can be proved by: (a) Fubini Theorem; (b) Tonelli Theorem; (c) Vitali Cover Theorem; (d) None of the above. Which option is correct?",(c),option,NONE,real analysis: theorem 5.2,mingyin/Vitali-cover-theorem1.json,vitali cover theorem,Real analysis,Math,"The Vitali Covering Theorem is a fundamental result in real analysis, specifically in measure theory. It is named after the Italian mathematician Giuseppe Vitali. The theorem provides a criterion for the existence of a finite or countable subcollection of sets that ""almost covers"" a given measurable set, up to a specified level of approximation. This result is particularly useful in the study of Lebesgue integration and the convergence of measurable functions.

The theorem can be stated as follows:

Let E be a Lebesgue measurable set in ℝⁿ with finite outer measure, and let 𝒞 be a collection of closed balls (or cubes) such that for every ε > 0 and every x ∈ E, there exists a ball B ∈ 𝒞 with x ∈ B and diameter(B) < ε. Then, there exists a countable disjoint subcollection {B₁, B₂, ...} of 𝒞 such that the outer measure of the difference between E and the union of these balls is arbitrarily small, i.e.,

m*(E \ (⋃ Bᵢ)) ≤ ε.

Here, m* denotes the Lebesgue outer measure.

In simpler terms, the Vitali Covering Theorem states that given a measurable set E and a collection of closed balls that ""covers"" E in the sense described above, we can find a countable disjoint subcollection of these balls such that the ""uncovered"" part of E has an arbitrarily small outer measure. This allows us to approximate the measurable set E using a countable disjoint collection of closed balls, which is a powerful tool in the study of measure theory and integration."
569,"Are the vectors [1, 2], [2, 3], and [3, 4] linearly independent?",False,bool,NONE,"text | Otto Bretscher, Linear Algebra with Applications.",elainewan/math_algebra_3_4.json,linear independence,Algebra,Math,"Linear independence is a concept in algebra, particularly in linear algebra, that refers to the relationship between vectors in a vector space. A set of vectors is said to be linearly independent if none of the vectors in the set can be expressed as a linear combination of the other vectors. In other words, no vector in the set can be created by adding or subtracting multiples of the other vectors.

Mathematically, a set of vectors {v1, v2, ..., vn} is linearly independent if the only solution to the equation:

c1 * v1 + c2 * v2 + ... + cn * vn = 0

is when all the coefficients c1, c2, ..., cn are equal to zero. Here, 0 represents the zero vector.

If there exists a non-zero solution for the coefficients, then the set of vectors is said to be linearly dependent. In this case, at least one vector can be expressed as a linear combination of the others.

Linear independence is an important concept in various areas of mathematics and engineering, as it helps determine the dimension of a vector space, the basis for a vector space, and the rank of a matrix, among other applications."
653,"Use divergence therem to evaluate $\iint_S \vec{F} \cdot d \vec{S}$ where $\vec{F} = sin(\pi x) \vec{i} + (z y^3)\vec{j} + (z^2 + 4x)\vec{k}$ and $S$ is the suface of the box with $-1 \le x \le 2, 0 \le y \le 1$ and $1 \le z \le 4$. Note that all six sides of the box are included in $S$.",67.5,float,NONE,website | https://tutorial.math.lamar.edu/Solutions/CalcIII/DivergenceTheorem/Prob2.aspx,wenhuchen/divergence3.json,divergence theorem,Calculus,Math,"The Divergence Theorem, also known as Gauss's Theorem or Ostrogradsky's Theorem, is a fundamental result in vector calculus that relates the flow of a vector field through a closed surface to the divergence of the field within the enclosed volume. It is an important tool for analyzing various physical phenomena, such as fluid flow, heat conduction, and electromagnetism.

Mathematically, the Divergence Theorem states that the net outward flux of a vector field F through a closed surface S is equal to the integral of the divergence of F over the volume V enclosed by S. Symbolically, it can be written as:

∮S F · dS = ∭V div(F) dV

Here, F is a continuously differentiable vector field defined on a three-dimensional region V with a smooth boundary S. The dot product (F · dS) represents the component of the vector field F that is perpendicular to the surface S at each point, and dS is the infinitesimal surface area element. The triple integral on the right-hand side represents the integration of the divergence of F over the entire volume V.

The Divergence Theorem has several important implications and applications in physics and engineering. For example, it can be used to derive conservation laws, such as the conservation of mass, momentum, and energy, by analyzing the flow of quantities through a closed surface. It also plays a crucial role in the study of electromagnetism, where it is used to derive Gauss's Law for electric and magnetic fields."
168,"Suppose f is an analytic function defined on $\{z \in C : IM(z) > 0\}$, the upper half plane. Given the information that f(f(z)) = z and f'(z) = 1/z^2 for every z. Find the most general possible expression of f(z). What is f(2)?",-0.5,float,NONE,website | https://haroldpboas.gitlab.io/courses/407-2017c/solution2.pdf,wenhuchen/cauchy_riemann3.json,cauchy riemann theorem,Complex analysis,Math,"The Cauchy-Riemann Theorem is a fundamental result in complex analysis that provides a set of necessary and sufficient conditions for a function to be holomorphic (i.e., complex-differentiable) in a domain. Holomorphic functions are complex functions that are differentiable at every point in their domain, and they play a central role in complex analysis.

The theorem is named after Augustin-Louis Cauchy and Bernhard Riemann, who independently developed the conditions now known as the Cauchy-Riemann equations. These equations relate the partial derivatives of the real and imaginary parts of a complex function.

Let f(z) be a complex function defined in a domain D, where z = x + iy is a complex variable with x and y being real numbers, and i is the imaginary unit (i.e., i^2 = -1). We can write f(z) as:

f(z) = u(x, y) + iv(x, y),

where u(x, y) and v(x, y) are real-valued functions representing the real and imaginary parts of f(z), respectively.

The Cauchy-Riemann equations are given by:

1. ∂u/∂x = ∂v/∂y
2. ∂u/∂y = -∂v/∂x

These equations state that the partial derivatives of u and v with respect to x and y must satisfy the above relationships for f(z) to be holomorphic in D.

The Cauchy-Riemann Theorem can be stated as follows:

A function f(z) = u(x, y) + iv(x, y) is holomorphic in a domain D if and only if the following conditions are satisfied:

1. The partial derivatives ∂u/∂x, ∂u/∂y, ∂v/∂x, and ∂v/∂y exist and are continuous in D.
2. The Cauchy-Riemann equations hold in D.

In other words, if a complex function satisfies the Cauchy-Riemann equations and its partial derivatives are continuous, then the function is holomorphic in its domain. Conversely, if a function is holomorphic, it must satisfy the Cauchy-Riemann equations."
178,"Let $A=\{n+\sum_{p=1}^{\infty} a_p 2^{-2p}: n \in \mathbf{Z}, a_p=0 or 1 \}$. What is the Lebesgue measure of A?",0.0,float,NONE,real analysis: 2.4 example 3,mingyin/Lebesgue-measure1.json,lebesgue measure,Real analysis,Math,"Lebesgue measure is a fundamental concept in real analysis and measure theory, which is a branch of mathematics that deals with the generalization of length, area, and volume. It was introduced by the French mathematician Henri Lebesgue in the early 20th century and has since become a standard tool in modern analysis.

The Lebesgue measure is an extension of the classical notion of length for intervals on the real line. It assigns a non-negative value, called the ""measure,"" to subsets of the real line (or more generally, to subsets of Euclidean spaces) in a way that is consistent with our intuitive understanding of length, area, and volume. The main idea behind the Lebesgue measure is to define the measure of a set by approximating it with simpler sets, such as intervals or rectangles, whose measures are easy to compute.

Here are some key properties of the Lebesgue measure:

1. Non-negativity: The measure of any set is always non-negative.

2. Countable additivity: If you have a countable collection of disjoint sets (i.e., sets that have no elements in common), the measure of their union is equal to the sum of their individual measures.

3. Translation invariance: The measure of a set does not change if you translate (shift) the set by a fixed amount.

4. Normalization: The measure of a closed interval [a, b] on the real line is equal to its length, i.e., b - a.

The Lebesgue measure is particularly useful because it allows us to measure sets that are too irregular or ""fractal-like"" for the classical notion of length or area to handle. For example, the Cantor set, which is a highly irregular subset of the real line, has Lebesgue measure zero, even though it is uncountably infinite.

In addition to its applications in real analysis, the Lebesgue measure plays a crucial role in probability theory, where it serves as the foundation for the concept of probability distributions on continuous sample spaces. It is also closely related to the Lebesgue integral, which is a generalization of the Riemann integral and is widely used in various branches of mathematics and physics."
445,On a day when the speed of sound is the fundamental frequency of a particular stopped organ pipe is 220 Hz. The second overtone of this pipe has the same wavelength as the third harmonic of an open pipe. How long is the open pipe? (Unit: m),0.47,float,NONE,University Physics with Modern Physics | Example 16.11,panlu/standing_sound_wave1.json,standing sound wave,Wave,Physics,"A standing sound wave, also known as a standing wave or stationary wave, is a wave pattern that occurs when two waves with the same frequency, amplitude, and wavelength travel in opposite directions and interfere with each other. This interference results in a wave pattern that appears to be stationary, as the nodes (points of no displacement) and antinodes (points of maximum displacement) remain in fixed positions.

In the context of sound waves, a standing sound wave is formed when a sound wave reflects off a surface and interferes with the incoming wave. This can occur in musical instruments, such as stringed instruments or wind instruments, where the sound waves are confined within a specific space and are reflected back and forth.

The standing sound wave has some key features:

1. Nodes: These are the points where the two interfering waves cancel each other out, resulting in no displacement of the medium (e.g., air). In a standing sound wave, the nodes remain stationary.

2. Antinodes: These are the points where the two interfering waves add up constructively, resulting in maximum displacement of the medium. Antinodes are located midway between nodes and also remain stationary in a standing sound wave.

3. Wavelength: The distance between two consecutive nodes or antinodes in a standing sound wave is half the wavelength of the original traveling waves.

4. Frequency: The frequency of a standing sound wave is the same as the frequency of the original traveling waves that created it.

Standing sound waves play a crucial role in the production of musical notes, as they determine the resonant frequencies of the instruments and contribute to their unique sounds."
72,"Calculate the Gross Domestic Product using the total expenditure approach:
Consumption Expenditures | $500 billion
Wages and salaries | $400 billion
(Gross Private) Investments Expenditures | $80 billion
Government Expenditures | $100 billion
Taxes | $70 billion
Imports | $50 billion
Exports | $30 billion
What is the GDP (in billions)?",660,integer,NONE,website | https://assessments.lumenlearning.com/assessments/6977,xueguangma/gross_domestic_product.json,gross domestic product,Economics,Finance,"Gross Domestic Product (GDP) is a key economic indicator that measures the total monetary value of all goods and services produced within a country's borders over a specific period, usually a year. It is used to assess the overall health and growth of a country's economy, as well as to compare the economic performance of different countries.

GDP can be calculated using three main approaches:

1. Production approach: This method calculates GDP by adding up the value of all goods and services produced in the economy. It involves summing the value-added at each stage of production across all industries.

2. Income approach: This method calculates GDP by adding up all the incomes earned by individuals and businesses in the economy, including wages, profits, rents, and interest.

3. Expenditure approach: This method calculates GDP by adding up all the spending on goods and services in the economy. It includes consumption, investment, government spending, and net exports (exports minus imports).

GDP is often used to measure the standard of living in a country, as it reflects the overall economic activity and wealth generation. However, it has some limitations, such as not accounting for income inequality, environmental impacts, or the value of unpaid work. Despite these limitations, GDP remains a widely used and important tool for understanding and comparing the economic performance of countries."
671,A neutron at rest decays (breaks up) to a proton and an electron. Energy is released in the decay and appears as kinetic energy of the proton and electron. The mass of a proton is 1836 times the mass of an electron. What fraction of the total energy released goes into the kinetic energy of the proton?,0.000544,float,NONE,textbook 8.101,xinyi/momentum.json,kinetics theorem,Kinetics,Physics,"Kinetic theory, also known as the kinetic theory of gases, is a scientific theorem that explains the behavior of gases based on the motion of their constituent particles, such as atoms or molecules. The main idea behind the kinetic theory is that the macroscopic properties of a gas, such as pressure, temperature, and volume, can be explained by the microscopic motion and interactions of its particles.

The key assumptions of the kinetic theory are:

1. Gases are composed of a large number of small particles (atoms or molecules) that are in constant, random motion.
2. The particles are so small compared to the distances between them that their individual volumes can be considered negligible.
3. The particles are in constant, random motion, and they collide with each other and the walls of the container. These collisions are perfectly elastic, meaning that there is no loss of kinetic energy during the collisions.
4. There are no attractive or repulsive forces between the particles, except during the brief moments of collision.
5. The average kinetic energy of the particles is directly proportional to the temperature of the gas.

Based on these assumptions, the kinetic theory can be used to derive various gas laws, such as Boyle's law, Charles's law, and the ideal gas law, which describe the relationships between pressure, volume, and temperature in a gas. The kinetic theory also provides a basis for understanding the diffusion of gases, the transport of heat in gases, and the behavior of gases in different thermodynamic processes."
557,"A company has 8 employees, including 3 managers and 5 engineers. How many different ways are there to form a team of 4 employees that includes at least 1 manager and at least 2 engineers?",60,integer,NONE,self,jianyu_xu/Binomial_1.json,binomial theorem,Combinatorics,Math,"The Binomial Theorem, in the context of combinatorics, is a powerful mathematical principle that allows us to expand expressions of the form (a + b)^n, where 'a' and 'b' are any real numbers, and 'n' is a non-negative integer. The theorem provides a systematic way to find the coefficients of the terms in the expanded form of the binomial expression.

The Binomial Theorem states that for any non-negative integer 'n' and any real numbers 'a' and 'b':

(a + b)^n = Σ [C(n, k) * a^(n-k) * b^k]

where the summation (Σ) runs from k = 0 to k = n, and C(n, k) represents the binomial coefficient, which is the number of ways to choose 'k' items from a set of 'n' items, also denoted as ""n choose k"" or C(n, k) = n! / (k! * (n-k)!), where '!' denotes the factorial function.

The binomial coefficients can also be represented using Pascal's Triangle, a triangular array of numbers where each number is the sum of the two numbers directly above it. The 'n'th row of Pascal's Triangle contains the coefficients of the binomial expansion of (a + b)^n.

In combinatorics, the Binomial Theorem is used to solve counting problems, such as finding the number of ways to arrange objects, the number of subsets of a given size, and the probability of certain outcomes in experiments.

For example, using the Binomial Theorem, we can find the expansion of (a + b)^4:

(a + b)^4 = C(4, 0) * a^4 * b^0 + C(4, 1) * a^3 * b^1 + C(4, 2) * a^2 * b^2 + C(4, 3) * a^1 * b^3 + C(4, 4) * a^0 * b^4
           = 1 * a^4 + 4 * a^3 * b + 6 * a^2 * b^2 + 4 * a * b^3 + 1 * b^4"
342,"Let a undirected graph G with edges E = {<2,1>,<2,0>,<2,3>,<1,4>,<4,3>}, which <A,B> represent Node A is connected to Node B. What is the minimum vertex cover of G? Represent the vertex cover in a list of ascending order.","[2, 4]",list of integer,NONE,self,maxku/graphtheory1-vertexcover.json,vertex cover,Graph theory,EECS,"Vertex Cover in graph theory is a set of vertices in a graph such that each edge of the graph is incident to at least one vertex in the set. In other words, a vertex cover is a subset of vertices that ""covers"" all the edges, meaning that every edge has at least one endpoint in the vertex cover.

The Vertex Cover problem is an optimization problem that aims to find the smallest possible vertex cover in a given graph. This problem is known to be NP-complete, which means that finding an optimal solution can be computationally challenging for large graphs.

For example, consider a graph with four vertices (A, B, C, and D) and four edges (AB, BC, CD, and DA). A possible vertex cover for this graph would be the set {A, C}, as each edge has at least one endpoint in the vertex cover. Another possible vertex cover would be {B, D}. In this case, the minimum vertex cover has a size of 2.

In practical applications, vertex cover problems can be used to model various real-world scenarios, such as network security, resource allocation, and scheduling tasks."
269,"A load dissipates 1.5kW of power in an ac series RC circuit. Given that the power factor is 0.75, what is its reactive power $(P_r)$? What is its apparent power $(P_a)$? Represent the answer in a list [$P_r, P_a$] with unit kVA and kVAR respectively.","[2.0, 1.32]",list of float,NONE,self,maxku/basic-electronics-6-3.json,rc circuit,Electromagnetism,Physics,"An RC circuit, also known as a resistor-capacitor circuit, is a simple electrical circuit that consists of a resistor (R) and a capacitor (C) connected in series or parallel. These circuits are widely used in various electronic applications, such as filters, timers, and integrators.

In an RC circuit, the resistor and capacitor work together to control the flow of electric current and the storage of electrical energy. The resistor controls the rate at which the current flows through the circuit, while the capacitor stores electrical energy and releases it when needed.

When a voltage is applied to an RC circuit, the capacitor starts charging, and the voltage across the capacitor increases. The time it takes for the capacitor to charge depends on the resistance and capacitance values in the circuit. This time constant (τ) is given by the product of the resistance (R) and capacitance (C) values: τ = RC.

During the charging process, the current flowing through the resistor decreases as the capacitor charges, and eventually, the current becomes zero when the capacitor is fully charged. When the voltage source is removed, the capacitor starts discharging through the resistor, and the voltage across the capacitor decreases.

In the context of electromagnetism, RC circuits can be used to filter out specific frequencies in a signal. For example, a low-pass filter allows low-frequency signals to pass through while attenuating high-frequency signals. This is achieved by selecting appropriate resistor and capacitor values that determine the cutoff frequency of the filter.

In summary, an RC circuit is a fundamental electrical circuit that combines a resistor and a capacitor to control the flow of electric current and the storage of electrical energy. It has various applications in electronic systems, including filtering, timing, and integration."
541,Roughly how many bits are required on the average to describe to 3 digit accuracy the decay time (in years) of a radium atom if the half-life of radium is 80 years? Note that half-life is the median of the distribution.,19,integer,NONE,textbook 8.4,xinyi/differential_entropy.json,differential entropy,Information theory,EECS,"Differential entropy, also known as continuous entropy, is a concept in information theory that extends the idea of entropy from discrete random variables to continuous random variables. Entropy, in general, is a measure of the uncertainty or randomness associated with a random variable. In the context of information theory, it quantifies the average amount of information required to describe the outcome of a random variable.

For discrete random variables, entropy is well-defined using the Shannon entropy formula, which sums the product of the probability of each outcome and the logarithm of its reciprocal probability. However, for continuous random variables, the probability of any specific outcome is zero, making the Shannon entropy formula inapplicable.

Differential entropy addresses this issue by considering the probability density function (pdf) of a continuous random variable instead of the probabilities of individual outcomes. The differential entropy H(X) of a continuous random variable X with a probability density function f(x) is defined as:

H(X) = - ∫ f(x) * log(f(x)) dx

where the integral is taken over the entire range of the random variable X, and log is the logarithm base 2 (or any other base, depending on the desired unit of measurement for entropy).

Differential entropy can be interpreted as the average amount of information required to describe the outcome of a continuous random variable with a given probability density function. However, unlike the entropy of discrete random variables, differential entropy can be negative, which occurs when the probability density function is highly concentrated around certain values.

It is important to note that differential entropy is not a direct extension of discrete entropy, and some properties of discrete entropy do not hold for differential entropy. For example, differential entropy is not invariant under changes of variables or coordinate transformations, whereas discrete entropy is invariant under permutations of the outcomes."
728,In how many ways can a group of 7 people be divided into 2 non-empty subsets?,63,integer,NONE,self,jianyu_xu/Stirling_number_second_kind_1.json,stirling number of the second kind,Combinatorics,Math,"Stirling Numbers of the second kind, denoted as S(n, k), are used in combinatorics to count the number of ways to partition a set of n elements into k non-empty subsets. In other words, they represent the number of ways to distribute n distinct items into k distinct groups, where each group has at least one item.

The Stirling Numbers of the second kind can be defined recursively using the following formula:

S(n, k) = k * S(n-1, k) + S(n-1, k-1)

with the base cases:

S(n, 0) = 0 if n > 0,
S(0, 0) = 1,
S(n, k) = 0 if k > n.

The first term, k * S(n-1, k), represents the case where the nth element is added to one of the existing k subsets. The second term, S(n-1, k-1), represents the case where the nth element forms a new subset by itself.

Here are some examples of Stirling Numbers of the second kind:

S(3, 2) = 3: There are three ways to partition a set of 3 elements into 2 non-empty subsets: {1, 2}, {3}; {1, 3}, {2}; {1}, {2, 3}.
S(4, 2) = 7: There are seven ways to partition a set of 4 elements into 2 non-empty subsets: {1, 2, 3}, {4}; {1, 2, 4}, {3}; {1, 3, 4}, {2}; {2, 3, 4}, {1}; {1, 2}, {3, 4}; {1, 3}, {2, 4}; {1, 4}, {2, 3}.

Stirling Numbers of the second kind have various applications in combinatorics, including counting the number of permutations with a given number of cycles, counting the number of functions from one set to another, and solving problems related to set partitions and groupings."
66,"The two-step Adams-Bashforth method of approximation uses the approximation scheme $y_{i+2}=y_{i+1} - 1/2 * hf(t_i,y_i)+ 3/2 * hf(t_{i+1},y_{i+1})$. Given that y(0)=1 and y(1)=2, use the Adams-Bashforth method to approximate y(3) for y=-y^2 with a step size of h=1.",-19.875,float,NONE,website | https://www.varsitytutors.com/differential_equations-help/numerical-solutions-of-ordinary-differential-equations,wenhuchen/Adams-Bashforth1.json,adams-bashforth method,Numerical analysis,Math,"The Adams-Bashforth method is a family of explicit numerical methods used for solving ordinary differential equations (ODEs) with initial value problems. These methods are part of a broader class of techniques called linear multistep methods, which use information from previous steps to compute the solution at the current step.

The general form of an ODE is:

dy/dt = f(t, y(t))

where y(t) is the unknown function we want to approximate, and f(t, y(t)) is a given function that describes the rate of change of y with respect to t.

The Adams-Bashforth methods are based on the idea of approximating the integral of the rate function f(t, y(t)) over a small time interval [t_n, t_(n+1)] using polynomial interpolation. The methods use the values of f(t, y(t)) at previous time steps to construct a polynomial that approximates f(t, y(t)) over the interval, and then integrate this polynomial to obtain an estimate for y(t_(n+1)).

The order of the Adams-Bashforth method depends on the number of previous time steps used in the polynomial interpolation. For example, the first-order Adams-Bashforth method (also known as the forward Euler method) uses only the most recent time step:

y(t_(n+1)) ≈ y(t_n) + h * f(t_n, y(t_n))

where h is the time step size.

The second-order Adams-Bashforth method uses the two most recent time steps:

y(t_(n+1)) ≈ y(t_n) + h * (3/2 * f(t_n, y(t_n)) - 1/2 * f(t_(n-1), y(t_(n-1))))

Higher-order Adams-Bashforth methods can be derived similarly by including more previous time steps in the polynomial interpolation.

The main advantage of Adams-Bashforth methods is that they can achieve high accuracy with relatively low computational cost, especially for higher-order methods. However, being explicit methods, they may suffer from stability issues for stiff ODEs, which may require the use of implicit methods like the Adams-Moulton methods."
577,"A hospital has a 3.0 x 10^14 Bq Co-60 source for cancer therapy. The rate of gamma rays incident on a patient of area 0.30 m^2 located 4.0 m from the source is $X*10^11$ Bq, what is X? Co-60 emits a 1.1- and a 1.3-MeV gamma ray for each disintegration.",8.95,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Exercise 13.51,tonyxia/nuclear6.json,nuclear physics,Atomic physics,Physics,"Nuclear physics, also known as atomic physics, is a branch of physics that deals with the study of atomic nuclei and their interactions. It focuses on understanding the properties, behavior, and structure of atomic nuclei, as well as the forces that hold protons and neutrons together within the nucleus.

The key components of nuclear physics include:

1. Nuclear structure: This involves the study of the arrangement of protons and neutrons within the nucleus, as well as the energy levels and quantum states of these particles. Nuclear structure also explores the various models that describe the nucleus, such as the shell model and the liquid drop model.

2. Nuclear reactions: These are processes in which atomic nuclei undergo changes, such as fusion (combining of nuclei), fission (splitting of nuclei), and radioactive decay (spontaneous transformation of a nucleus into another). Nuclear reactions are responsible for the release of energy in nuclear power plants and the functioning of nuclear weapons.

3. Nuclear forces: The strong nuclear force, also known as the strong interaction, is the force that holds protons and neutrons together within the nucleus. It is one of the four fundamental forces of nature and is responsible for the stability of atomic nuclei. Nuclear forces also include the weak nuclear force, which is responsible for certain types of radioactive decay.

4. Radioactivity: This is the spontaneous emission of particles or electromagnetic radiation from unstable atomic nuclei. There are several types of radioactive decay, including alpha decay, beta decay, and gamma decay. Radioactivity plays a crucial role in various applications, such as medical imaging, cancer treatment, and dating of archaeological artifacts.

5. Particle physics: Nuclear physics overlaps with particle physics, which studies the fundamental particles that make up the universe and their interactions. This includes the study of quarks, which are the building blocks of protons and neutrons, as well as other subatomic particles like neutrinos and mesons.

Overall, nuclear physics is a vital field of study that has contributed significantly to our understanding of the universe and has numerous practical applications in energy production, medicine, and technology."
265,Calculate the de Broglie Wavelength of a tennis ball of mass 57 g traveling 25 m/s in meters.,4.7e-34,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Example 5.2,tonyxia/wave2.json,wave theorem,Wave,Physics,"Wave Theorem, also known as the Wave Equation, is a fundamental concept in physics that describes the behavior of waves, such as sound waves, light waves, and water waves. It is a partial differential equation that relates the wave's displacement at a given point in space and time to the properties of the medium through which the wave is propagating.

The general form of the wave equation is:

∂²ψ/∂t² = c² ∇²ψ

Here, ψ represents the wave's displacement, t is time, c is the wave's speed, and ∇² is the Laplacian operator, which represents the spatial derivatives of the wave's displacement. The equation states that the acceleration of the wave's displacement with respect to time (∂²ψ/∂t²) is proportional to the spatial curvature of the wave (∇²ψ) multiplied by the square of the wave's speed (c²).

The wave equation is essential in understanding various phenomena in physics, such as the propagation of sound in air, the behavior of electromagnetic waves, and the motion of waves on a string or in a fluid. It helps predict the behavior of waves under different conditions and is widely used in engineering, acoustics, optics, and other fields."
648,"Mr. Jackson bought his house in 1995, and financed the loan for 30 years at an interest rate of 7.8%. His monthly payment was $1260. In 2015, Mr. Jackson decides to pay off the loan. Find the balance of the loan he still owes.",104761.48,float,NONE,website | https://math.libretexts.org/Bookshelves/Applied_Mathematics/Applied_Finite_Mathematics_(Sekhon_and_Bloom)/06%3A_Mathematics_of_Finance/6.05%3A_Miscellaneous_Application_Problems,xueguangma/outstanding_balance_of_loan.json,outstanding balance of loan,Fixed income,Finance,"Outstanding Balance of Loan (Fixed Income) refers to the remaining unpaid principal amount on a loan or fixed-income security, such as a bond or mortgage, at any given point in time. It is the amount that the borrower still owes to the lender, excluding any interest or fees. As the borrower makes regular payments, the outstanding balance decreases over time until it is fully paid off.

In the context of fixed-income securities, the outstanding balance represents the portion of the principal that has not yet been repaid to the bondholders or investors. This balance is important for both borrowers and investors, as it helps them track the progress of loan repayment and assess the credit risk associated with the loan.

For borrowers, the outstanding balance is crucial for managing their debt and understanding their financial obligations. For investors, the outstanding balance helps them evaluate the creditworthiness of the borrower and the likelihood of receiving their principal and interest payments on time."
524,"A surveyor uses a steel measuring tape that is exactly 50.000 m long at a temperature of 20°C. The markings on the tape are calibrated for this temperature. When it is 35°C, the surveyor uses the tape to measure a distance. The value that she reads off the tape is 35.794 m. What is the actual distance? (Unit: m)",35.8,float,NONE,University Physics with Modern Physics | Example 17.2,panlu/linear_expansion1.json,linear expansion,Thermodynamics,Physics,"Linear expansion is a concept in thermodynamics that refers to the change in length of a solid material when it is subjected to a change in temperature. When a solid is heated, the kinetic energy of its atoms or molecules increases, causing them to vibrate more vigorously. This increased vibration leads to an increase in the average distance between the particles, resulting in an expansion of the material.

In the case of linear expansion, we are specifically concerned with the change in length of a one-dimensional object, such as a rod or a wire, when its temperature changes. The extent of linear expansion depends on three factors: the initial length of the object, the change in temperature, and the material's coefficient of linear expansion.

The coefficient of linear expansion (α) is a property of the material that quantifies how much it expands or contracts per unit length for a given change in temperature. It is usually expressed in units of (1/°C) or (1/K).

The formula for linear expansion is given by:

ΔL = L₀ × α × ΔT

where:
- ΔL is the change in length of the object
- L₀ is the initial length of the object
- α is the coefficient of linear expansion of the material
- ΔT is the change in temperature

It is important to note that linear expansion is generally an approximation that holds true for small temperature changes. For larger temperature changes, the expansion behavior may become non-linear, and more complex models may be required to accurately describe the material's behavior."
539,"Let a undirected graph G with edges E = {<0,2>, <2,4>, <3,4>, <1,4>}, which <A,B> represent Node A is connected to Node B. What is the minimum vertex cover of G if 0 is one of vertex cover? Represent the vertex cover in a list of ascending order.","[0, 4]",list of integer,NONE,self,maxku/graphtheory3-vertexcover.json,vertex cover,Graph theory,EECS,"Vertex Cover in graph theory is a set of vertices in a graph such that each edge of the graph is incident to at least one vertex in the set. In other words, a vertex cover is a subset of vertices that ""covers"" all the edges, meaning that every edge has at least one endpoint in the vertex cover.

The Vertex Cover problem is an optimization problem that aims to find the smallest possible vertex cover in a given graph. This problem is known to be NP-complete, which means that finding an optimal solution can be computationally challenging for large graphs.

For example, consider a graph with four vertices (A, B, C, and D) and four edges (AB, BC, CD, and DA). A possible vertex cover for this graph would be the set {A, C}, as each edge has at least one endpoint in the vertex cover. Another possible vertex cover would be {B, D}. In this case, the minimum vertex cover has a size of 2.

In practical applications, vertex cover problems can be used to model various real-world scenarios, such as network security, resource allocation, and scheduling tasks."
681,"Let C[0,1] be all the continuous function on in the interval [0,1]. For the integral equation $x(t)-\lambda \int_0^1 e^{t-s} x(s) ds=y(t)$, where $y(t)\in C[0,1]$ is a given function. \lambda is a constant and |\lambda|<1. Then there exists a unique solution x(t)\in C[0,1]. This conclusion can be proved by: 1. Implicit function theorem, 2. Riesz representation theorem, 3. Banach fixed point theorem, 4. None of the above. Return the number as the answer.",3.0,float,NONE,functional analysis: theorem 1.1.11,mingyin/banach-fixed-point-theorem1.json,banach fixed point theorem,Mathematical analysis,Math,"Banach Fixed Point Theorem, also known as the Contraction Mapping Principle, is a fundamental result in mathematical analysis that guarantees the existence and uniqueness of fixed points for certain types of mappings, specifically contraction mappings, in complete metric spaces. It has important applications in various fields, including differential equations, optimization, and game theory.

Let's break down the main components of the theorem:

1. Complete metric space: A metric space is a set equipped with a distance function that satisfies certain properties, such as non-negativity, symmetry, and the triangle inequality. A complete metric space is a metric space in which every Cauchy sequence (a sequence where the distance between its elements becomes arbitrarily small as the sequence progresses) converges to a limit within the space.

2. Contraction mapping: A contraction mapping (or contraction) is a function that maps a metric space into itself and satisfies a ""contracting"" property, meaning that the distance between any two points in the space is strictly reduced after applying the mapping. Formally, a function f is a contraction mapping if there exists a constant 0 ≤ k < 1 such that for any two points x and y in the space, the distance between f(x) and f(y) is at most k times the distance between x and y.

Banach Fixed Point Theorem states that:

If (X, d) is a complete metric space and f: X → X is a contraction mapping, then there exists a unique fixed point x* in X such that f(x*) = x*.

In other words, the theorem asserts that for a contraction mapping on a complete metric space, there is a unique point in the space that remains unchanged under the mapping. Moreover, the theorem provides an iterative method to approximate the fixed point: starting from any initial point x0, the sequence of iterates x1 = f(x0), x2 = f(x1), x3 = f(x2), ... converges to the fixed point x*.

The Banach Fixed Point Theorem is a powerful tool in mathematical analysis, as it not only guarantees the existence and uniqueness of fixed points but also provides a practical method for finding them."
370,How many ways are there to arrange the letters in the word *BANANA* up to the symmetries of the word?,30,integer,NONE,self,jianyu_xu/Burnside_3.json,burnside's lemma,Combinatorics,Math,"Burnside's Lemma, also known as the Cauchy-Frobenius Lemma or the Orbit-Counting Theorem, is a fundamental result in combinatorics that deals with counting the number of distinct elements in a set under the action of a group. It is particularly useful in counting problems involving symmetries and permutations.

The lemma is named after the British mathematician William Burnside, who contributed significantly to the development of group theory.

Statement of Burnside's Lemma:

Let G be a finite group that acts on a finite set X. Then the number of distinct orbits of X under the action of G is given by:

(1/|G|) * Σ |Fix(g)|

where |G| is the order of the group (i.e., the number of elements in G), the sum is taken over all elements g in G, and |Fix(g)| is the number of elements in X that are fixed by the action of g (i.e., the number of elements x in X such that g(x) = x).

In simpler terms, Burnside's Lemma states that the number of distinct orbits (or equivalence classes) in a set under the action of a group can be found by averaging the number of fixed points of each group element.

Burnside's Lemma is often used in combinatorial problems where we need to count the number of distinct configurations of an object, taking into account its symmetries. By applying the lemma, we can avoid overcounting configurations that are equivalent under a given symmetry operation."
410,Consider the 7.0-TeV protons that are produced in the LHC collider at CERN. Find the available center-of-mass energy if these protons collide with other protons in a fixed-target experiment in GeV.,114.5,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Exercise 14.55,tonyxia/particle6.json,particle,Particle,Physics,"Particle (Particle) is a bit ambiguous, but I assume you are referring to a particle in the context of physics. 

A particle is a small, localized object that can be described as having a mass and other physical properties. In physics, particles are the basic building blocks of matter and can exist in various forms, such as elementary particles, composite particles, and virtual particles. 

1. Elementary particles: These are the most fundamental particles that cannot be broken down into smaller constituents. They include quarks, leptons (such as electrons), and gauge bosons (such as photons). Elementary particles are the building blocks of all matter and are responsible for the fundamental forces in the universe.

2. Composite particles: These are particles made up of two or more elementary particles. Examples include protons and neutrons, which are composed of quarks held together by the strong nuclear force.

3. Virtual particles: These are temporary particles that exist for very short periods of time and are involved in the mediation of fundamental forces. They are not directly observable but play a crucial role in understanding the behavior of other particles.

Particles can also be classified as fermions or bosons, depending on their quantum properties. Fermions, such as electrons and quarks, follow the Pauli Exclusion Principle, which states that no two fermions can occupy the same quantum state simultaneously. Bosons, such as photons and gluons, do not follow this principle and can occupy the same quantum state.

In summary, a particle is a small, localized object with mass and other physical properties, which serves as the basic building block of matter and is responsible for the fundamental forces in the universe."
558,The open mapping theorem can be proved by (a) Baire category theorem; (b) Cauchy integral theorem; (c) random graph theorem; (d) None of the above. Which option is correct?,(a),option,NONE,wiki,mingyin/baire-category-theorem1.json,baire category theorem,Functional analysis,Math,"The Baire Category Theorem is a fundamental result in functional analysis, a branch of mathematics that deals with the study of function spaces and linear operators between them. The theorem is named after the French mathematician René-Louis Baire, who first stated it in 1899. The Baire Category Theorem has important implications in various areas of mathematics, including topology, measure theory, and the theory of Banach spaces.

The theorem can be stated in several equivalent forms, but one of the most common versions is as follows:

Baire Category Theorem: Let X be a complete metric space. Then, X cannot be expressed as a countable union of nowhere dense sets.

Here, a set is called nowhere dense if its closure has an empty interior, meaning that it does not contain any open sets. A complete metric space is a space in which every Cauchy sequence converges to a limit within the space.

The Baire Category Theorem essentially states that ""large"" spaces, such as complete metric spaces, cannot be ""built up"" from ""small"" pieces, like nowhere dense sets. This result has several important consequences in functional analysis, including the following:

1. The Banach-Steinhaus Theorem (Uniform Boundedness Principle): If a family of continuous linear operators between two Banach spaces is pointwise bounded, then it is uniformly bounded.

2. The Open Mapping Theorem: A continuous linear operator between two Banach spaces is an open map if and only if it is surjective.

3. The Closed Graph Theorem: A linear operator between two Banach spaces has a closed graph if and only if it is continuous.

These theorems are cornerstones of functional analysis and have numerous applications in various branches of mathematics. The Baire Category Theorem is not only a powerful tool in functional analysis but also a foundational result in the study of topological spaces and their properties."
350,"Let M be the set of bounded functions (i.e. \sup_{x\in[a,b]}|f(x)|<\infty) in C[0,1]. Is the set ${F(x)=\int_0^x f(t) dt | f \in M }$ a sequentially compact set? Answer 1 for yes and 0 for no. Furthermore, it can be proved using 1. Arzelà-Ascoli theorem, 2. Riesz representation theorem, 3. Banach fixed point theorem, 4. None of the above. Return the answers of the two questions in a list. For example, if you think the answer is no and Riesz representation theorem, then return [0,2].","[1, 1]",list of integer,NONE,functional analysis: exercise 1.3.5,mingyin/Arzela-Ascoli-theorem1.json,arzelà-ascoli theorem,Functional analysis,Math,"The Arzelà-Ascoli theorem is a fundamental result in functional analysis, specifically in the study of sequences of functions. It provides a set of criteria for determining when a sequence of functions has a uniformly convergent subsequence. The theorem is named after Italian mathematicians Cesare Arzelà and Giulio Ascoli.

The theorem is typically stated in the context of continuous functions defined on a compact metric space. Here's the statement of the theorem:

Let X be a compact metric space, and let F be a family of continuous functions from X to the real numbers R. Then F is relatively compact in the space of continuous functions C(X, R) equipped with the uniform topology (i.e., F has a compact closure) if and only if the following two conditions hold:

1. Equicontinuity: For every x in X and every ε > 0, there exists a neighborhood U of x such that for all f in F and all y in U, |f(y) - f(x)| < ε.

2. Pointwise boundedness: For every x in X, the set {f(x) : f in F} is bounded in R.

In simpler terms, the Arzelà-Ascoli theorem states that a family of continuous functions on a compact metric space has a uniformly convergent subsequence if and only if the family is equicontinuous and pointwise bounded.

The importance of the Arzelà-Ascoli theorem lies in its ability to provide a powerful tool for understanding the behavior of sequences of functions. It is widely used in various areas of mathematics, including the study of differential equations, approximation theory, and the analysis of dynamical systems."
456,Given the following equation: x - e^{-x} = 0. determine the initial approximations for finding the smallest positive root. Use these to find the root correct to three decimal places with Regula-Falsi method.,0.567,float,NONE,website | http://ndl.ethernet.edu.et/bitstream/123456789/79528/3/Numerical_Methods_Problems_and_Solutions_cropped.pdf,wenhuchen/Regula-Falsi.json,regula-falsi algorithm,Numerical analysis,Math,"The Regula-Falsi Algorithm, also known as the False Position Method, is a numerical analysis technique used to find the root of a continuous function within a given interval. It is an iterative method that combines aspects of both the Bisection Method and the Secant Method to approximate the root more efficiently.

The algorithm works as follows:

1. Start with a continuous function f(x) and an interval [a, b] such that f(a) and f(b) have opposite signs, i.e., f(a) * f(b) < 0. This ensures that there is at least one root within the interval according to the Intermediate Value Theorem.

2. Calculate the point c, which is the intersection of the secant line passing through the points (a, f(a)) and (b, f(b)), using the formula:

   c = a - f(a) * (b - a) / (f(b) - f(a))

3. Evaluate the function at point c, i.e., calculate f(c).

4. Check if f(c) is close enough to zero (within a specified tolerance) or if the maximum number of iterations has been reached. If either condition is met, the algorithm stops, and c is considered as the approximate root.

5. If f(c) is not close enough to zero, update the interval [a, b] as follows:
   - If f(a) * f(c) < 0, then the root lies in the interval [a, c], so update b = c.
   - If f(a) * f(c) > 0, then the root lies in the interval [c, b], so update a = c.

6. Repeat steps 2-5 until the stopping criteria are met.

The Regula-Falsi Algorithm converges faster than the Bisection Method because it uses the secant line's slope to approximate the root, which generally provides a better estimate. However, it may converge slower than the Secant Method or Newton's Method in some cases. The algorithm is guaranteed to converge to a root if the function is continuous and has a root within the given interval."
362,which n <= 20 can be constructed a regular n-gonwith compass and straightedge? return all the possible numbers in a list,"[3, 4, 5, 6, 8, 10, 12, 15, 16, 17, 20]",list of integer,NONE,Constructible polygon | https://en.wikipedia.org/wiki/Constructible_polygon,mingyin/Gauss–Wantzel_theorem1.json,gauss-wantzel theorem,Geometry,Math,"The Gauss-Wantzel theorem, also known as the Gauss-Wantzel-Steiner theorem, is a result in geometry that provides a necessary and sufficient condition for a polygon to be constructible using only a compass and straightedge. This theorem is named after the mathematicians Carl Friedrich Gauss, Johann Jakob Steiner, and Pierre Wantzel.

The theorem states that a regular polygon with n sides can be constructed using only a compass and straightedge if and only if n is the product of a power of 2 and any number of distinct Fermat primes. A Fermat prime is a prime number that can be expressed in the form 2^(2^k) + 1, where k is a non-negative integer.

For example, a regular polygon with 3, 4, 5, 6, 8, 10, 12, 15, 16, 17, 20, 24, 30, 32, 34, 40, 48, 51, 60, 64, 68, 80, 85, 96, 102, 120, 128, 136, 160, 170, 192, 204, 240, 255, 256, 257, 272, 320, 340, 384, 408, 480, 510, 512, 514, 544, 640, 680, 768, 816, 960, 1020, 1024, 1028, 1088, 1280, 1360, 1536, 1632, 1920, 2040, 2048, 2056, 2176, 2560, 2720, 3072, 3264, 3840, 4080, 4096, 4112, 4352, 5120, 5440, 6144, 6528, 7680, 8160, 8192, 8224, 8704, 10240, 10880, 12288, 13056, 15360, 16320, 16384, 16448, 17408, 20480, 21760, 24576, 26112, 30720, 32640, 32768, 32896, 34816, 40960, 43520, 49152, 52224, 61440, 65280, 65536, 65692, 69632, 81920, 87040, 98304, 104448, 122880, 130560, 131072, 131184, 139264, 163840, 174080, 196608, 208896, 245760, 261120, 262144, 262288, 278528, 327680, 348160, 393216, 417792, 491520, 522240, 524288, 524576, 557056, 655360, 696320, 786432, 835584, 983040, 1044480, 1048576, 1049088, 1114112, 1310720, 1392640, 1572864, 1671168, 1966080, 2088960, 2097152, 2098176, 2228224, 2621440, 2785280, 3145728, 3342336, 3932160, 4177920, 4194304, 4196352, 4456448, 5242880, 5570560, 6291456, 6684672, 7864320, 8355840, 8388608, 8392704, 8912896, 10485760, 11141120, 12582912, 13369344, 15728640, 16711680, 16777216, 16785408, 17825792, 20971520, 22282240, 25165824, 26738688, 31457280, 33423360, 33554432, 33570816, 35651584, 41943040, 44564480, 50331648, 53477376, 62914560, 66846720, 67108864, 67141632, 71303168, 83886080, 89128960, 100663296, 106954752, 125829120, 133693440, 134217728, 134283264, 142606336, 167772160, 178257920, 201326592, 213909504, 251658240, "
224,What is $\lim _{r \rightarrow \infty} (\int_0^{\pi/2} x^r sin(x) dx)/(r\int_0^{\pi/2} x^r cos(x) dx)$?,0.63662,float,NONE,,mingyin/Limit-of-sequence5.json,limiting theorem,Mathematical analysis,Math,"In mathematical analysis, a limiting theorem refers to a result that describes the behavior of a sequence, function, or series as it approaches a specific value or point. These theorems are fundamental in understanding the properties of mathematical objects and their convergence or divergence. There are several important limiting theorems in mathematical analysis, including:

1. Limit of a sequence: A sequence is a list of numbers arranged in a specific order. The limit of a sequence is the value that the terms of the sequence approach as the index goes to infinity. If the limit exists, the sequence is said to be convergent; otherwise, it is divergent.

2. Limit of a function: The limit of a function is the value that the function approaches as its input approaches a specific value. Limits are used to define continuity, derivatives, and integrals, which are essential concepts in calculus.

3. Squeeze theorem: Also known as the sandwich theorem or the pinching theorem, this theorem states that if a function is ""squeezed"" between two other functions that have the same limit at a specific point, then the squeezed function must also have the same limit at that point.

4. Monotone convergence theorem: This theorem states that a monotone (either non-decreasing or non-increasing) and bounded sequence always converges to a limit.

5. Bolzano-Weierstrass theorem: This theorem states that every bounded sequence has a convergent subsequence, which is a sequence formed by selecting terms from the original sequence while preserving their order.

6. Dominated convergence theorem: This theorem provides a condition under which the limit of an integral can be interchanged with the integral of a limit. It is particularly useful in the study of Lebesgue integration.

7. Central limit theorem: In probability theory and statistics, the central limit theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the shape of the original distribution.

These limiting theorems play a crucial role in various branches of mathematics, including calculus, real analysis, complex analysis, and probability theory. They help us understand the behavior of mathematical objects and provide a foundation for further study and applications."
409,"compute the line integral of \int_K xy dx, \int_L xy dx, where K is a straight line from (0,0) to (1,1) and L is the Parabola y=x^2 from (0,0) to (1,1). return the answer as a list","[0.333, 0.25]",list of float,NONE,mathematical analysis 2; 17.2 example 1,mingyin/double-integral4.json,line integral theorem,Calculus,Math,"The Line Integral Theorem, also known as the Fundamental Theorem for Line Integrals, is a fundamental result in vector calculus that relates the line integral of a vector field along a curve to the value of a potential function at the endpoints of the curve. It is used to evaluate line integrals of conservative vector fields and to determine if a vector field is conservative.

The theorem states that if a vector field F is conservative, meaning it has a potential function f (i.e., F = ∇f, where ∇ is the gradient operator), then the line integral of F along a curve C with endpoints A and B is equal to the difference in the potential function's values at these endpoints:

∫(C) F · dr = f(B) - f(A)

Here, F · dr represents the dot product of the vector field F and the differential displacement vector dr along the curve C.

The Line Integral Theorem has several important implications:

1. If a vector field is conservative, the line integral is path-independent, meaning the value of the integral depends only on the endpoints A and B, not on the specific path taken between them.

2. For a conservative vector field, the line integral around a closed curve (where the initial and final points are the same) is always zero.

3. The theorem provides a method for evaluating line integrals of conservative vector fields by finding the potential function and computing the difference in its values at the endpoints of the curve.

In summary, the Line Integral Theorem is a powerful tool in vector calculus that connects the concepts of line integrals, conservative vector fields, and potential functions, allowing for more efficient evaluation of line integrals and analysis of vector fields."
147,"A cascade of $n$ identical independent binary symmetric channels each with raw error probability $p$, and $0<p<1$. What is the capacity of the cascade when $n$ goes to infinity?",0.0,float,NONE,textbook 7.7,xinyi/binary_symmetric_channel_1.json,binary symmetric channel,Information theory,EECS,"A Binary Symmetric Channel (BSC) is a fundamental concept in information theory that represents a communication channel model for transmitting binary data (0s and 1s) between a sender and a receiver. The term ""symmetric"" refers to the fact that the probability of error is the same for both binary values (0 and 1).

In a BSC, there are two possible input values (0 and 1) and two possible output values (0 and 1). The channel is characterized by a single parameter, the crossover probability (p), which represents the probability that a transmitted bit is flipped (i.e., changed from 0 to 1 or from 1 to 0) during transmission. The probability of a bit being transmitted correctly is (1-p).

The BSC model assumes that the errors in the channel are independent and identically distributed, meaning that the probability of an error occurring at any given bit position is the same and does not depend on the values of other bits.

In summary, a Binary Symmetric Channel is a simple model used in information theory to study the transmission of binary data over a noisy communication channel. It is characterized by a single parameter, the crossover probability (p), which represents the probability of a bit being flipped during transmission."
399,"Astrophysical theory suggests that a burned-out star whose mass is at least three solar masses will collapse under its own gravity to form a black hole. If it does, the radius of its event horizon is X * 10^3 m, what is X?",8.9,float,NONE,University Physics with Modern Physics | Example 13.11,panlu/black_hole1.json,black hole,Celestial mechanics,Physics,"A black hole is a celestial object with an extremely strong gravitational force, resulting from the collapse of a massive star. In celestial mechanics, black holes are considered the final stage in the evolution of massive stars, after they have exhausted their nuclear fuel and undergone a supernova explosion.

The gravitational pull of a black hole is so strong that nothing, not even light, can escape it once it crosses the event horizon, which is the boundary surrounding the black hole. This is why black holes are called ""black"" – they do not emit or reflect any light, making them invisible to traditional observation methods.

Black holes can be described by three main properties: mass, charge, and angular momentum (or spin). The mass of a black hole determines its size and gravitational strength. The charge, if any, affects the electromagnetic interactions with surrounding matter. The angular momentum is related to the rotation of the black hole.

There are three main types of black holes:

1. Stellar black holes: These are formed by the gravitational collapse of massive stars, typically with masses between 3 and 20 times that of the Sun. They are the most common type of black holes in the universe.

2. Supermassive black holes: These are much larger than stellar black holes, with masses ranging from millions to billions of times the mass of the Sun. They are believed to reside at the centers of most galaxies, including our own Milky Way.

3. Intermediate-mass black holes: These black holes have masses between those of stellar and supermassive black holes. Their existence is still a subject of debate among astronomers, as they are difficult to detect and their formation mechanisms are not well understood.

Black holes play a crucial role in celestial mechanics, as they can influence the orbits of nearby stars and gas, trigger the formation of new stars, and even merge with other black holes to create gravitational waves, which are ripples in the fabric of spacetime."
281,"Find the absolute minimum value of the function $f(x,y)=x^2+y^2$ subject to the constraint $x^2+2*y^2=1$.",0.5,float,NONE,website | https://www.varsitytutors.com/gre_subject_test_math-help/lagrange-s-theorem,wenhuchen/Lagrange's_multiplier2.json,lagrange's multiplier,Mathematical analysis,Math,"Lagrange's multiplier is a mathematical method used in optimization problems to find the local maxima and minima of a function subject to equality constraints. It is named after the French mathematician Joseph-Louis Lagrange.

The method involves introducing a new variable, called the Lagrange multiplier (usually denoted by λ), to transform the constrained optimization problem into an unconstrained one. The basic idea is to convert the constraint equation into a new function that can be added to the original function, and then find the critical points of this new function.

Suppose we have a function f(x, y) that we want to optimize (maximize or minimize) subject to a constraint g(x, y) = c, where x and y are variables, and c is a constant. The method of Lagrange multipliers states that the gradient of f(x, y) must be parallel to the gradient of g(x, y) at the optimal point. Mathematically, this can be expressed as:

∇f(x, y) = λ ∇g(x, y)

Where ∇f(x, y) and ∇g(x, y) are the gradients of f and g, respectively, and λ is the Lagrange multiplier. This equation, along with the constraint g(x, y) = c, forms a system of equations that can be solved to find the optimal values of x, y, and λ.

In summary, Lagrange's multiplier is a powerful technique in mathematical analysis that allows us to solve constrained optimization problems by transforming them into unconstrained ones and finding the critical points of the new function. This method is widely used in various fields, including economics, physics, and engineering, to solve optimization problems with constraints."
712,In how many ways can a group of 9 people be divided into 3 non-empty subsets?,3025,integer,NONE,self,jianyu_xu/Stirling_number_second_kind_2.json,stirling number of the second kind,Combinatorics,Math,"Stirling Numbers of the second kind, denoted as S(n, k), are used in combinatorics to count the number of ways to partition a set of n elements into k non-empty subsets. In other words, they represent the number of ways to distribute n distinct items into k distinct groups, where each group has at least one item.

The Stirling Numbers of the second kind can be defined recursively using the following formula:

S(n, k) = k * S(n-1, k) + S(n-1, k-1)

with the base cases:

S(n, 0) = 0 if n > 0,
S(0, 0) = 1,
S(n, k) = 0 if k > n.

The first term, k * S(n-1, k), represents the case where the nth element is added to one of the existing k subsets. The second term, S(n-1, k-1), represents the case where the nth element forms a new subset by itself.

Here are some examples of Stirling Numbers of the second kind:

S(3, 2) = 3: There are three ways to partition a set of 3 elements into 2 non-empty subsets: {1, 2}, {3}; {1, 3}, {2}; {1}, {2, 3}.
S(4, 2) = 7: There are seven ways to partition a set of 4 elements into 2 non-empty subsets: {1, 2, 3}, {4}; {1, 2, 4}, {3}; {1, 3, 4}, {2}; {2, 3, 4}, {1}; {1, 2}, {3, 4}; {1, 3}, {2, 4}; {1, 4}, {2, 3}.

Stirling Numbers of the second kind have various applications in combinatorics, including counting the number of permutations with a given number of cycles, counting the number of functions from one set to another, and solving problems related to set partitions and groupings."
476,"suppose a,b,c,\alpha,\beta,\gamma are six real numbers with a^2+b^2+c^2>0.  In addition, $a=b*cos(\gamma)+c*cos(\beta), b=c*cos(\alpha)+a*cos(\gamma), c=a*cos(\beta)+b*cos(\alpha)$. What is the value of $cos^2(\alpha)+cos^2(\beta)+cos^2(\gamma)+2*cos(\alpha)*cos(\beta)*cos(\gamma)? return the numeric.",1.0,float,NONE,linear algebra 3.5 example 7,mingyin/linear-dependence2.json,linear dependence,Algebra,Math,"Linear dependence in algebra refers to a relationship between two or more vectors or functions, where one can be expressed as a linear combination of the others. In other words, if one vector or function can be obtained by multiplying the others by some scalar constants and adding them together, they are said to be linearly dependent.

For example, consider three vectors A, B, and C. If there exist constants k1, k2, and k3 such that:

k1A + k2B + k3C = 0

and at least one of the constants (k1, k2, or k3) is non-zero, then the vectors A, B, and C are linearly dependent.

In contrast, if no such constants exist, the vectors are said to be linearly independent. Linearly independent vectors do not have any redundant information and cannot be expressed as a linear combination of the others.

Linear dependence is an important concept in linear algebra, as it helps determine the dimension of a vector space, the rank of a matrix, and the solutions to systems of linear equations."
595,What is \lim_{x 	o 9} ((x - 9)/(\sqrt{x} - 3))?,6,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2.json,l'hôpital's rule,Calculus,Math,"L'Hôpital's rule is a mathematical technique used in calculus to evaluate limits of indeterminate forms, specifically when the limit involves a fraction where both the numerator and the denominator approach zero or infinity. It is named after the French mathematician Guillaume de l'Hôpital, who published the rule in his book ""Analyse des Infiniment Petits"" in 1696.

The rule states that if the limit of a function f(x)/g(x) as x approaches a certain value (say, x=a) results in an indeterminate form of the type 0/0 or ∞/∞, then the limit of the function can be found by taking the limit of the derivative of the numerator divided by the derivative of the denominator, i.e.,

lim (x→a) [f(x) / g(x)] = lim (x→a) [f'(x) / g'(x)],

provided that the limit on the right-hand side exists or is a finite number.

L'Hôpital's rule can be applied repeatedly if the resulting limit after applying the rule is still an indeterminate form. It is important to note that L'Hôpital's rule can only be applied when the given conditions are met, and it is not a universal method for solving all types of limits.

In summary, L'Hôpital's rule is a powerful technique in calculus for evaluating limits of indeterminate forms involving fractions where both the numerator and the denominator approach zero or infinity. It involves taking the derivatives of the numerator and the denominator and then finding the limit of the resulting fraction."
285,Compute the mean translational kinetic energy of a single ideal gas molecule in eV.,0.038,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Example 9.1,tonyxia/statisticalphysics1.json,statistical physics,Statistical physics,Physics,"Statistical Physics, also known as Statistical Mechanics, is a branch of physics that uses statistical methods and probability theory to study the behavior of a large number of particles in a system. It aims to explain the macroscopic properties of matter, such as temperature, pressure, and volume, by considering the microscopic interactions and motions of individual particles, such as atoms and molecules.

Statistical Physics is particularly useful for understanding systems in thermodynamic equilibrium, where the macroscopic properties remain constant over time. It provides a bridge between microscopic laws of physics, like quantum mechanics and classical mechanics, and macroscopic thermodynamic laws, like the laws of thermodynamics.

The fundamental idea behind Statistical Physics is that the macroscopic properties of a system can be derived from the statistical behavior of its microscopic components. This is achieved by considering the ensemble of all possible microscopic states that the system can be in, and then calculating the probabilities of these states using statistical methods.

There are two main approaches in Statistical Physics:

1. Microcanonical ensemble: In this approach, the system is assumed to be isolated, with a fixed energy, volume, and number of particles. The microcanonical ensemble considers all possible microscopic states with the same energy, and the probabilities of these states are assumed to be equal. This leads to the concept of entropy, which is a measure of the number of accessible states for a given energy.

2. Canonical ensemble: In this approach, the system is assumed to be in contact with a heat reservoir, allowing energy exchange between the system and the reservoir. The canonical ensemble considers all possible microscopic states with varying energies, and the probabilities of these states are determined by the Boltzmann distribution. This leads to the concept of temperature, which is a measure of the average energy per particle in the system.

Statistical Physics has numerous applications in various fields, including condensed matter physics, astrophysics, biophysics, and even social sciences. It has been instrumental in explaining phenomena such as phase transitions, critical phenomena, and the behavior of systems near absolute zero temperature."
28,Is the Taylor Series for $f$ at x=5 where $f(x)=\sum_{n=0}^{\infty}\frac{x^n}{n!} absolutely converging?,1.0,float,NONE,website | https://www.varsitytutors.com/gre_subject_test_math-help/taylor-s-theorem,wenhuchen/taylor_expansion2.json,taylor's approximation theorem,Calculus,Math,"Taylor's approximation theorem, also known as Taylor's theorem, is a fundamental concept in calculus that provides an approximation of a differentiable function near a specific point using a polynomial called the Taylor polynomial. The theorem is named after the mathematician Brook Taylor, who introduced it in the early 18th century.

The Taylor polynomial is constructed using the function's derivatives at that specific point. The more terms included in the polynomial, the more accurate the approximation becomes. The Taylor polynomial of degree n for a function f(x) at a point a is given by:

P_n(x) = f(a) + f'(a)(x-a) + (f''(a)(x-a)^2)/2! + ... + (f^n(a)(x-a)^n)/n!

where f'(a), f''(a), and f^n(a) represent the first, second, and nth derivatives of the function evaluated at the point a, respectively.

Taylor's theorem states that if a function f(x) is (n+1) times differentiable in an interval containing the point a, then the error (or remainder) between the function and its Taylor polynomial of degree n is given by:

R_n(x) = (f^(n+1)(c)(x-a)^(n+1))/((n+1)!)

where c is a number between a and x.

In other words, Taylor's theorem provides a way to approximate a function using a polynomial, and it also gives an estimate of the error involved in the approximation. This is particularly useful when dealing with complex functions or when exact solutions are difficult to obtain. Taylor's theorem is the foundation for many numerical methods and is widely used in various fields of mathematics, physics, and engineering."
345,"For any triangle ABC, we have cos(A)cost(B)cos(C) $\leq$ 1/8, is this true or false?",True,bool,NONE,website | https://artofproblemsolving.com/wiki/index.php/Jensen%27s_Inequality,wenhuchen/jensen2.json,jensen's inequality,Mathematical analysis,Math,"Jensen's Inequality is a fundamental result in mathematical analysis and probability theory that provides an inequality involving convex functions and expectations. It is named after Danish mathematician Johan Jensen, who introduced it in 1906.

The inequality states that for a convex function f and a random variable X with a finite expected value E(X), the following inequality holds:

f(E(X)) ≤ E(f(X))

In other words, the value of the convex function f at the expected value of X is less than or equal to the expected value of the function f applied to X. If f is a concave function, the inequality is reversed:

f(E(X)) ≥ E(f(X))

Jensen's Inequality has important applications in various fields, including economics, finance, optimization, and statistics. It is often used to derive bounds on quantities of interest, to prove the convergence of algorithms, and to establish the convexity or concavity of functions.

Here's a simple example to illustrate Jensen's Inequality:

Let f(x) = x^2 be a convex function, and let X be a random variable with E(X) = μ. Then, by Jensen's Inequality, we have:

f(μ) = μ^2 ≤ E(X^2) = E(f(X))

This inequality is the basis for the definition of variance in statistics, as it shows that the expected value of the squared deviations from the mean is always non-negative."
641,Point charges q1=50μC and  q2=−25μC are placed 1.0 m apart. What is the force on a third charge q3=20μC placed midway between q1 and q2?,53.94,float,NONE,textbook | https://phys.libretexts.org/Bookshelves/University_Physics/Book%3A_University_Physics_(OpenStax)/Book%3A_University_Physics_II_-_Thermodynamics_Electricity_and_Magnetism_(OpenStax)/05%3A_Electric_Charges_and_Fields/5.0E%3A_5.E%3A_Electric_Charges_and_Fields_(Exercises),xueguangma/physics_coulombs_law.json,coulomb's law,Electromagnetism,Physics,"Coulomb's Law is a fundamental principle in electromagnetism that describes the force between two charged particles. It was first formulated by French physicist Charles-Augustin de Coulomb in 1785. The law states that the electrostatic force between two charged particles is directly proportional to the product of their charges and inversely proportional to the square of the distance between them.

Mathematically, Coulomb's Law can be expressed as:

F = k * (|q1 * q2|) / r^2

where:
- F is the electrostatic force between the two charged particles,
- q1 and q2 are the magnitudes of the charges of the two particles,
- r is the distance between the centers of the two particles,
- k is the electrostatic constant, also known as Coulomb's constant, which has a value of approximately 8.9875 × 10^9 N m^2 C^-2 in the International System of Units (SI).

The force acts along the line connecting the two charges and has a repulsive nature if the charges have the same sign (both positive or both negative), and an attractive nature if the charges have opposite signs (one positive and one negative).

Coulomb's Law is a fundamental principle in the study of electromagnetism and plays a crucial role in understanding various phenomena such as electric fields, electric potential, and the behavior of charged particles in different environments."
601,"For (10236, 244), use the Euclidean algorithm to find their gcd.",4,integer,NONE,self,wenhuchen/euclidean_algorithm.json,euclidean algorithm,Number theory,Math,"The Euclidean algorithm, also known as the Euclid's algorithm, is an ancient and efficient method for finding the greatest common divisor (GCD) of two integers. The GCD of two numbers is the largest positive integer that divides both numbers without leaving a remainder. The algorithm is based on the principle that the GCD of two numbers does not change if the smaller number is subtracted from the larger number.

The Euclidean algorithm can be described using the following steps:

1. Given two integers a and b, where a ≥ b > 0, perform the division a ÷ b and obtain the remainder r.
2. If r = 0, then the GCD is b, and the algorithm terminates.
3. If r ≠ 0, replace a with b and b with r, and repeat steps 1-2 until the remainder becomes 0.

The algorithm can also be implemented using the modulo operation, which directly computes the remainder of the division. In this case, the steps are as follows:

1. Given two integers a and b, where a ≥ b > 0, compute the remainder r = a mod b.
2. If r = 0, then the GCD is b, and the algorithm terminates.
3. If r ≠ 0, replace a with b and b with r, and repeat steps 1-2 until the remainder becomes 0.

The Euclidean algorithm is widely used in number theory and has several applications, such as simplifying fractions, solving Diophantine equations, and finding multiplicative inverses in modular arithmetic."
43,Calculate the future value of an ordinary annuity of $800 per year for 4 years at 5% rate of return.,3448.1,float,NONE,self,xueguangma/future_value_2.json,future value,Quantitive methods,Finance,"Future Value (FV) is a concept in finance and quantitative methods that refers to the estimated value of an investment or cash flow at a specific point in the future. It is based on the principle of time value of money, which states that a dollar today is worth more than a dollar in the future due to its potential earning capacity.

The Future Value calculation takes into account the initial investment amount, the interest rate, and the number of compounding periods to determine how much an investment will be worth in the future. The formula for calculating Future Value is:

FV = PV * (1 + r)^n

Where:
- FV is the Future Value of the investment
- PV is the Present Value or the initial investment amount
- r is the interest rate per compounding period (expressed as a decimal)
- n is the number of compounding periods

The Future Value calculation is essential in various financial applications, such as retirement planning, investment analysis, and capital budgeting. It helps investors and financial analysts to estimate the potential growth of an investment over time and make informed decisions about where to allocate their resources."
588,"The diagonals of rhombus QRST intersect at P. If m∠QTS = 76, find m∠TSP.",52,integer,NONE,Geometry Textbook | 11.png,panlu/rhombus1.json,rhombus,Geometry,Math,"A rhombus is a type of quadrilateral (a four-sided polygon) in geometry, where all four sides have equal length. It is sometimes referred to as an equilateral quadrilateral, as all its sides are congruent. The opposite sides of a rhombus are parallel, and the opposite angles are equal.

Some key properties of a rhombus include:

1. All four sides are equal in length.
2. Opposite sides are parallel.
3. Opposite angles are equal.
4. The diagonals of a rhombus bisect each other at right angles (90 degrees).
5. The diagonals also bisect the angles of the rhombus, meaning they divide the angles into two equal parts.
6. The area of a rhombus can be calculated using the formula: Area = (d1 * d2) / 2, where d1 and d2 are the lengths of the diagonals.

It is important to note that a square is a special case of a rhombus, where all angles are also equal to 90 degrees. However, not all rhombi are squares."
421,Assume a temperature of 300 K and find the wavelength of the photon necessary to cause an electron to jump from the valence to the conduction band in silicon in nm.,1130.0,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Exercise 11.9,tonyxia/semiconductor3.json,semiconductor theory,Condensed matter physics,Physics,"Semiconductor Theory is a branch of condensed matter physics that deals with the study of semiconductors, which are materials that have electrical conductivity between that of insulators and conductors. These materials have unique properties that make them essential for various electronic devices, such as transistors, diodes, and solar cells.

The fundamental concept in semiconductor theory is the electronic band structure, which describes the energy levels available to electrons in a solid material. In a semiconductor, there are two important bands: the valence band and the conduction band. The valence band is filled with electrons, while the conduction band is initially empty. The energy gap between these two bands is called the bandgap.

In insulators, the bandgap is large, making it difficult for electrons to move from the valence band to the conduction band. In conductors, the valence and conduction bands overlap, allowing electrons to move freely. Semiconductors have a small bandgap, which means that electrons can be excited from the valence band to the conduction band under certain conditions, such as the application of heat or light.

There are two main types of semiconductors: intrinsic and extrinsic. Intrinsic semiconductors are pure materials, such as silicon or germanium, with no impurities. In these materials, the number of electrons in the conduction band is equal to the number of holes (empty spaces) in the valence band. The electrical conductivity of intrinsic semiconductors is relatively low.

Extrinsic semiconductors are created by introducing impurities, or dopants, into the intrinsic semiconductor. This process, called doping, can create either n-type or p-type semiconductors. In n-type semiconductors, the dopant atoms have more valence electrons than the semiconductor atoms, resulting in an excess of free electrons. In p-type semiconductors, the dopant atoms have fewer valence electrons, creating an excess of holes. The interaction between n-type and p-type materials forms the basis for many electronic devices, such as diodes and transistors.

Semiconductor theory also involves the study of various physical phenomena, such as carrier transport (how electrons and holes move through the material), recombination (the process by which electrons and holes combine), and the behavior of semiconductors under different conditions, such as temperature and illumination.

Overall, semiconductor theory is a crucial area of condensed matter physics that has led to the development of modern electronics and continues to drive advancements in technology."
211,The bandwidth of an analog signal is 4kHz. An A/D converter is used to convert the signal from analog to digital. What is the minimum sampling rate for eliminating the aliasing problem? (in kHz),8,integer,NONE,website | https://vinesmsuic.github.io/notes-dimagep-QA3/,maxku/signalprocessing5-nyquist.json,nyquist-shannon sampling theorem,Signal processing,EECS,"The Nyquist-Shannon sampling theorem, also known as the Nyquist theorem or simply the sampling theorem, is a fundamental principle in the field of signal processing and digital communication. It provides a guideline for converting continuous-time (analog) signals into discrete-time (digital) signals by establishing a minimum sampling rate to accurately represent the original analog signal without losing any information.

The theorem states that a continuous-time signal can be accurately reconstructed from its discrete-time samples if the sampling rate (the number of samples taken per second) is at least twice the highest frequency component present in the original signal. This critical sampling rate is called the Nyquist rate, and the corresponding frequency is called the Nyquist frequency.

In mathematical terms, if a continuous-time signal x(t) has a highest frequency component B (measured in Hz), then the signal can be completely reconstructed from its samples if the sampling rate fs (measured in samples per second) is greater than or equal to 2B:

fs ≥ 2B

If the sampling rate is lower than the Nyquist rate, a phenomenon called aliasing occurs, where higher frequency components in the original signal are misrepresented as lower frequency components in the sampled signal. This leads to distortion and loss of information, making it impossible to accurately reconstruct the original signal from the sampled data.

In summary, the Nyquist-Shannon sampling theorem is a fundamental principle in signal processing that dictates the minimum sampling rate required to accurately represent a continuous-time signal in discrete-time form without losing any information. By ensuring that the sampling rate is at least twice the highest frequency component in the original signal, the theorem guarantees that the original signal can be perfectly reconstructed from its samples."
517,"Consider a strategy of the form $(\gamma, 0, 0)$ for the investment wheel. Show that the overall factor multiplying your money after $n$ steps is likely to be $(1+2\gamma)^{n/2}(1-\gamma)^{n/2}$. Find the value of $\gamma$ that maximizes this factor.",0.25,float,NONE,"textbook | Chapter 15, Exercise 1. Luenberger",xueguangma/wheel_strategy.json,wheel strategy,Derivatives,Finance,"The Wheel Strategy, also known as the Triple Income Strategy, is a popular options trading strategy that involves selling covered calls and cash-secured puts to generate consistent income from the stock market. It is called the ""Wheel"" because it involves a cyclical process of selling options and potentially owning the underlying stock. The strategy is typically employed by conservative investors who are looking for a way to enhance their income from their stock holdings without taking on significant risk.

Here's a step-by-step description of the Wheel Strategy:

1. Select a stock: Choose a stock that you are comfortable owning and has a decent dividend yield, good fundamentals, and relatively stable price movement. Ideally, the stock should also have high options liquidity, which means that there are many buyers and sellers for its options contracts.

2. Sell cash-secured put: Start by selling a cash-secured put option on the chosen stock. This means you are agreeing to buy the stock at the strike price if the option is exercised by the buyer. In return for this obligation, you receive a premium (income) from the option buyer. Make sure you have enough cash in your account to cover the potential purchase of the stock at the strike price.

3. Wait for expiration or assignment: If the stock price stays above the strike price at expiration, the put option will expire worthless, and you keep the premium. You can then sell another cash-secured put to continue the process. If the stock price falls below the strike price, you will be assigned the stock, meaning you will have to buy the shares at the strike price.

4. Sell covered call: Once you own the stock, you can sell a covered call option. This means you are agreeing to sell the stock at the strike price if the option is exercised by the buyer. In return for this obligation, you receive a premium (income) from the option buyer.

5. Wait for expiration or assignment: If the stock price stays below the strike price at expiration, the call option will expire worthless, and you keep the premium. You can then sell another covered call to continue the process. If the stock price rises above the strike price, your shares will be called away, meaning you will have to sell the shares at the strike price.

6. Repeat the process: If your shares are called away, you can start the process again by selling a cash-secured put. If your shares are not called away, you can continue selling covered calls until they are.

The Wheel Strategy aims to generate income from the premiums received from selling options while also potentially benefiting from stock ownership, dividends, and capital appreciation. However, it is essential to understand the risks involved, such as the potential for the stock price to decline significantly or the possibility of missing out on substantial gains if the stock price rises above the call option's strike price."
405,"George is seen to place an even-money $100,000 bet on the Bulls to win the NBA Finals. If George has a logarithmic utility-of-wealth function and if his current wealth is $1,000,000, what must he believe is the minimum probability that the Bulls will win?",0.525,float,NONE,"text | Walter Nicholson, Microeconomic Theory Basic Principles and Extensions.",elainewan/econ_micro_7.json,expected utility,Economics,Finance,"Expected Utility is a concept in economics and decision theory that refers to the total satisfaction or value that an individual expects to receive from a particular choice or decision, taking into account the probabilities of different outcomes. It is a key concept in understanding how people make decisions under uncertainty and is widely used in various fields, including finance, insurance, and game theory.

The Expected Utility Theory assumes that individuals are rational decision-makers who aim to maximize their utility or satisfaction. When faced with multiple options, individuals will choose the one that provides the highest expected utility. This means that they will weigh the potential benefits and costs of each option, considering the likelihood of each outcome occurring.

To calculate the expected utility of a decision, one must:

1. Identify all possible outcomes of the decision.
2. Assign a utility value to each outcome, representing the satisfaction or value that the individual would receive from that outcome.
3. Determine the probability of each outcome occurring.
4. Multiply the utility value of each outcome by its probability.
5. Sum the products of the utility values and probabilities to obtain the expected utility of the decision.

By comparing the expected utilities of different options, individuals can make informed choices that maximize their overall satisfaction.

It is important to note that the concept of expected utility is based on subjective evaluations of utility and probabilities, which may vary from person to person. Additionally, the theory assumes that individuals have perfect information and can accurately assess probabilities and utility values, which may not always be the case in real-world situations. Despite these limitations, expected utility remains a fundamental concept in understanding decision-making under uncertainty."
735,Find the sum of $\sum_{n=1}^{\infty} (1/e^n + 1/(n*(n+1)))$,1.581,float,NONE,website | https://people.math.sc.edu/josephcf/Teaching/142/Files/Worksheets/Infinite%20Series.pdf,wenhuchen/infinite_series_sum2.json,limiting theorem,Mathematical analysis,Math,"In mathematical analysis, a limiting theorem refers to a result that describes the behavior of a sequence, function, or series as it approaches a specific value or point. These theorems are fundamental in understanding the properties of mathematical objects and their convergence or divergence. There are several important limiting theorems in mathematical analysis, including:

1. Limit of a sequence: A sequence is a list of numbers arranged in a specific order. The limit of a sequence is the value that the terms of the sequence approach as the index goes to infinity. If the limit exists, the sequence is said to be convergent; otherwise, it is divergent.

2. Limit of a function: The limit of a function is the value that the function approaches as its input approaches a specific value. Limits are used to define continuity, derivatives, and integrals, which are essential concepts in calculus.

3. Squeeze theorem: Also known as the sandwich theorem or the pinching theorem, this theorem states that if a function is ""squeezed"" between two other functions that have the same limit at a specific point, then the squeezed function must also have the same limit at that point.

4. Monotone convergence theorem: This theorem states that a monotone (either non-decreasing or non-increasing) and bounded sequence always converges to a limit.

5. Bolzano-Weierstrass theorem: This theorem states that every bounded sequence has a convergent subsequence, which is a sequence formed by selecting terms from the original sequence while preserving their order.

6. Dominated convergence theorem: This theorem provides a condition under which the limit of an integral can be interchanged with the integral of a limit. It is particularly useful in the study of Lebesgue integration.

7. Central limit theorem: In probability theory and statistics, the central limit theorem states that the distribution of the sum (or average) of a large number of independent, identically distributed random variables approaches a normal distribution, regardless of the shape of the original distribution.

These limiting theorems play a crucial role in various branches of mathematics, including calculus, real analysis, complex analysis, and probability theory. They help us understand the behavior of mathematical objects and provide a foundation for further study and applications."
245,A $200-cm^3$ glass flask is filled to the brim with mercury at 20°C How much mercury overflows when the temperature of the system is raised to 100°C. The coefficient of linear expansion of the glass is $0.40 \times 10^{-5} K^{-1}. (Unit: cm^3),2.7,float,NONE,University Physics with Modern Physics | Example 17.3,panlu/volume_thermal_expansion1.json,volume thermal expansion,Thermodynamics,Physics,"Volume thermal expansion is a phenomenon in thermodynamics where the volume of a substance changes as a result of a change in temperature. When a substance is heated, its particles gain kinetic energy and start to move more rapidly. This increased movement causes the particles to occupy more space, leading to an increase in the volume of the substance. Conversely, when a substance is cooled, its particles lose kinetic energy, move less, and occupy less space, resulting in a decrease in volume.

This behavior is observed in solids, liquids, and gases, although the degree of expansion varies depending on the type of substance and its specific properties. In general, gases exhibit the most significant volume expansion when heated, followed by liquids, and then solids.

The relationship between the change in volume and the change in temperature can be described by the coefficient of volume expansion, which is a material-specific property. The coefficient of volume expansion (β) is defined as the fractional change in volume per unit change in temperature:

β = (ΔV / V₀) / ΔT

where ΔV is the change in volume, V₀ is the initial volume, and ΔT is the change in temperature.

The coefficient of volume expansion is typically expressed in units of inverse Kelvin (K⁻¹) or inverse Celsius (°C⁻¹). Different materials have different coefficients of volume expansion, which means they expand or contract at different rates when subjected to temperature changes. Understanding and accounting for volume thermal expansion is crucial in various engineering and scientific applications, such as designing bridges, buildings, and other structures that may be exposed to temperature fluctuations."
47,"Given image \begin{tabular}{|llll|} \hline 7 & 1 & 6 & 0 \\ 3 & 3 & 7 & 6 \\ 6 & 6 & 5 & 7 \\ \hline \end{tabular} , and the bit-depth of the image is 4. Is the contrast of the image is poor? Judge it based on the histogram of the image.",True,bool,NONE,self | https://vinesmsuic.github.io/notes-dimagep-QA/,maxku/cv-imageprocessing5-histogram.json,image contrast,Signal processing,EECS,"Image contrast in signal processing refers to the difference in intensity or color between various elements or regions within an image. It is a crucial aspect of image processing, as it determines the visibility and distinguishability of features within the image. High contrast images have a wide range of intensity values, making it easier to distinguish between different elements, while low contrast images have a narrow range of intensity values, making it harder to differentiate between various elements.

In signal processing, image contrast can be enhanced or manipulated using various techniques, such as histogram equalization, contrast stretching, and adaptive contrast enhancement. These methods aim to improve the visibility of features within the image by adjusting the intensity values or color distribution.

In summary, image contrast in signal processing is a measure of the difference in intensity or color between various elements within an image, and it plays a vital role in determining the quality and visibility of features in the image."
716,"Let $P_5(x)$ be the fifth-degree Taylor polynomial approximation for f(x)=sin(x), centered at x=0. What is the Lagrange error of the polynomial approximation to sin(1)?.",0.000198,float,NONE,website | https://www.varsitytutors.com/gre_subject_test_math-help/lagrange-s-theorem,wenhuchen/taylor_expansion1.json,taylor's approximation theorem,Calculus,Math,"Taylor's approximation theorem, also known as Taylor's theorem, is a fundamental concept in calculus that provides an approximation of a differentiable function near a specific point using a polynomial called the Taylor polynomial. The theorem is named after the mathematician Brook Taylor, who introduced it in the early 18th century.

The Taylor polynomial is constructed using the function's derivatives at that specific point. The more terms included in the polynomial, the more accurate the approximation becomes. The Taylor polynomial of degree n for a function f(x) at a point a is given by:

P_n(x) = f(a) + f'(a)(x-a) + (f''(a)(x-a)^2)/2! + ... + (f^n(a)(x-a)^n)/n!

where f'(a), f''(a), and f^n(a) represent the first, second, and nth derivatives of the function evaluated at the point a, respectively.

Taylor's theorem states that if a function f(x) is (n+1) times differentiable in an interval containing the point a, then the error (or remainder) between the function and its Taylor polynomial of degree n is given by:

R_n(x) = (f^(n+1)(c)(x-a)^(n+1))/((n+1)!)

where c is a number between a and x.

In other words, Taylor's theorem provides a way to approximate a function using a polynomial, and it also gives an estimate of the error involved in the approximation. This is particularly useful when dealing with complex functions or when exact solutions are difficult to obtain. Taylor's theorem is the foundation for many numerical methods and is widely used in various fields of mathematics, physics, and engineering."
394,Is (t-y)y' - 2y +3t + y^2/t = 0 an Euler homogeneous equation?,True,bool,NONE,website | https://users.math.msu.edu/users/gnagy/teaching/ode.pdf,wenhuchen/differential_equation2.json,differential equation,Calculus,Math,"A differential equation is a mathematical equation that relates a function with its derivatives. In calculus, differential equations are used to describe various phenomena in fields such as physics, engineering, biology, and economics, where the rate of change of a variable is essential to understanding the system's behavior.

Differential equations can be classified into several types, including ordinary differential equations (ODEs) and partial differential equations (PDEs). ODEs involve functions of a single variable and their derivatives, while PDEs involve functions of multiple variables and their partial derivatives.

Differential equations can also be categorized based on their order, which is determined by the highest derivative present in the equation. For example, a first-order differential equation contains only the first derivative of the function, while a second-order differential equation contains both the first and second derivatives.

Solving a differential equation involves finding a function or a set of functions that satisfy the given equation. Depending on the type and complexity of the equation, various techniques can be employed to find the solution, such as separation of variables, integrating factors, or numerical methods.

In summary, differential equations are a fundamental concept in calculus that describe the relationship between a function and its derivatives. They are widely used in various fields to model and analyze systems where the rate of change plays a crucial role."
307,"In year N, the 300th day of the year is a Tuesday. In year N + 1, the 200th day is also a Tuesday. Suppose Monday is the 1-th day of the week, on which day of the week did the 100th day of the year N - 1 occur? Return a numeric between 1 and 7.",4,integer,NONE,2000 AMC 12 18,tonyxia/modulararithmetic1.json,modular arithmetic,Number theory,Math,"Modular arithmetic, also known as clock arithmetic or the arithmetic of congruences, is a branch of number theory that deals with the properties and relationships of integers under the operation of modular addition, subtraction, multiplication, and sometimes division. It is a fundamental concept in number theory, cryptography, and computer science.

In modular arithmetic, numbers ""wrap around"" upon reaching a certain value called the modulus. The modulus is a positive integer that defines the size of the set of numbers being considered. When performing arithmetic operations, the result is always reduced to the remainder when divided by the modulus. This can be thought of as working with numbers on a circular number line, where the numbers wrap around after reaching the modulus.

The basic idea of modular arithmetic can be illustrated using a clock. A clock has a modulus of 12 (for a 12-hour clock) or 24 (for a 24-hour clock). When the hour hand moves past 12 or 24, it wraps around to 1 or 0, respectively. For example, if it is 10 o'clock and we add 5 hours, the result is 3 o'clock, not 15 o'clock. In this case, we are working modulo 12 or modulo 24.

In mathematical notation, modular arithmetic is often represented using the congruence symbol (≡). Two numbers a and b are said to be congruent modulo n if their difference (a - b) is divisible by n. This is written as:

a ≡ b (mod n)

For example, 17 ≡ 5 (mod 12) because 17 - 5 = 12, which is divisible by 12.

Modular arithmetic has many applications in various fields, including number theory, cryptography, computer science, and algebra. It is particularly useful in solving problems involving remainders, divisibility, and periodic patterns."
242,An IPv4 packet contains the following data (in hexadecimal value) in the IP header: 4500 0034 B612 4000 4006 6F80 0A00 008B 5BC6 AEE0 . Does the header contains error?,False,bool,NONE,website | https://vinesmsuic.github.io/notes-networking3-qa/,maxku/ipnetwork3-ip.json,internet protocol,Computer networking,EECS,"Internet Protocol (IP) is a set of rules and standards that govern how data is transmitted, received, and routed across computer networks, including the internet. It is a fundamental component of the Internet Protocol Suite, which is a collection of protocols and technologies that enable communication between devices over the internet.

IP operates at the network layer (Layer 3) of the Open Systems Interconnection (OSI) model and is responsible for addressing, packaging, and routing data packets between devices. It ensures that data is sent from a source device to a destination device, even if they are on different networks.

There are two main versions of IP in use today: IPv4 (Internet Protocol version 4) and IPv6 (Internet Protocol version 6).

IPv4 is the most widely used version, which uses 32-bit addresses, allowing for approximately 4.3 billion unique IP addresses. Due to the rapid growth of the internet, the number of available IPv4 addresses has become limited, leading to the development of IPv6.

IPv6 uses 128-bit addresses, providing a vastly larger number of unique IP addresses (approximately 3.4 x 10^38) to accommodate the growing number of devices connected to the internet.

Key features of Internet Protocol include:

1. Addressing: IP assigns unique addresses to devices on a network, enabling them to be identified and located. These addresses are used to route data packets to their intended destinations.

2. Packetization: IP divides data into smaller units called packets, which are then transmitted independently across the network. This allows for more efficient use of network resources and enables data to be sent over multiple paths.

3. Routing: IP uses routing algorithms to determine the best path for data packets to travel from the source device to the destination device. Routers, which are specialized devices that connect networks, use IP addresses to forward packets along the most efficient route.

4. Error detection: IP includes a checksum mechanism to detect errors in the header of data packets. If an error is detected, the packet is discarded, and the sender may be notified to resend the data.

5. Fragmentation and reassembly: IP can fragment large packets into smaller ones to accommodate the maximum transmission unit (MTU) of different networks. The destination device then reassembles the fragments back into the original data.

Overall, Internet Protocol plays a crucial role in enabling communication between devices on computer networks and the internet, providing the foundation for various applications and services we use daily."
113,"If T_1 and T_2 are stopping times with respect to a filtration F. Is T_1+T_2 stopping time? Is max(T_1, T_2} stopping time? Is min(T_1, T_2} stopping time? Answer 1 for yes and 0 for no. Return the answers of the three questions as a list.","[1, 1, 1]",list of integer,NONE,probability: 12.4.1,mingyin/stopping-time1.json,martingale,Probability theory,Math,"Martingale, in probability theory, is a mathematical model used to describe a fair game or a stochastic process where the expected value of a random variable at a future time step is equal to its present value, given all the past information. In other words, the expected gain or loss in a Martingale system is always zero, regardless of the outcomes of previous events.

The concept of Martingale is often used in betting strategies, finance, and statistical analysis. In betting, the Martingale strategy involves doubling the bet after each loss, so that the first win would recover all previous losses plus a profit equal to the original stake. However, this strategy has its limitations, as it requires an infinite bankroll and has no guarantee of winning in the long run.

In a formal definition, a sequence of random variables {X1, X2, X3, ...} is called a Martingale with respect to another sequence of random variables {Y1, Y2, Y3, ...} if the following conditions are met:

1. The random variables {X1, X2, X3, ...} are integrable, meaning their expected values exist and are finite.
2. The random variables {Y1, Y2, Y3, ...} form a filtration, which is a sequence of increasing sigma-algebras (collections of events) that represent the information available at each time step.
3. For each time step, the expected value of the next random variable in the sequence, given the information available up to the current time step, is equal to the current random variable's value. Mathematically, this can be expressed as E[Xn+1 | Y1, Y2, ..., Yn] = Xn.

In summary, a Martingale is a sequence of random variables that represents a fair game or process, where the expected value of a future event is equal to the current value, given all past information."
112,"Suppose C is a compact convex set in a linear normed space, and let T: C → C be a continuous mapping. Then, there exists a fixed point of T in C. Is this correct? Answer 1 for yes and 0 for no.",1.0,float,NONE,functional analysis: exercise 1.5.3,mingyin/Schauder-fix-point-theorem1.json,schauder fixed point theorem,Functional analysis,Math,"Schauder fixed point theorem is a fundamental result in functional analysis, a branch of mathematics that deals with the study of function spaces and linear operators between them. The theorem is named after Juliusz Schauder, a Polish mathematician who first stated it in 1930. The Schauder fixed point theorem is a generalization of the Brouwer fixed point theorem and provides a powerful tool for proving the existence of fixed points for certain types of nonlinear operators.

Statement of the theorem:

Let X be a non-empty, compact, and convex subset of a locally convex topological vector space (a generalization of normed vector spaces). If T: X → X is a continuous and compact (i.e., T maps bounded sets to relatively compact sets) operator, then T has a fixed point, i.e., there exists an x ∈ X such that T(x) = x.

In simpler terms, the Schauder fixed point theorem states that under certain conditions, a continuous and compact operator acting on a compact and convex set has at least one fixed point.

The Schauder fixed point theorem has several important applications in various fields of mathematics, including partial differential equations, integral equations, and game theory. It is particularly useful in proving the existence of solutions to certain types of nonlinear equations, where traditional methods like the inverse function theorem or the implicit function theorem may not be applicable."
333,"Let P[0,1] denotes all the polynomials on the interval [0,1]. Define the distance \rho(p, q)=\int_0^1|p(x)-q(x)| dx. Is (P[0,1],\rho) a complete space? Return 1 for yes and 0 for no.",0.0,float,NONE,functional analysis: exercise 1.2.4,mingyin/complete-metric-space1.json,complete space,Functional analysis,Math,"In functional analysis, a complete space, also known as a complete metric space, is a metric space in which every Cauchy sequence converges to a limit within the space. A metric space is a set equipped with a distance function (or metric) that defines the distance between any two points in the set.

To understand the concept of a complete space, it is essential to know what a Cauchy sequence is. A Cauchy sequence is a sequence of elements in a metric space such that the distance between any two elements in the sequence becomes arbitrarily small as the sequence progresses. In other words, the elements of the sequence get closer and closer together as the sequence goes on.

A complete space is a metric space with the property that every Cauchy sequence converges to a limit within the space. This means that, given any Cauchy sequence in the space, there exists an element in the space that the sequence converges to. This is an important property in functional analysis because it ensures that certain mathematical operations, such as taking limits, can be performed within the space without having to worry about the limit being outside the space.

An example of a complete space is the set of real numbers with the usual distance function (the absolute difference between two numbers). Every Cauchy sequence of real numbers converges to a real number, so the space is complete. On the other hand, the set of rational numbers is not complete, as there are Cauchy sequences of rational numbers that do not converge to a rational number (e.g., a sequence converging to an irrational number like the square root of 2).

In functional analysis, complete spaces play a crucial role in the study of function spaces, linear operators, and other related concepts. One of the most important types of complete spaces in functional analysis is a Banach space, which is a complete normed vector space. Banach spaces are fundamental in the study of various problems in analysis, differential equations, and optimization."
652,\lim_{x \to c} |f(x)| = 0. What is \lim_{x \to c} f(x)?,0,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_7.json,squeeze theorem,Calculus,Math,"The Squeeze Theorem, also known as the Sandwich Theorem or the Pinching Theorem, is a fundamental concept in calculus that helps to determine the limit of a function when direct substitution or algebraic manipulation is not possible. The theorem states that if you have three functions, f(x), g(x), and h(x), such that f(x) ≤ g(x) ≤ h(x) for all x in a certain interval around a point 'a' (except possibly at 'a' itself), and if the limit of f(x) and h(x) as x approaches 'a' is the same value L, then the limit of g(x) as x approaches 'a' must also be L.

In mathematical notation, the Squeeze Theorem can be written as:

If f(x) ≤ g(x) ≤ h(x) for all x in an interval around 'a' (except possibly at 'a') and lim (x→a) f(x) = lim (x→a) h(x) = L, then lim (x→a) g(x) = L.

The Squeeze Theorem is particularly useful when dealing with trigonometric functions or functions that are difficult to evaluate directly. By comparing the function of interest (g(x)) to two other functions (f(x) and h(x)) that ""squeeze"" or ""sandwich"" it, we can determine the limit of g(x) as x approaches a certain point."
437,"Consider $x(t)$ to be given as, $$ x(t)=10 \cos (20 \pi-\pi / 4)-5 \cos (50 \pi t) $$ What is minimum sampling rate (/Hz) such that $y(t)=x(t)$ ?",50,integer,NONE,website | https://vinesmsuic.github.io/notes-signal3/#Aliasing,maxku/signalprocessing10-nyquist.json,nyquist-shannon sampling theorem,Signal processing,EECS,"The Nyquist-Shannon sampling theorem, also known as the Nyquist theorem or simply the sampling theorem, is a fundamental principle in the field of signal processing and digital communication. It provides a guideline for converting continuous-time (analog) signals into discrete-time (digital) signals by establishing a minimum sampling rate to accurately represent the original analog signal without losing any information.

The theorem states that a continuous-time signal can be accurately reconstructed from its discrete-time samples if the sampling rate (the number of samples taken per second) is at least twice the highest frequency component present in the original signal. This critical sampling rate is called the Nyquist rate, and the corresponding frequency is called the Nyquist frequency.

In mathematical terms, if a continuous-time signal x(t) has a highest frequency component B (measured in Hz), then the signal can be completely reconstructed from its samples if the sampling rate fs (measured in samples per second) is greater than or equal to 2B:

fs ≥ 2B

If the sampling rate is lower than the Nyquist rate, a phenomenon called aliasing occurs, where higher frequency components in the original signal are misrepresented as lower frequency components in the sampled signal. This leads to distortion and loss of information, making it impossible to accurately reconstruct the original signal from the sampled data.

In summary, the Nyquist-Shannon sampling theorem is a fundamental principle in signal processing that dictates the minimum sampling rate required to accurately represent a continuous-time signal in discrete-time form without losing any information. By ensuring that the sampling rate is at least twice the highest frequency component in the original signal, the theorem guarantees that the original signal can be perfectly reconstructed from its samples."
477,"RS is the midsegment of trapezoid MNOP. If MN = 10x+3, RS=9x-1, and PO = 4x+7, what is the length of RS?",26,integer,NONE,Geometry Textbook | 12.png,panlu/trapezoid1.json,trapezoidal rule,Calculus,Math,"The Trapezoidal Rule is a numerical integration technique used in calculus to approximate the definite integral of a function. It works by dividing the area under the curve of the function into a series of trapezoids and then summing the areas of these trapezoids to estimate the total area. This method is particularly useful when dealing with functions that are difficult or impossible to integrate analytically.

The basic idea behind the Trapezoidal Rule is to approximate the function with a series of straight lines connecting the points on the curve. These lines form the bases of the trapezoids, and the height of each trapezoid is determined by the difference in the x-values (Δx) between consecutive points.

To apply the Trapezoidal Rule, follow these steps:

1. Divide the interval [a, b] into n equal subintervals, where a and b are the limits of integration, and n is the number of subintervals.
2. Calculate the width of each subinterval, Δx = (b - a) / n.
3. Evaluate the function at each endpoint of the subintervals: f(a), f(a + Δx), f(a + 2Δx), ..., f(b).
4. Calculate the area of each trapezoid using the formula: Area = (1/2) * (f(x_i) + f(x_(i+1))) * Δx, where x_i and x_(i+1) are consecutive endpoints of the subintervals.
5. Sum the areas of all the trapezoids to obtain the approximate value of the definite integral.

The accuracy of the Trapezoidal Rule increases as the number of subintervals (n) increases, but it may require a large number of subintervals for functions with high curvature or rapid changes. Other numerical integration techniques, such as Simpson's Rule, may provide more accurate results with fewer subintervals."
694,"Let $X \sim N(0,1)$ and let the distortion measure be squared error. Here we do not allow block descriptions. Compute the minimum expected distortion for one bit quantization of $X$ using a squared error distortion measure.",0.363,float,NONE,textbook 10.1,xinyi/expected_distortion.json,rate-distortion theory,Information theory,EECS,"Rate-distortion theory is a fundamental concept in information theory that deals with the trade-off between the compression rate of a source and the distortion or loss of information that occurs during the compression process. It was first introduced by Claude Shannon in 1948 and has since become an essential tool in the analysis and design of communication systems, particularly in the field of data compression and signal processing.

In simple terms, rate-distortion theory aims to find the optimal balance between the amount of data that can be compressed (rate) and the quality of the reconstructed data after decompression (distortion). The main idea is that as the compression rate increases, the distortion in the reconstructed data also increases, and vice versa. The goal is to minimize the distortion while maintaining an acceptable compression rate.

Rate-distortion theory is based on two main components:

1. Rate: The rate refers to the number of bits per symbol required to represent the compressed data. A lower rate means higher compression, but it may also result in more distortion in the reconstructed data.

2. Distortion: Distortion is a measure of the difference between the original data and the reconstructed data after compression and decompression. It quantifies the loss of information or quality that occurs during the compression process. Distortion can be measured in various ways, such as mean squared error, signal-to-noise ratio, or perceptual quality metrics.

The rate-distortion function (R(D)) is a mathematical representation of the relationship between the rate and distortion. It describes the minimum achievable rate for a given level of distortion or the minimum distortion that can be achieved for a given rate. The rate-distortion function is typically derived using probabilistic models of the source data and the distortion measure.

In practical applications, rate-distortion theory is used to design efficient compression algorithms, such as image and video codecs, audio codecs, and lossy data compression techniques. By understanding the trade-offs between rate and distortion, engineers can develop algorithms that provide the best possible compression performance while maintaining an acceptable level of quality in the reconstructed data."
704,"Compute the integral $\iint_D xy^2 dA$, where $D$ is the rectangle defined by 0 <= x <= 2 and 0 <= y <= 1.",0.66667,float,NONE,website | https://mathinsight.org/double_integral_examples,wenhuchen/double_integral2.json,double integral theorem,Calculus,Math,"The double integral theorem, also known as Fubini's theorem or Tonelli's theorem, is a fundamental result in calculus that allows us to evaluate double integrals by iterated integration. In other words, it allows us to break down a double integral over a rectangular region into two single integrals, making it easier to compute.

Suppose we have a function f(x, y) that is continuous over a rectangular region R = [a, b] x [c, d] in the xy-plane. The double integral theorem states that the double integral of f(x, y) over the region R can be computed as the iterated integral:

∬(R) f(x, y) dA = ∫(a to b) [∫(c to d) f(x, y) dy] dx = ∫(c to d) [∫(a to b) f(x, y) dx] dy

Here, dA represents the differential area element, and the order of integration can be chosen based on the convenience of computation.

The theorem is named after Guido Fubini and Leonida Tonelli, who contributed significantly to the development of the theory of integration. It is important to note that Fubini's theorem holds under certain conditions, such as when the function f(x, y) is continuous or when it is integrable and the integral of the absolute value of the function is finite.

In summary, the double integral theorem is a powerful tool in calculus that allows us to evaluate double integrals by breaking them down into two single integrals, making the computation process more manageable."
40,"Suppose the graph of a polynomial f(t) = a + bt + ct^2 passes through points (1, -1), (2, 3), and (3, 13). What is f(-1)?",9,integer,NONE,"text | Otto Bretscher, Linear Algebra with Applications.",elainewan/math_algebra_1_2.json,linear systems,Algebra,Math,"Linear systems, also known as systems of linear equations, are a collection of linear equations that involve the same set of variables. In algebra, these systems are used to model and solve problems where multiple variables are related to each other through linear relationships.

A linear equation is an equation of the form:

a1 * x1 + a2 * x2 + ... + an * xn = b

where x1, x2, ..., xn are the variables, a1, a2, ..., an are the coefficients, and b is a constant term.

A linear system can be represented as:

a11 * x1 + a12 * x2 + ... + a1n * xn = b1
a21 * x1 + a22 * x2 + ... + a2n * xn = b2
...
am1 * x1 + am2 * x2 + ... + amn * xn = bm

where m is the number of equations and n is the number of variables.

The main goal when working with linear systems is to find the values of the variables that satisfy all the equations simultaneously. There are several methods to solve linear systems, including graphing, substitution, elimination, and matrix methods.

There are three possible outcomes when solving a linear system:

1. Unique solution: The system has exactly one solution, which means there is a unique set of values for the variables that satisfy all the equations.
2. No solution: The system has no solution, which means there is no set of values for the variables that satisfy all the equations. This occurs when the equations are inconsistent.
3. Infinite solutions: The system has infinitely many solutions, which means there are multiple sets of values for the variables that satisfy all the equations. This occurs when the equations are dependent and describe the same relationship between the variables."
210,Using Taylor's Approximation Theorem to show: What is $\lim_{x \to 0} \frac{e^\frac{x^4}{2}-\cos(x^2)}{x^4}$,1.0,float,NONE,textbook | Forrest_M147CN_F20.pdf,xueguangma/taylors_approximation_theorem.json,taylor's approximation theorem,Calculus,Math,"Taylor's approximation theorem, also known as Taylor's theorem, is a fundamental concept in calculus that provides an approximation of a differentiable function near a specific point using a polynomial called the Taylor polynomial. The theorem is named after the mathematician Brook Taylor, who introduced it in the early 18th century.

The Taylor polynomial is constructed using the function's derivatives at that specific point. The more terms included in the polynomial, the more accurate the approximation becomes. The Taylor polynomial of degree n for a function f(x) at a point a is given by:

P_n(x) = f(a) + f'(a)(x-a) + (f''(a)(x-a)^2)/2! + ... + (f^n(a)(x-a)^n)/n!

where f'(a), f''(a), and f^n(a) represent the first, second, and nth derivatives of the function evaluated at the point a, respectively.

Taylor's theorem states that if a function f(x) is (n+1) times differentiable in an interval containing the point a, then the error (or remainder) between the function and its Taylor polynomial of degree n is given by:

R_n(x) = (f^(n+1)(c)(x-a)^(n+1))/((n+1)!)

where c is a number between a and x.

In other words, Taylor's theorem provides a way to approximate a function using a polynomial, and it also gives an estimate of the error involved in the approximation. This is particularly useful when dealing with complex functions or when exact solutions are difficult to obtain. Taylor's theorem is the foundation for many numerical methods and is widely used in various fields of mathematics, physics, and engineering."
542,"If at the beginning of each month a deposit of $500 is made in an account that pays 8% compounded monthly, what will the final amount be after five years?",36983.35,float,NONE,website | https://math.libretexts.org/Bookshelves/Applied_Mathematics/Applied_Finite_Mathematics_(Sekhon_and_Bloom)/06%3A_Mathematics_of_Finance/6.03%3A_Annuities_and_Sinking_Funds,xueguangma/annuity_due.json,annuity due,Quantitive methods,Finance,"Annuity Due is a type of annuity payment structure in quantitative methods, where a series of equal payments are made at the beginning of each period, rather than at the end. It is a financial instrument commonly used in retirement planning, loans, and other financial agreements.

In an annuity due, the present value and future value calculations differ from those of an ordinary annuity, as the cash flows occur at the beginning of each period. This results in a higher present value and future value compared to an ordinary annuity, as the payments are received or made earlier.

The present value of an annuity due can be calculated using the following formula:

PV = PMT * [(1 - (1 + r)^(-n)) / r] * (1 + r)

Where:
- PV is the present value of the annuity due
- PMT is the periodic payment amount
- r is the interest rate per period
- n is the number of periods

The future value of an annuity due can be calculated using the following formula:

FV = PMT * [((1 + r)^n - 1) / r] * (1 + r)

Where:
- FV is the future value of the annuity due
- PMT is the periodic payment amount
- r is the interest rate per period
- n is the number of periods

Annuity due is often used in situations where payments need to be made upfront, such as rent payments, lease agreements, or insurance premiums. It is also used in calculating the present value of a series of cash flows when the first cash flow occurs immediately."
256,What is the value of the integral $\int_2^4 \frac{\sqrt{log(9-x)}}{\sqrt{log(9-x)}+\sqrt{log(x+3)}} dx$?,1.0,float,NONE,Putnam college math competition,mingyin/integral-theorem3.json,integral rules,Calculus,Math,"Integral rules in calculus are a set of techniques and formulas used to evaluate and solve integrals. Integrals are a fundamental concept in calculus, representing the area under a curve or the accumulation of a quantity over a given interval. The integral rules provide a systematic approach to finding the antiderivative (the inverse of the derivative) of a function, which is essential for solving various mathematical and real-world problems.

Here are some of the most common integral rules:

1. Constant Rule: The integral of a constant (c) with respect to a variable (x) is equal to the product of the constant and the variable, plus a constant of integration (C). 
   ∫c dx = cx + C

2. Power Rule: The integral of x raised to the power of n (x^n) with respect to x is equal to x raised to the power of (n+1) divided by (n+1), plus a constant of integration (C). This rule is valid for n ≠ -1.
   ∫x^n dx = (x^(n+1))/(n+1) + C

3. Sum/Difference Rule: The integral of the sum or difference of two functions is equal to the sum or difference of their integrals.
   ∫(f(x) ± g(x)) dx = ∫f(x) dx ± ∫g(x) dx

4. Constant Multiple Rule: The integral of a constant multiplied by a function is equal to the constant multiplied by the integral of the function.
   ∫(cf(x)) dx = c∫f(x) dx

5. Substitution Rule (u-substitution): This rule is used when a function is composed of another function. It involves substituting a new variable (u) for a part of the original function, and then integrating with respect to the new variable.
   If u = g(x) and du/dx = g'(x), then ∫f(g(x))g'(x) dx = ∫f(u) du

6. Integration by Parts: This rule is used for integrating the product of two functions. It is based on the product rule for differentiation.
   If u = f(x) and v = g(x), then ∫u dv = uv - ∫v du

7. Trigonometric Integrals: These rules involve the integration of various trigonometric functions, such as sine, cosine, tangent, and their combinations.

8. Partial Fractions: This technique is used to integrate rational functions (fractions with polynomials in the numerator and denominator). It involves decomposing the rational function into simpler fractions, which can be integrated individually.

9. Improper Integrals: These rules deal with integrals that have infinite limits or involve functions with discontinuities. They often require the use of limits to evaluate the integral.

These integral rules, along with other advanced techniques, form the foundation of integral calculus and are essential for solving a wide range of mathematical problems."
385,"In how many ways can we form a 7-digit number using the digits 1, 2, 2, 3, 3, 3, 4?",420,integer,NONE,self,jianyu_xu/Multinomial_3.json,multinomial theorem,Combinatorics,Math,"The Multinomial theorem is a generalization of the binomial theorem, which deals with the expansion of powers of a sum of multiple terms. In combinatorics, the Multinomial theorem is used to count the number of ways to partition a set of objects into multiple groups, taking into account the order of the groups.

The theorem states that for any non-negative integer n and any positive integers k1, k2, ..., kr, such that k1 + k2 + ... + kr = n, the expansion of the power (x1 + x2 + ... + xr)^n can be expressed as:

(x1 + x2 + ... + xr)^n = Σ (n! / (k1! * k2! * ... * kr!)) * (x1^k1 * x2^k2 * ... * xr^kr)

where the summation is taken over all possible combinations of k1, k2, ..., kr that satisfy the condition k1 + k2 + ... + kr = n, and n! denotes the factorial of n (i.e., the product of all positive integers up to n).

The coefficients in the expansion, n! / (k1! * k2! * ... * kr!), are called multinomial coefficients, and they represent the number of ways to divide a set of n objects into r groups, with k1 objects in the first group, k2 objects in the second group, and so on.

In combinatorics, the Multinomial theorem is often used to solve counting problems, such as the number of ways to arrange objects with repetitions, or the number of ways to distribute objects into different containers with restrictions on the number of objects in each container."
50,Suppose ${X_n:n\geq 1}$ be independent and exponentially distributed with parameter 1. what is the probability $P(\limsup _{n \rightarrow infty} X_n/\log(n)=1)? Return a numeric value.,1.0,float,NONE,probability: 7.3.9,mingyin/borel-cantelli-lemma1.json,borel cantelli lemma,Probability theory,Math,"The Borel-Cantelli Lemma is a fundamental result in probability theory that provides a criterion for determining the convergence of an infinite sequence of events. It is named after French mathematician Émile Borel and Italian mathematician Francesco Paolo Cantelli. The lemma comes in two parts: the first part is known as the ""Borel-Cantelli Lemma"" or ""First Borel-Cantelli Lemma,"" and the second part is known as the ""Converse Borel-Cantelli Lemma"" or ""Second Borel-Cantelli Lemma.""

First Borel-Cantelli Lemma:
The first Borel-Cantelli Lemma states that if the sum of the probabilities of an infinite sequence of events is finite, then the probability that infinitely many of these events occur is zero. Mathematically, given a sequence of events {A_n} in a probability space, if the sum of their probabilities is finite, i.e.,

Σ P(A_n) < ∞,

then the probability of infinitely many of these events occurring is zero, i.e.,

P(lim sup A_n) = 0.

Here, ""lim sup"" denotes the limit superior of the sequence of events, which is the event that infinitely many of the events A_n occur.

Second Borel-Cantelli Lemma (Converse):
The second Borel-Cantelli Lemma provides a converse to the first lemma under an additional condition of independence. It states that if the events are independent and the sum of their probabilities is infinite, then the probability that infinitely many of these events occur is one. Mathematically, given an independent sequence of events {A_n} in a probability space, if the sum of their probabilities is infinite, i.e.,

Σ P(A_n) = ∞,

then the probability of infinitely many of these events occurring is one, i.e.,

P(lim sup A_n) = 1.

The Borel-Cantelli Lemma is a powerful tool in probability theory and has applications in various fields, including ergodic theory, number theory, and statistics. It helps to understand the long-term behavior of random processes and the convergence of infinite sequences of events."
12,"How many labeled graphs with a score of (6, 2, 2, 2, 2, 2, 2) are there?",15,integer,NONE,Self,tonyxia/score4.json,score theorem,Graph theory,EECS,"In graph theory, the Score Theorem, also known as the Havel-Hakimi Theorem, is a method used to determine if a given degree sequence can be realized by a simple, undirected graph. A degree sequence is a list of non-negative integers that represents the degrees of the vertices in a graph. A simple graph is a graph with no loops or multiple edges between the same pair of vertices.

The Score Theorem is based on the Havel-Hakimi algorithm, which is a recursive algorithm that works as follows:

1. Sort the degree sequence in non-increasing order.
2. If all the degrees are zero, then the sequence is graphical, meaning it can be realized by a simple graph.
3. If there are any negative degrees, the sequence is non-graphical, meaning it cannot be realized by a simple graph.
4. Remove the first degree (d1) from the sequence and reduce the next d1 degrees by 1.
5. Repeat steps 1-4 with the new sequence.

The theorem states that a degree sequence is graphical if and only if the sequence obtained by applying the Havel-Hakimi algorithm is graphical.

In other words, the Score Theorem provides a way to check if it is possible to construct a simple, undirected graph with a given degree sequence. If the algorithm terminates with all degrees being zero, then the original sequence can be realized by a simple graph. Otherwise, it cannot."
733,What is the value of the inflection point of f(x) =(10 ln(x))/(x^2)?,2.301,float,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_7_5.json,inflection points,Calculus,Math,"In calculus, inflection points are points on a curve where the curve changes its concavity, i.e., it switches from being concave up (shaped like a U) to concave down (shaped like an upside-down U), or vice versa. In other words, an inflection point is a point on the curve where the second derivative of the function changes its sign.

To find inflection points, you need to follow these steps:

1. Find the first derivative (dy/dx) of the function, which represents the slope of the tangent line to the curve at any given point.
2. Find the second derivative (d^2y/dx^2) of the function, which represents the curvature or concavity of the curve at any given point.
3. Set the second derivative equal to zero and solve for x. These x-values are potential inflection points.
4. Test the intervals around the potential inflection points to determine if the second derivative changes its sign. If it does, then the point is an inflection point.

Inflection points are important in calculus because they help us understand the behavior of a function and its graph. They can be used to analyze the shape of the curve, optimize functions, and solve various real-world problems."
294,"In triangle ACD, B is located on the side AC, and E is located on the side AD. If AB = 3, AC = 5, CD = 3.5, ED = 3, and EB ∥ DC, what is the length of AD?",7.5,float,NONE,Geometry Textbook | 17.png,panlu/similarity5.json,similarity,Geometry,Math,"Similarity in geometry refers to the relationship between two shapes or figures that have the same shape but may have different sizes. In other words, two geometric figures are similar if they have the same proportions and angles, but their side lengths may be different. This concept is particularly important in the study of triangles, polygons, and other geometric shapes.

When two figures are similar, their corresponding angles are congruent (equal in measure), and their corresponding sides are proportional, meaning that the ratio of the lengths of corresponding sides is constant. For example, if two triangles are similar, the ratio of the lengths of their corresponding sides will be the same, and their angles will be equal.

Similarity can be determined using various methods, such as the Angle-Angle (AA) criterion, Side-Side-Side (SSS) criterion, and Side-Angle-Side (SAS) criterion. These criteria help to establish the similarity between two geometric figures by comparing their angles and side lengths.

In summary, similarity in geometry is a concept that deals with the comparison of shapes and figures based on their proportions, angles, and side lengths. Two figures are considered similar if they have the same shape but may differ in size."
291,"If x(n) and X(k) are an N-point DFT pair, then x(n+N)=x(n). Is it true?",True,bool,NONE,website | https://www.sanfoundry.com/digital-signal-processing-questions-answers-properties-dft/,maxku/fourier6-FT.json,fourier's theorem,Signal processing,EECS,"Fourier's theorem, also known as the Fourier Transform, is a fundamental concept in signal processing and mathematics that allows the decomposition of a complex signal into its constituent frequency components. It is named after the French mathematician Jean-Baptiste Joseph Fourier, who introduced the concept in the early 19th century.

In signal processing, signals are often represented as functions of time, such as audio signals, images, or any other time-varying data. Fourier's theorem states that any continuous, periodic signal can be represented as the sum of a series of sinusoidal functions (sines and cosines) with different frequencies, amplitudes, and phases. This representation is called the frequency domain representation of the signal, as opposed to the time domain representation.

The Fourier Transform is a mathematical operation that converts a time-domain signal into its frequency-domain representation. It essentially reveals the different frequency components present in the signal and their respective amplitudes. The inverse Fourier Transform, on the other hand, converts the frequency-domain representation back into the time-domain signal.

Fourier's theorem has numerous applications in signal processing, including:

1. Filtering: By transforming a signal into the frequency domain, it becomes easier to remove or enhance specific frequency components, such as removing noise or equalizing audio signals.

2. Compression: The frequency domain representation of a signal can often be compressed more efficiently than the time-domain representation, which is useful for data storage and transmission.

3. Analysis: Fourier analysis helps in understanding the behavior of signals and systems by examining their frequency content.

4. Convolution: Convolution is a mathematical operation used in signal processing to combine two signals or to apply a filter to a signal. It can be performed more efficiently in the frequency domain using the Fourier Transform.

In summary, Fourier's theorem is a fundamental concept in signal processing that allows the decomposition of a complex signal into its constituent frequency components, enabling various applications such as filtering, compression, analysis, and convolution."
38,"Is there exist a holomorphic function $f$ on the unit disk $B(0,1)$ (boundary excluded) such that $f(B(0,1))=C$? Here C is the complex space. Return 1 for yes and 0 for no.",1.0,float,NONE,complex analysis: exercise 7.2.6,mingyin/Riemann-conformal-mapping-theorem1.json,riemann conformal mapping theorem,Complex analysis,Math,"The Riemann Conformal Mapping Theorem, also known as the Riemann Mapping Theorem, is a fundamental result in complex analysis that states that any simply connected open subset of the complex plane, which is not the whole plane itself, can be conformally mapped onto the open unit disk. In other words, given a simply connected domain D in the complex plane (excluding the whole plane), there exists a bijective and holomorphic function f: D → U, where U is the open unit disk, such that its inverse function f^(-1): U → D is also holomorphic.

A conformal mapping, or conformal transformation, is a function that preserves angles locally, meaning that the angles between intersecting curves are the same in both the original domain and the transformed domain. In the context of complex analysis, a conformal mapping is a holomorphic function with a non-zero derivative.

The Riemann Mapping Theorem has important implications in various areas of mathematics, including potential theory, harmonic functions, and the study of partial differential equations. It also plays a crucial role in understanding the geometric properties of complex functions and their domains.

The theorem is named after the German mathematician Bernhard Riemann, who first stated it in 1851. However, the proof of the theorem was completed by other mathematicians, including Karl Weierstrass and Charles Neumann, in the years following Riemann's initial statement."
214,"You have a coin and you would like to check whether it is fair or biased. More specifically, let $\theta$ be the probability of heads, $\theta = P(H)$. Suppose that you need to choose between the following hypotheses: H_0 (null hypothesis): The coin is fair, i.e. $\theta = \theta_0 = 1 / 2$. H_1 (the alternative hypothesis): The coin is not fair, i.e. $\theta > 1 / 2$. We toss 100 times and observe 60 heads. What is the P-value?",0.023,float,NONE,website | https://www.probabilitycourse.com/chapter8/8_4_4_p_vals.php,wenhuchen/p_value3.json,p-value,Statistics,Math,"In statistics, the P-value (probability value) is a measure used to help determine the significance of a result or finding in hypothesis testing. It represents the probability of observing a test statistic as extreme or more extreme than the one obtained from the sample data, assuming that the null hypothesis is true.

The null hypothesis is a statement that assumes there is no effect or relationship between the variables being tested, and the alternative hypothesis is the statement that there is an effect or relationship. The P-value is used to make a decision about whether to reject or fail to reject the null hypothesis.

A smaller P-value indicates stronger evidence against the null hypothesis, suggesting that the observed result is unlikely to have occurred by chance alone. A larger P-value indicates weaker evidence against the null hypothesis, suggesting that the observed result may have occurred by chance.

Typically, a threshold value called the significance level (commonly denoted as α) is set, often at 0.05 or 5%. If the P-value is less than or equal to α, the null hypothesis is rejected, and the result is considered statistically significant. If the P-value is greater than α, the null hypothesis is not rejected, and the result is considered not statistically significant."
247,In how many ways can 10 distinct balls be placed into 4 identical boxes if each box must have at least 1 balls?,26335,integer,NONE,self,jianyu_xu/Stirling_number_second_kind_4.json,stirling number of the second kind,Combinatorics,Math,"Stirling Numbers of the second kind, denoted as S(n, k), are used in combinatorics to count the number of ways to partition a set of n elements into k non-empty subsets. In other words, they represent the number of ways to distribute n distinct items into k distinct groups, where each group has at least one item.

The Stirling Numbers of the second kind can be defined recursively using the following formula:

S(n, k) = k * S(n-1, k) + S(n-1, k-1)

with the base cases:

S(n, 0) = 0 if n > 0,
S(0, 0) = 1,
S(n, k) = 0 if k > n.

The first term, k * S(n-1, k), represents the case where the nth element is added to one of the existing k subsets. The second term, S(n-1, k-1), represents the case where the nth element forms a new subset by itself.

Here are some examples of Stirling Numbers of the second kind:

S(3, 2) = 3: There are three ways to partition a set of 3 elements into 2 non-empty subsets: {1, 2}, {3}; {1, 3}, {2}; {1}, {2, 3}.
S(4, 2) = 7: There are seven ways to partition a set of 4 elements into 2 non-empty subsets: {1, 2, 3}, {4}; {1, 2, 4}, {3}; {1, 3, 4}, {2}; {2, 3, 4}, {1}; {1, 2}, {3, 4}; {1, 3}, {2, 4}; {1, 4}, {2, 3}.

Stirling Numbers of the second kind have various applications in combinatorics, including counting the number of permutations with a given number of cycles, counting the number of functions from one set to another, and solving problems related to set partitions and groupings."
388,"Every published book has a ten-digit ISBN-10 number that is usually of the form x_1 - x_2 x_3 x_4 - x_5 x_6 x_7 x_8 x_9 - x_{10} (where each x_i is a single digit). The first 9 digits identify the book. The last digit x_{10} is a check digit, it is chosen so that 10 x_1 + 9 x_2 + 8 x_3 + 7 x_4 + 6 x_5 + 5 x_6 + 4 x_7 + 3 x_8 + 2 x_9 + x_{10} = 0 (mod 11). Is 3-540-90518-9 a valid ISBN number?",True,bool,NONE,"text | Thomas W. Hungerford, Abstract Algebra An Introduction.",elainewan/math_abstact_algebra_2.json,modular arithmetic,Number theory,Math,"Modular arithmetic, also known as clock arithmetic or the arithmetic of congruences, is a branch of number theory that deals with the properties and relationships of integers under the operation of modular addition, subtraction, multiplication, and sometimes division. It is a fundamental concept in number theory, cryptography, and computer science.

In modular arithmetic, numbers ""wrap around"" upon reaching a certain value called the modulus. The modulus is a positive integer that defines the size of the set of numbers being considered. When performing arithmetic operations, the result is always reduced to the remainder when divided by the modulus. This can be thought of as working with numbers on a circular number line, where the numbers wrap around after reaching the modulus.

The basic idea of modular arithmetic can be illustrated using a clock. A clock has a modulus of 12 (for a 12-hour clock) or 24 (for a 24-hour clock). When the hour hand moves past 12 or 24, it wraps around to 1 or 0, respectively. For example, if it is 10 o'clock and we add 5 hours, the result is 3 o'clock, not 15 o'clock. In this case, we are working modulo 12 or modulo 24.

In mathematical notation, modular arithmetic is often represented using the congruence symbol (≡). Two numbers a and b are said to be congruent modulo n if their difference (a - b) is divisible by n. This is written as:

a ≡ b (mod n)

For example, 17 ≡ 5 (mod 12) because 17 - 5 = 12, which is divisible by 12.

Modular arithmetic has many applications in various fields, including number theory, cryptography, computer science, and algebra. It is particularly useful in solving problems involving remainders, divisibility, and periodic patterns."
528,what is the value of \int_a^b \frac{dx}{\sqrt{(x-a)(b-x)}}? Round the answer to the thousands decimal.,3.1415926,float,NONE,mathematical analysis 1 exercise 7.7 example 7,mingyin/Fundamental-Theorem-of-Calculus3.json,integral rules,Calculus,Math,"Integral rules in calculus are a set of techniques and formulas used to evaluate and solve integrals. Integrals are a fundamental concept in calculus, representing the area under a curve or the accumulation of a quantity over a given interval. The integral rules provide a systematic approach to finding the antiderivative (the inverse of the derivative) of a function, which is essential for solving various mathematical and real-world problems.

Here are some of the most common integral rules:

1. Constant Rule: The integral of a constant (c) with respect to a variable (x) is equal to the product of the constant and the variable, plus a constant of integration (C). 
   ∫c dx = cx + C

2. Power Rule: The integral of x raised to the power of n (x^n) with respect to x is equal to x raised to the power of (n+1) divided by (n+1), plus a constant of integration (C). This rule is valid for n ≠ -1.
   ∫x^n dx = (x^(n+1))/(n+1) + C

3. Sum/Difference Rule: The integral of the sum or difference of two functions is equal to the sum or difference of their integrals.
   ∫(f(x) ± g(x)) dx = ∫f(x) dx ± ∫g(x) dx

4. Constant Multiple Rule: The integral of a constant multiplied by a function is equal to the constant multiplied by the integral of the function.
   ∫(cf(x)) dx = c∫f(x) dx

5. Substitution Rule (u-substitution): This rule is used when a function is composed of another function. It involves substituting a new variable (u) for a part of the original function, and then integrating with respect to the new variable.
   If u = g(x) and du/dx = g'(x), then ∫f(g(x))g'(x) dx = ∫f(u) du

6. Integration by Parts: This rule is used for integrating the product of two functions. It is based on the product rule for differentiation.
   If u = f(x) and v = g(x), then ∫u dv = uv - ∫v du

7. Trigonometric Integrals: These rules involve the integration of various trigonometric functions, such as sine, cosine, tangent, and their combinations.

8. Partial Fractions: This technique is used to integrate rational functions (fractions with polynomials in the numerator and denominator). It involves decomposing the rational function into simpler fractions, which can be integrated individually.

9. Improper Integrals: These rules deal with integrals that have infinite limits or involve functions with discontinuities. They often require the use of limits to evaluate the integral.

These integral rules, along with other advanced techniques, form the foundation of integral calculus and are essential for solving a wide range of mathematical problems."
335,Find the fraction of the standard solar flux reaching the Earth (about 1000 W/m^2) available to a solar collector lying flat on the Earth’s surface at Miami (latitude 26°N) at noon on the winter solstice.,0.656,float,NONE,Modern Physics for Scientists and Engineers Fourth Edition | Exercise 11.13,tonyxia/semiconductor5.json,semiconductor theory,Condensed matter physics,Physics,"Semiconductor Theory is a branch of condensed matter physics that deals with the study of semiconductors, which are materials that have electrical conductivity between that of insulators and conductors. These materials have unique properties that make them essential for various electronic devices, such as transistors, diodes, and solar cells.

The fundamental concept in semiconductor theory is the electronic band structure, which describes the energy levels available to electrons in a solid material. In a semiconductor, there are two important bands: the valence band and the conduction band. The valence band is filled with electrons, while the conduction band is initially empty. The energy gap between these two bands is called the bandgap.

In insulators, the bandgap is large, making it difficult for electrons to move from the valence band to the conduction band. In conductors, the valence and conduction bands overlap, allowing electrons to move freely. Semiconductors have a small bandgap, which means that electrons can be excited from the valence band to the conduction band under certain conditions, such as the application of heat or light.

There are two main types of semiconductors: intrinsic and extrinsic. Intrinsic semiconductors are pure materials, such as silicon or germanium, with no impurities. In these materials, the number of electrons in the conduction band is equal to the number of holes (empty spaces) in the valence band. The electrical conductivity of intrinsic semiconductors is relatively low.

Extrinsic semiconductors are created by introducing impurities, or dopants, into the intrinsic semiconductor. This process, called doping, can create either n-type or p-type semiconductors. In n-type semiconductors, the dopant atoms have more valence electrons than the semiconductor atoms, resulting in an excess of free electrons. In p-type semiconductors, the dopant atoms have fewer valence electrons, creating an excess of holes. The interaction between n-type and p-type materials forms the basis for many electronic devices, such as diodes and transistors.

Semiconductor theory also involves the study of various physical phenomena, such as carrier transport (how electrons and holes move through the material), recombination (the process by which electrons and holes combine), and the behavior of semiconductors under different conditions, such as temperature and illumination.

Overall, semiconductor theory is a crucial area of condensed matter physics that has led to the development of modern electronics and continues to drive advancements in technology."
508,Two argon atoms form the molecule $Ar_2$ as a result of a van der Waals interaction with $U_0 = 1.68 \times 10 ^ {-21}$ J and $R_0 = 3.82 \times 10 ^ {-10}$ m. Find the frequency of small oscillations of one Ar atom about its equilibrium position. (Unit: 10^11 Hz),5.63,float,NONE,University Physics with Modern Physics | Example 14.7,panlu/molecule_vibration1.json,molecule vibration,Atomic physics,Physics,"Molecule vibration refers to the oscillatory motion of atoms within a molecule. In atomic physics, it is essential to understand that molecules are not static entities; instead, they are in constant motion. The atoms within a molecule are held together by chemical bonds, which can be thought of as springs that allow the atoms to move relative to one another. These vibrations are a fundamental aspect of molecular behavior and play a crucial role in various physical and chemical properties of substances.

There are different types of molecular vibrations, including stretching, bending, and torsional vibrations. Here's a brief description of each:

1. Stretching vibrations: This type of vibration involves the change in the distance between two bonded atoms. Stretching vibrations can be further classified into symmetric and asymmetric stretching. In symmetric stretching, the atoms move towards or away from the central atom simultaneously, while in asymmetric stretching, the atoms move in opposite directions.

2. Bending vibrations: Bending vibrations involve the change in the angle between three atoms connected by chemical bonds. There are different types of bending vibrations, such as scissoring, rocking, wagging, and twisting. In scissoring, the angle between the atoms decreases, while in rocking, the atoms move back and forth in a plane perpendicular to the plane of the molecule.

3. Torsional vibrations: Torsional vibrations involve the rotation of a group of atoms around a bond axis. This type of vibration is particularly relevant in larger, more complex molecules where multiple atoms are connected by single bonds, allowing for rotation around those bonds.

Molecular vibrations are quantized, meaning they can only occur at specific energy levels. This quantization is a result of the wave-like nature of the atoms and the bonds within the molecule. The study of molecular vibrations and their associated energy levels is essential in understanding various phenomena, such as infrared spectroscopy, Raman spectroscopy, and molecular structure determination."
470,"Suppose $E \subset(0,2 \pi) is a measurable set. \left\{\xi_n
ight\}$ is an arbitrary sequence of real numbers. If the Lebesgue measure of E is 2, what is $\lim _{n 
ightarrow \infty} \int_E \cos ^2 (n x+\xi_n ) dx$? Return the numeric.",1.0,float,NONE,real analysis: chapter 4 exercise 2 question 17,mingyin/Lebesgue-measure4.json,lebesgue measure,Real analysis,Math,"Lebesgue measure is a fundamental concept in real analysis and measure theory, which is a branch of mathematics that deals with the generalization of length, area, and volume. It was introduced by the French mathematician Henri Lebesgue in the early 20th century and has since become a standard tool in modern analysis.

The Lebesgue measure is an extension of the classical notion of length for intervals on the real line. It assigns a non-negative value, called the ""measure,"" to subsets of the real line (or more generally, to subsets of Euclidean spaces) in a way that is consistent with our intuitive understanding of length, area, and volume. The main idea behind the Lebesgue measure is to define the measure of a set by approximating it with simpler sets, such as intervals or rectangles, whose measures are easy to compute.

Here are some key properties of the Lebesgue measure:

1. Non-negativity: The measure of any set is always non-negative.

2. Countable additivity: If you have a countable collection of disjoint sets (i.e., sets that have no elements in common), the measure of their union is equal to the sum of their individual measures.

3. Translation invariance: The measure of a set does not change if you translate (shift) the set by a fixed amount.

4. Normalization: The measure of a closed interval [a, b] on the real line is equal to its length, i.e., b - a.

The Lebesgue measure is particularly useful because it allows us to measure sets that are too irregular or ""fractal-like"" for the classical notion of length or area to handle. For example, the Cantor set, which is a highly irregular subset of the real line, has Lebesgue measure zero, even though it is uncountably infinite.

In addition to its applications in real analysis, the Lebesgue measure plays a crucial role in probability theory, where it serves as the foundation for the concept of probability distributions on continuous sample spaces. It is also closely related to the Lebesgue integral, which is a generalization of the Riemann integral and is widely used in various branches of mathematics and physics."
115,"As scotch whiskey ages, its value increases. One dollar of scotch at year 0 is worth $V(t) = exp{2\sqrt{t} - 0.15t}$ dollars at time t. If the interest rate is 5 percent, after how many years should a person sell scotch in order to maximize the PDV of this sale?",25,integer,NONE,"text | Walter Nicholson, Microeconomic Theory Basic Principles and Extensions.",elainewan/econ_micro_17.json,present discounted value,Quantitive methods,Finance,"Present Discounted Value (PDV), also known as Present Value (PV), is a concept in finance and economics that calculates the current worth of a future cash flow or series of cash flows, considering the time value of money. The time value of money is the idea that a dollar today is worth more than a dollar in the future, due to factors such as inflation, risk, and opportunity cost. Quantitative methods are used to determine the PDV, which helps in making investment decisions, comparing different projects, and valuing financial assets.

The Present Discounted Value is calculated using the following formula:

PDV = CF / (1 + r)^t

Where:
- PDV is the Present Discounted Value
- CF is the future cash flow
- r is the discount rate (interest rate or required rate of return)
- t is the time period in the future when the cash flow occurs

In the case of multiple cash flows, the PDV can be calculated by summing the PDV of each cash flow:

PDV = Σ [CF_t / (1 + r)^t]

Where:
- CF_t is the cash flow at time t
- t ranges from 1 to n, where n is the number of periods

The discount rate (r) is a crucial factor in determining the PDV, as it reflects the risk associated with the investment, the opportunity cost of capital, and the expected rate of inflation. A higher discount rate will result in a lower PDV, indicating that the future cash flows are worth less in today's terms.

In summary, Present Discounted Value is a quantitative method used to evaluate the worth of future cash flows in today's terms, considering the time value of money. It is widely used in finance and economics for investment decisions, project evaluations, and asset valuations."
327,"Let G_n(s) be the probability generating function of the size Z_n of the n-th generation of a branching process, where Z_0=1 and var(Z_1)>0. Let H_n be the inverse function of the function G_n, viewed as a function on the interval [0, 1].  Is M_n= {H_n(s)}^{Z_n} defines a martingale with respect to the sequence Z? Return 1 for yes and 0 for no.",1.0,float,NONE,probability: 12.1.9,mingyin/martingale1.json,martingale,Probability theory,Math,"Martingale, in probability theory, is a mathematical model used to describe a fair game or a stochastic process where the expected value of a random variable at a future time step is equal to its present value, given all the past information. In other words, the expected gain or loss in a Martingale system is always zero, regardless of the outcomes of previous events.

The concept of Martingale is often used in betting strategies, finance, and statistical analysis. In betting, the Martingale strategy involves doubling the bet after each loss, so that the first win would recover all previous losses plus a profit equal to the original stake. However, this strategy has its limitations, as it requires an infinite bankroll and has no guarantee of winning in the long run.

In a formal definition, a sequence of random variables {X1, X2, X3, ...} is called a Martingale with respect to another sequence of random variables {Y1, Y2, Y3, ...} if the following conditions are met:

1. The random variables {X1, X2, X3, ...} are integrable, meaning their expected values exist and are finite.
2. The random variables {Y1, Y2, Y3, ...} form a filtration, which is a sequence of increasing sigma-algebras (collections of events) that represent the information available at each time step.
3. For each time step, the expected value of the next random variable in the sequence, given the information available up to the current time step, is equal to the current random variable's value. Mathematically, this can be expressed as E[Xn+1 | Y1, Y2, ..., Yn] = Xn.

In summary, a Martingale is a sequence of random variables that represents a fair game or process, where the expected value of a future event is equal to the current value, given all past information."
511,\lim_{x \to 1}(1/(x - 1) - c/(x^3 - 1)) exists. What is the value of c?,3,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_2.json,indeterminate form,Calculus,Math,"Indeterminate form in calculus refers to an expression or limit that cannot be directly determined or evaluated due to its ambiguous or undefined nature. These forms typically arise when evaluating limits of functions that involve operations like division, multiplication, or exponentiation, where the individual components of the expression tend to conflicting or competing values.

The most common indeterminate forms are:

1. 0/0: This form arises when both the numerator and denominator of a fraction tend to zero. It is indeterminate because it is unclear whether the limit should be zero, a finite value, or infinity.

2. ∞/∞: This form occurs when both the numerator and denominator tend to infinity. It is indeterminate because it is uncertain whether the limit should be zero, a finite value, or infinity.

3. ∞ - ∞: This form arises when two infinite quantities are subtracted from each other. It is indeterminate because the result could be zero, a finite value, or infinity, depending on the relative rates at which the two quantities grow.

4. 0 × ∞: This form occurs when a quantity tending to zero is multiplied by a quantity tending to infinity. It is indeterminate because the result could be zero, a finite value, or infinity, depending on the relative rates at which the two quantities approach their respective limits.

5. ∞^0: This form arises when an infinite quantity is raised to the power of zero. It is indeterminate because it is unclear whether the limit should be one, a finite value, or infinity.

6. 0^0: This form occurs when a quantity tending to zero is raised to the power of another quantity tending to zero. It is indeterminate because it is uncertain whether the limit should be zero, one, or a finite value.

7. 1^∞: This form arises when a quantity tending to one is raised to the power of an infinite quantity. It is indeterminate because it is unclear whether the limit should be one, a finite value, or infinity.

To resolve indeterminate forms, mathematicians often use techniques such as L'Hôpital's rule, algebraic manipulation, or series expansions to find the limit or simplify the expression."
92,In how many ways can a set of 6 distinct letters be partitioned into 2 non-empty groups if each group must contain at least 2 letters?,25,integer,NONE,self,jianyu_xu/Stirling_number_second_kind_5.json,stirling number of the second kind,Combinatorics,Math,"Stirling Numbers of the second kind, denoted as S(n, k), are used in combinatorics to count the number of ways to partition a set of n elements into k non-empty subsets. In other words, they represent the number of ways to distribute n distinct items into k distinct groups, where each group has at least one item.

The Stirling Numbers of the second kind can be defined recursively using the following formula:

S(n, k) = k * S(n-1, k) + S(n-1, k-1)

with the base cases:

S(n, 0) = 0 if n > 0,
S(0, 0) = 1,
S(n, k) = 0 if k > n.

The first term, k * S(n-1, k), represents the case where the nth element is added to one of the existing k subsets. The second term, S(n-1, k-1), represents the case where the nth element forms a new subset by itself.

Here are some examples of Stirling Numbers of the second kind:

S(3, 2) = 3: There are three ways to partition a set of 3 elements into 2 non-empty subsets: {1, 2}, {3}; {1, 3}, {2}; {1}, {2, 3}.
S(4, 2) = 7: There are seven ways to partition a set of 4 elements into 2 non-empty subsets: {1, 2, 3}, {4}; {1, 2, 4}, {3}; {1, 3, 4}, {2}; {2, 3, 4}, {1}; {1, 2}, {3, 4}; {1, 3}, {2, 4}; {1, 4}, {2, 3}.

Stirling Numbers of the second kind have various applications in combinatorics, including counting the number of permutations with a given number of cycles, counting the number of functions from one set to another, and solving problems related to set partitions and groupings."
106,Find the entropy rate of the Markov chain associated with a random walk of a king on the 3 by 3 chessboard. Use base 2 logarithm and return the entropy rate in bits.,2.24,float,NONE,textbook 4.20,xinyi/random_walk_on_chessboard.json,random walk,Probability theory,Math,"Random walk is a mathematical concept in probability theory that describes a path consisting of a series of random steps. It is a stochastic process, meaning it involves a sequence of random variables, where each variable represents a step in the walk. The random walk can occur in one-dimensional, two-dimensional, or even higher-dimensional spaces.

In a random walk, an object, often referred to as a ""walker,"" starts at an initial position and moves in a series of steps, with each step being determined by a random variable. The direction and distance of each step are typically drawn from a probability distribution, which can be uniform, Gaussian, or any other distribution depending on the problem being modeled.

A simple example of a random walk is a one-dimensional random walk on a number line, where a walker starts at position 0 and at each step, moves either one step to the left or one step to the right with equal probability. After a certain number of steps, the walker's position can be anywhere on the number line, and the probability of being at a particular position can be calculated.

Random walks have applications in various fields, including physics, biology, economics, and computer science. They are used to model phenomena such as stock market fluctuations, diffusion processes, animal foraging behavior, and even the movement of molecules in a fluid. Random walks also play a crucial role in the development of algorithms for search and optimization problems."
722,"Find the arc length of y = x^{-1} over the interval [1,2] using the Simpson's Rule S_8.",1.132,float,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_9_3.json,simpson's rule,Calculus,Math,"Simpson's Rule is a numerical integration technique used in calculus to approximate the definite integral of a function. It is named after the British mathematician Thomas Simpson, who popularized it in the 18th century. The rule is based on the idea of approximating the area under the curve of a function by using parabolic (quadratic) segments instead of linear segments, as in the case of the trapezoidal rule.

Simpson's Rule works by dividing the interval of integration [a, b] into an even number of equally spaced subintervals (2n), and then fitting a parabola (a quadratic polynomial) through the points of the function at the endpoints and the midpoint of each subinterval. The area under each parabolic segment is then calculated and summed up to approximate the total area under the curve, which represents the definite integral of the function.

The formula for Simpson's Rule is given by:

∫(a to b) f(x) dx ≈ (Δx/3) [f(x0) + 4f(x1) + 2f(x2) + 4f(x3) + ... + 2f(x_{2n-2}) + 4f(x_{2n-1}) + f(x_{2n})]

where:
- a and b are the limits of integration
- f(x) is the function to be integrated
- Δx = (b - a) / (2n) is the width of each subinterval
- n is the number of subintervals (must be an even number)
- x_i = a + iΔx for i = 0, 1, 2, ..., 2n are the endpoints and midpoints of the subintervals

Simpson's Rule provides a more accurate approximation of the definite integral compared to other methods like the trapezoidal rule, especially for functions that have continuous second derivatives. However, it may not be as accurate for functions with discontinuities or rapidly changing behavior."
545,True of false: one can draw a simple connected planar graph with 200 vertices and 400 faces,False,bool,NONE,Self,tonyxia/maxplanar3.json,maximal planar graph,Graph theory,EECS,"A Maximal Planar Graph, in graph theory, is a type of graph that is planar and cannot have any more edges added to it without losing its planarity. In other words, it is a graph that can be drawn on a plane without any of its edges crossing, and adding any more edges would cause at least one crossing.

Here are some key properties of maximal planar graphs:

1. Every face (region enclosed by edges) in a maximal planar graph is a triangle. This is because if there is a face with more than three sides, we can add an edge between two non-adjacent vertices of that face without violating planarity, which contradicts the maximality of the graph.

2. A maximal planar graph with 'n' vertices (n ≥ 3) has exactly 3n - 6 edges. This can be derived from Euler's formula for planar graphs, which states that for any connected planar graph, the number of vertices (V), edges (E), and faces (F) are related by the equation V - E + F = 2.

3. A maximal planar graph is also called a triangulation, as it can be seen as a way to divide a planar region into triangles by adding edges.

4. A maximal planar graph is always 3-connected, meaning that it remains connected even after removing any two vertices and their incident edges. This is because removing a vertex from a maximal planar graph results in a planar graph with a single face that is a polygon, which can always be triangulated.

5. Every maximal planar graph is a subgraph of some complete graph, where a complete graph is a graph in which every pair of distinct vertices is connected by a unique edge.

In summary, a maximal planar graph is a planar graph that is ""as connected as possible"" without losing its planarity. It has several interesting properties, such as every face being a triangle and having a fixed relationship between the number of vertices and edges."
700,What is \lim_{x \to 1} ((x - 1) sin((\pi)/(x - 1))?,0,integer,NONE,"text | Jon Rogawski and Colin Adams, Calculus.",elainewan/math_calculus_2_4.json,squeeze theorem,Calculus,Math,"The Squeeze Theorem, also known as the Sandwich Theorem or the Pinching Theorem, is a fundamental concept in calculus that helps to determine the limit of a function when direct substitution or algebraic manipulation is not possible. The theorem states that if you have three functions, f(x), g(x), and h(x), such that f(x) ≤ g(x) ≤ h(x) for all x in a certain interval around a point 'a' (except possibly at 'a' itself), and if the limit of f(x) and h(x) as x approaches 'a' is the same value L, then the limit of g(x) as x approaches 'a' must also be L.

In mathematical notation, the Squeeze Theorem can be written as:

If f(x) ≤ g(x) ≤ h(x) for all x in an interval around 'a' (except possibly at 'a') and lim (x→a) f(x) = lim (x→a) h(x) = L, then lim (x→a) g(x) = L.

The Squeeze Theorem is particularly useful when dealing with trigonometric functions or functions that are difficult to evaluate directly. By comparing the function of interest (g(x)) to two other functions (f(x) and h(x)) that ""squeeze"" or ""sandwich"" it, we can determine the limit of g(x) as x approaches a certain point."
